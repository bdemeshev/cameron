
\chapter {Байесовские методы}


\section{Введение}

Эта глава является введением в байесовскую эконометрику. Интерес к анализу байесовских регрессий фантастически вырос с момента публикации трудов Зеллнера (1971) и Лимера (1978). Широкое распространение получило использование байесовского подхода при рутинном анализе данных.  Оно в значительной степени было обусловлено развитием компьютерной техники и разработкой специализированного программного обеспечения. Ввиду столь широкого распространения байесовского  подхода в одной главе невозможно рассмотреть все стороны вопроса. Цель настоящей главы --- дать общее представление об основных идеях и приложениях байесовской эконометрики. Не смотря на скромность поставленных целей, некоторые разделы содержат много технических деталей. 

В отличие от метода правдоподобия или частотного или классического подхода к оценке вероятностей, рассмотренного ранее, для применения байесовского подхода требуется определение априорных вероятностей для неизвестных параметров модели, помимо спецификации самой модели. Это требование, как с теоретической, так и с практической сторон является причиной затруднений для многих исследователей. Существенность определения априорного мнения дало повод многим считать байесовский подход субъективным. Однако, как будет обсуждаться в этой главе, на больших выборках значимость априорного мнения незначительна, можно указать довольно слабые априорные ограничения на параметры, и существуют специальные методы, позволяющие  определить чувствительность выводов к априорным предпосылкам. Из этого можно заключить, что субъективность определения априорных предпосылок преувеличена.

Байесовские подходы имеют большой потенциал использования в прикладной микроэконометрике, в частности, для оценивания параметров сложных моделей, для которых зачастую отсутствует аналитическая форма функции правдоподобия. В главе 12 обсуждаются методы симуляционного моделирования, которые могут применяться в подобной ситуации. Однако, применение методов  симуляицонного моделирования, в особенности метода максимального правдоподобия, может вызвать затруднения, поскольку для максимизации функции требуется достаточно большое количество симуляций, растущее с увеличением размера выборки. Даже при современных возможностях вычислительной техники анализ больших выборок и моделей высокой размерности могут потребовать значительных расчетов. Байесовские методы, напротив, не требуют применения методов максимизации. Эти подходы оценивания могут заменить метод максимального правдоподобия и полученные оценки во многих случаях будут отличной, если не идеальной, заменой. В действительности, выбор байесовских методов не требует смены философии, эти методы могут быть использованы из прагматических соображений.

Вышеприведенные аргументы не означают, что байесовские методы не имеют глубокого теоретического обоснования. Обоснования есть. Следует отметить, по меньшей мере, три особенности. Во-первых, байесовские методы позволяют оценить апостериорное распределение исследуемых параметров целиком, оставив на усмотрение исследователя выбор момента или квантили распределения для представления в отчёте, как правило, этот выбор осуществляется согласно цели исследования. Не нужно формировать отдельные оценки для среднего, медианы, квантилей и т.д., поскольку в апостериорном распределении они есть сразу все! Во-вторых, байесовский подход, учитывающий характеристики исходных данных, дает точные результаты для конечной выборки, тем самым снимая  необходимость корректировок или уточнений. Распределение полученных оценок сходится к нормальному на больших выборках, где влияние априорных предпосылок снижается. В-третьих, байесовские методы позволяют естественным способом выбрать модель.

В разделе 13.2 описаны  основные идеи и положения байесовского анализа, а также основные свойства байесовских оценок. Основные идеи проиллюстрированы в разделе 13.3 с помощью относительно просто интерпретируемых регрессионных моделей. В общем случае для апостериорного распределения не существует аналитического решения. В разделе 13.4 представлены методы Монте-Карло, а именно метод сэмплирования по важности, который применяется для получения численных оценок моментов апостериорного распределения. В разделе 13.5 обсуждается использование марковских цепей для реализации метода Монте-Карло, а именно алгоритм Гиббса и алгоритм Метрополиса-Хастингса, используемые для генерирования случайных значений из сложного апостериорного распределения. Примеры этих способов рассмотрены в разделе 13.6. Дополнительные темы о пополнении данных и байесовского выбора между моделями представлены в разделах 13.7 и 13.8.

\section{Байесовский подход}

В байесовском подходе неопределенность относительно значения параметра $\theta$ моделируется в явном виде путём введением плотности $\pi(\theta)$ априорного распределения, названного так, поскольку спецификация априорного распределения не учитывает имеющиеся данные. Априорное распределение основано на субъективных предположениях, выраженных в терминах вероятности. Спецификация распределения подробно рассмотрена в разделе 13.2.4. Например, предположим, что $\theta$ эластичность дохода, причем на основе экономической модели или  предыдущих исследований допустим, что $\theta$ лежит в промежутке от 0.8 до 1.2 с вероятностью 0.95. Тогда разумным априорным распределением $\theta$ может быть $\theta{\sim}N[1,0.1^{2}]$.

Другой компонент байесовского оценивания ---  функция совместной плотности выборки или функция правдоподобия $f(y|\theta)$. Если система состоит только из одного уравнения, тогда $y$ это вектор $N{\times}1$. Для простоты обозначений зависимость плотности от независимых переменных явно не прописывается. Экзогенные переменные введены в рассмотрение в разделе 13.3, тогда плотность $f(y|\theta)$ заменяется на $f(y|X,\theta)$ и байесовский анализ проводится при фиксированных независимых переменных. Также следует отметить, что в этой главе обозначение $f(\cdot)$ используется для совместной плотности для всех наблюдений, а не плотности $i$-го наблюдения.

Если данных нет, тогда единственное что мы имеем --- это априорное мнение. После того как данные получены, при классическом подходе делается оценка параметра $\theta$ методом максимального правдоподобия. В байесовском подходе анализ правдоподобия выборки объединяется с анализом доступной априорной информации, даже если такая информация представлена в виде распределения вероятностей. В целом подход может рассматриваться как пересмотр априорных предпосылок с помощью полученных данных (функции правдоподобия). В частности, распределение неизвестного параметра $\theta$ может быть получено комбинированием априорной информации и метода максимального правдоподобия. Полученное распределение называется апостериорным, так как оно отражает представления о исследуемом параметре апостериори, то есть после анализа данных.

\subsection{Теорема Байеса}

Основным результатом, с помощью которого формируется апостериорное распределение, является теорема Байеса, согласно которой

\begin{equation}
f(\theta|y)=\dfrac{f(y|\theta)\pi{(\theta)}}{f(y)},
\end{equation}

где $f(y)$   частная плотность распределения вероятностей $y$, определяемая следующим уравнением

\begin{equation}
f(y)=\int R(\theta)f(y|\theta)\pi{(\theta)}d(\theta),
\end{equation}

где $R(\theta)$ обозначает носитель функции $\pi(\theta)$. Этот результат получается из выражения для условной вероятности события $A$ при условии $B$

\[
\Pr[A|B]=\dfrac{\Pr[A{\cap}B]}{\Pr[B]}
\]

\[
\dfrac{\Pr[B|A]\Pr[A]}{\Pr[B]},
\]

где второе равенство следует из $\Pr[B|A]=\Pr[A{\cap}B]/\Pr[A]$.

Поскольку знаменатель $f(y)$ в (13.1) не зависит от $\theta$, можно представить $p(\theta|y)$ как произведение плотности  и априорного распределения; тогда

\begin{equation}
p(\theta|y){\propto}L(y|\theta)\pi(\theta).
\end{equation}

Полученное соотношение позволяет опустить несущественные константы в формуле апостериорной вероятности, тем самым позволяя упростить вывод и представление. Опущенные константы  могут быть восстановлены позже, это будет показано в разделе 13.2.2. Когда функция плотности записывается без нормализирующих констант, ее называют ядром плотности. 

Во многих случаях использование формул (13.1) и (13.3) не позволяет получить аналитическое выражение для плотности апостериорного распределения. Однако, аналитическое выражение для плотности необязательно и  далее будут рассмотрены примеры применения симуляционного моделирования для расчета приближенного значения апостериорной плотности. Этот  метод позволяет использовать байесовский анализ практически для любых приложений параметрической микроэконометрики.

Обычно для обозначения апостериорной плотности используются специальные обозначения, $f(\theta|y)$ заменяется на $p(\theta|y)$. При этом обычная совместная плотность распределения $f(y|\theta)$ обозначается как функция максимального правдоподобия $L(y|\theta)$. Далее будем записывать апостериорную плотность как

\begin{equation}
p(\theta|y){\propto}L(y|\theta)\pi(\theta).
\end{equation}

Такое представление плотности является одним из основных для байесовского подхода и подчеркивает значимое отличие между частотным и байесовским подходами. Согласно частотному подходу, истинное значение параметра является постоянным, при этом оценки значения параметра рассматриваются как случайные величины. В  байесовском подходе, напротив, параметры также считаются случайными величинами.

\subsection{Пример использования теоремы Байеса}

Предположим, что $y{\sim}N[\theta,\sigma^{2}]$, где дисперсия $\sigma^{2}$ известна, а скалярный параметр $\theta$ неизвестен. Для случайной выборки $y_1,\ldots ,y_N$, совместная плотность $y$ определяется выражением

\[
L(y|\theta)=\Pi^{N}_{i=1}(2\pi\sigma^{2})^{-1/2}\exp\left\lbrace -(y_i-\theta)^{2}/2\sigma^{2}\right\rbrace 
\]

\[
=(2\pi\sigma^{2})^{-N/2}\exp\left\lbrace-\sum^{N}_{i=1}(y_i-\theta)^{2}/2\sigma^{2}\right\rbrace
\]

\[
\propto{\exp}\left\lbrace-\frac{N}{2\sigma^{2}}(\overline{y}-\theta)^{2}\right\rbrace,
\]

где $\overline{y}=N^{-1}\sum_{i}y_i$, и $\sum_{i}(y_i-\theta)^{2}=\sum_{i}(y_i-\overline{y}+\overline{y}-\theta)^{2}=\sum_{i}(y_i-\overline{y})^{2}+\sum_{i}(\overline{y}-\theta)^{2}$. Множители, не содержащие параметр $\theta$, входят в константу и отбрасываются. В рамках частотного подхода мы максимизируем  логарифмическую функцию правдоподобия по параметру $\theta$, в результате определяется  оценка параметра $\hat{\theta}=\overline{y}$.

В байесовском подходе дополнительно описывается априорное распределение параметра $\theta$. Для построения аналитических выкладок удобно использовать нормальное априорное распределение $\theta{\sim}N[\mu,\tau^{2}]$, где значения математического ожидания $\mu$ и дисперсии $\tau^{2}$ полагаются известными. Большие значения дисперсии $\tau^{2}$ свидетельствуют о значительной априорной неопределенности. Тогда априорную плотность можно записать как

\[
\pi(\theta)=(2\pi\tau^{2})^{-1/2}\exp\left\lbrace-(\theta-\mu)^{2}/2\tau^{2}\right\lbrace
\]

\[
\propto{\exp}\left\lbrace-(\theta-\mu)^{2}/2\tau^{2}\right\rbrace,
\]

где $(2\pi\tau^{2})^{-1/2}$ не зависит от $\theta$ и включается в коэффициент пропорциональности. Используя (13.4), получим апостериорную плотность, равную

\[
p(\theta|y)=\frac{L(y|\theta)\pi(\theta)}{\int^{\infty}_{-\infty}L(y|\theta)\pi(\theta)d\theta}, -\infty<\theta<\infty.
\]

Знаменатель гарантирует, что апостериорное распределение корректно (т.е., интеграл от плотности равен единице). Для большинства задач знаменатель можно опустить, в этом случае мы работаем с $p(\theta|y)\propto{L(y|\theta)\pi(\theta)}$. Числитель (13.5) может быть записан, как

\[
L(y|\theta)\pi(\theta)
\]

\[
=(2\pi{\sigma}^{2})^{-N/2}\exp{\left\lbrace-\frac{N}{2{\sigma}^{2}}{(\overline{y}-\theta)}^{2}\right\rbrace}{(2\pi{\tau}^{2})}^{-1/2}\exp\left\lbrace-\dfrac{(\theta-\mu)^{2}}{2\tau^{2}}\right\rbrace
\]

\[
=(2\pi)^{(N+1)/2}{(\sigma)^{2}}^{-N/2}(\tau^{2})^{-1/2}\exp\left\lbrace-\dfrac{1}{2\sigma^{2}}\sum^{N}_{i=1}(y_i-\theta)^{2}-\dfrac{(\theta-\mu)^{2}}{2\tau^{2}}\right\rbrace.
\]

Поскольку 

\[
\sum^{N}_{i=1}(y_i-\theta)^{2}=\sum^{N}_{i=1}(y_i-\overline{y})^{2}+N(\overline{y}-\theta)^{2},
\]

а константа интегрирования в (13.5) и другие константы не зависящие от $\theta$ могут быть включены в коэффициент пропорциональности, получим

\begin{equation}
p(\theta|y)\propto{\exp}\left\lbrace-\dfrac{N}{2\sigma^{2}}(\theta-\overline{y})^{2}\right\rbrace{\exp}\left\lbrace-\dfrac{1}{2}\dfrac{(\theta-\mu)^{2}}{\tau^{2}}\right\rbrace
\end{equation}

\[
\propto\lbrace-\dfrac{1}{2}\left[\dfrac{(\theta-\mu)^{2}}{\tau^{2}}+\dfrac{(\overline{y}-\theta)^{2}}{N^{-1}\sigma^{2}}\right] \rbrace
\]

\begin{equation}
\propto{\exp}\lbrace-\dfrac{1}{2}\left[\dfrac{(\theta-\mu_{1})^{2}}{\tau^{2}_{1}}\right]\rbrace.
\end{equation}

Последнее выражение равно ядру $N[\mu_1,\tau^{2}_1]$ распределения, где 

\begin{equation}
\mu_1=\tau^{2}_{1}(N\overline{y}/\sigma^{2}+\mu/\tau^{2}),
\end{equation}

\[
\tau^{2}_1=(N/\sigma^{2}+1/\tau^{2})^{-1}.
\]

Последнее соотношение в (13.7) получено дополнением до полного квадрата, при этом использовалось тождество верное для произвольных скаляров $z$, $y$, $a_1$, $a_2$, $c_1$ и $c_2$:

\[
c_{1}(z-a_1)^{2}+c_{2}(z-a_2)^{2}=(c_1+c_2)\left(z-\left( \dfrac{c_{1}a_1+c_{2}a_2}{(c_1+c_2)^{2}}\right)\right)^{2}+\dfrac{c_{1}c_{2}}{(c_1+c_2)}(a_{1}-a_2)^{2}, 
\]

где $z=\theta, a_1=\mu, a_2=\overline{y}, c_1=1/\tau^2$, и $c_2=1/(N^{-1}\sigma^{2}+\tau^{2})$. Слагаемые, которые не зависят от $\theta$, отбрасываются.

В итоге получим следующие результаты:

Данные: $y|\theta{\sim}N[\theta,\sigma^{2}]$, где значение $\sigma^{2}$ известно.

Априорное распределение: $\theta{\sim}N[\mu,\tau^2]$, $\mu$, $\tau^2$ определено.

Апостериорное распределение: $\theta|y{\sim}N[\mu_1,\tau^{2}_{1}]$, $\mu_1$, $\tau^{2}_1$ задаются соотношениями (13.8).

Среднее значение апостериорного распределения, $\mu_1$, равно взвешенной сумме среднего априорного распределения $\mu$ и выборочного среднего $\overline{y}$, где веса учитывают точность правдоподобия через $\sigma^{2}/N$ и априорного распределения через $\tau^{2}$. Применение байесовского подхода позволяет определить общую дисперсию на основе  параметра точности, значение которого обратно дисперсии. В вышеприведенных выражениях значение параметра точности апостериорного распределения $\tau^{-2}_1$ равно сумме значений параметра точности для выборочного среднего,$\overline{y}$, $N/\sigma^{2}$ и априорного параметра точности $1/\tau^{2}$, таким образом точность будет возрастать при объединении информации из выборки и априорной информации. 

Если априорная информация является неточной, т.е. значение $1/\tau^{2}$ мало, тогда вес среднего априорного распределения также будет мал по отношению к выборочной информации; и априорное распределение играет незначительную роль при определении апостериорного распределения. Аналогично, при большом размере выборки, выборочная информация играет главную роль, поскольку рост размера выборки приводит к росту $N/\sigma^{2}$ относительно $1/\tau^{2}$. Апостериорное распределение сходится к известному асимптотическому нормальному распределению, если не считать, что байесовский подход приводит к трактовке $\theta{sim^{a}N[\overline{y},\sigma^{2}/N]}$, а не $\overline{y}{sim}^{a}N[\theta,\sigma^{2}/N]$.


Рисунок 13.1


Приведем конкретный пример, предположим, что $\sigma^{2}=100$, параметры априорного распределения $\mu=5$ и $\tau^{2}=3$, размер выборки равен $N=50$ и среднее значение $\overline{y}=10$. Тогда параметры функции правдоподобия --- $N[10,2]$, априорного распределения --- $N[5,3]$ и согласно (13.7) и (13.8) апостериорного распределения --- $N[8,12]$. Графики функций плотности изображены на Рисунке 13.1. Среднее значение апостериорного распределения лежит между средним априорного распределения и выборочного среднего, в то время как дисперсия апостериорного  меньше, чем у априорного распределения и функции правдоподобия. 

\subsection{Сравнение байесовского и небайесовского подходов}

Полезно рассмотреть сходства и различия частотного и байесовского подходов.

В параметрическом частотном подходе все статистические выводы основываются на функции правдоподобия. При выполнении соответствующих условий регулярности оценка максимального правдоподобия состоятельна и асимптотически нормальна. Теория выборочного оценивания предоставляет основу для построения вероятностных утверждений об оцениваемых величинах, их  функциях, или условных прогнозах. Априорная информация о параметрах может быть учтена посредством использования ограниченной оценки правдоподобия.

В байесовском анализе, результаты которого представлены в Таблице 13.1, процесс порождающий данные и сами данные  объединены с априорным распределением параметров. Спецификация априорного распределения детально рассмотрена в Разделе 13.2.4. Априорное распределение включает сформулированную в терминах вероятностей информацию доступную до получения исходных данных. Также априорное  распределение может быть основано на <<полученной информации>>. Априорная информация и данные объединяются с помощью теоремы Байеса. 

В результате мы получим апостериорное распределение параметров $\theta$, которое можно рассматривать как преобразованную функцию максимального правдоподобия. Иначе, при наличии данных, при помощи апостериорного распределения возможно отобразить <<исправленное априорное распределение>>. Если выборка малого размера, и, возможно, относительно неинформативна, апостериорное распределение может выглядеть как априорное, однако при больших размерах выборки апостериорное распределение будет отражать особенности данных.


\begin{tabular}{p{6cm}p{8cm}}
Компонента & Формула \\ 
\hline
Выборочная модель & $(y_1,\ldots ,y_N)$ независимы с плотностью $f(y|\theta)$ \\  
Совместная плотность/ функция правдоподобия & $\pi(\theta)$, $L(y|\theta)$; $\theta{\epsilon}\Theta$ \\ 
Априорное распределение & $\pi(\theta), \theta{\epsilon}\Theta$ \\ 
Апостериорное распределение & $p(\theta|y):\begin{cases} 
=f(y|\theta)\pi(\theta)/\int f(y|\theta)\pi(\theta)\,d\theta \\
\propto f(y|\theta)\pi(\theta) \\
\propto L(y|\theta)\pi(\theta)
\end{cases}$ \\ 
Апостериорная плотность распределения$\rightarrow$ апостериорные результаты$\rightarrow$ & оценка параметров, вероятностные утверждения, прогнозирование, сравнение моделей
\end{tabular} 



\subsection{Спецификация априорного распределения}

Байесовский анализ требует спецификации процесса порождающего данные $f(y|\theta)$ и априорной информации $\pi(\theta)$. Процесс порождающий данные, как правило, определяется аналогично случаю полностью параметрического анализа на основе функции правдоподобия. Для анализа бинарных данных применяются логит и пробит-модели, для дискретных данных --- пуассоновская или отрицательная биномиальная модель и т.д.

Принципиальный отличие байесовского анализа, по сравнению с классическим подходом, заключается в необходимости построения априорного распределения. Результаты могут меняться в зависимости от выбора априорного распределения, поскольку разные априорные распределения дают разные апостериорные распределения за исключением случаев, когда объем выборки достаточен для обеспечения преобладания  выборочной информации.

Один из подходов определения исходных предпосылок состоит в выборе такого априорного распределения, которое будет незначительно влиять на формирование апостериорного распределения, тогда результаты будут зависеть в основном от выборочных данных. При наличии достоверной априорной информации предпочтителен  альтернативный подход, согласно которому априорное распределение будет отражать имеющуюся информацию. Ранее применение обоих подходов, в особенности последнего, ограничивалось проблемой  получения апостериорного распределения, на данный момент эта проблема частично решена благодаря развитию вычислительных алгоритмов. Популярный комбинированный подход построен на использовании иерархически выстроенной априорной информации, где неопределенность параметров выражена в терминах функций вероятности, которые, в свою очередь, зависят от других неизвестных параметров. 

\subsubsection*{Неинформативные предпосылки об априорном распределении}

Неинформативные априорные предположения это те, которые не оказывают существенного влияния на итоговое апостериорное распределение.

Простой способ построить априорное распределение, отражающее скудость априорного знание --- использовать равномерное априорное распределение с $\pi(\theta)=c$ для всех значений параметра $\theta$, где $c>0$ постоянно, в этом случае веса для всевозможных значений $\theta$ будут равны между собой.

Одно из ограничений использования равномерного априорного распределения возникает в задачах, где на значение параметров $\theta$ не накладываются ограничения, в этом случае использование равномерного априорного распределения дает неправильные значения плотности, поскольку $\pi(\theta)d\theta=\infty$. При этом, итоговое апостериорное распределение возможно также будет неверным, тем не менее в отдельных случаях получаемое апостериорное распределение является собственным.

Другим недостатком равномерного априорного распределения является неинвариантность к замене параметров. Например, для скалярного параметра $\theta>0$ альтернативной параметризацией плотности $y$ будет запись через параметр $\gamma=\ln {\theta}$, поскольку $-\infty<\gamma<\infty$. Если априорное распределение $\theta$ является равномерным, тогда $\pi(\theta)=c$, однако соответствующее априорное распределение $\pi^{*}(\gamma)$ для $\gamma$ не является равномерным, поскольку $\pi^{*}(\gamma)=\pi(\theta)|d\theta/d\gamma|=ce^{\gamma}$. Таким образом, неинформативное априорное распределение для одной параметризации, может нести информацию в другой параметризации.

Равномерное априорное распределение может быть выражена распределением с очень высокой дисперсией. Например, предположим, что скаляр $\theta$ имеет априорное распределение $N[\mu,\tau^{2}]$, где $\tau^{2}$ очень велико. Тогда для значений $\theta$ правдоподобных при имеющихся данных, значение априорной плотности будет равно $\pi(\theta){\simeq}1/(2\pi\tau^{2})$, т.е. примерно константе, поскольку $\exp[-(\theta-\mu)/2\tau^{2}]{\simeq}1$. Следует отметить, что этот подход, называемый неопределенной, размытой или плоской априорной информацией, имеет такой же недостаток, как равномерное априорное распределение, а именно неинвариантность к замене параметров.

Вместо этого подхода широкое применение среди неинформативных априорных предположений получила априорная вероятность Джеффри (Jeffreys' prior), определяемая из соотношения
\begin{equation}
\pi(\theta)\propto|I(\theta)|^{1/2},
\end{equation}

где для вектора $\theta$, $|I(\theta)|$ является определителем информационной матрицы $I(\theta)=-E[\partial^{2}L/\partial\theta\partial\theta']$ с $L=\ln {L(y|\theta)}$. Априорная вероятность Джеффри, названная в честь пионера байесовского подхода Гарольда Джеффри, обладает свойством инвариантности к замене параметров или к преобразованию параметров модели, так что в независимости от выбранной параметризации модели будет сформирована одна и та же априорная информация.

С целью проверки правила Джеффри рассмотрим простой случай со скалярным параметром. Допустим, что задано преобразование $\gamma=h(\theta), \partial{L}/\partial\gamma=\partial{L}/\partial\theta{\times}\partial\theta/\partial\gamma$ и

\[
\dfrac{\partial^{2}L}{\partial\gamma^{2}}=\dfrac{\partial^{2}L}{\partial\theta^{2}}\left( \dfrac{\partial\theta}{\partial\gamma}\right)^{2}+\dfrac{\partial{L}}{\partial\theta}\dfrac{\partial^{2}\theta}{\partial\gamma^{2}}
\]

Взяв математическое ожидание логарифма функции правдоподобия используя функцию плотности выборки, и заметив, что $\E[\partial{L}/\partial\theta]=0$, в соответствии со свойством скор-функции, имеем

\[
I(\gamma)=I(\theta)\left(\dfrac{\partial\theta}{\partial\gamma}\right)^{2}.
\]

Следовательно,

\[
|I(\gamma)|^{1/2}=|I(\theta)|^{1/2}\mid\dfrac{\partial\theta}{\partial\gamma}\mid.
\]

В общем случае априорное плотность $\pi(\theta)$ для $\theta$ приводит к априорной плотности $\pi^{*}(\gamma)=\pi(\theta){\times}|d\theta/d\gamma|$. Применяя (13.9), получим $\pi^{*}(\gamma)\propto|I(\theta)|^{1/2}{\times}|d\theta/d\gamma|$ и это  равно $|I(\gamma)|^{1/2}$, что и требовалось доказать.

В качестве примера предположим $y{\sim}N[\mu,\sigma^{2}]$  рассмотрим три случая. В первом случае предположим, что $\mu$ неизвестно, а значение $\sigma^{2}$ известно, тогда мера информативности для $\mu$ равна $I(\mu)=N/\sigma^{2}$, и значение априорная вероятность Джеффри $|I(\gamma)|^{1/2}{\propto}c$ равно константе, поскольку в данном случае $\sigma^{2}$ известна. Заметим, что полученное значение не является собственным. Второй случай, когда $\sigma^{2}$ неизвестно, а значение $\mu$ дано, информационная мера для $\sigma^{2}$ равна $I(\sigma^{2})=N/(2\sigma^{4})$ и априорная вероятность Джеффри $|I(\sigma^{2})|^{1/2}\propto\sigma^{2}$. Третья ситуация, когда оба параметра $\mu$ и $\sigma^{2}$ неизвестны, информационная матрица $|I{(\mu,\sigma^{2})}|=(N/\sigma^{2})(N/2\sigma^{4})=N^{2}/2\sigma^{6}$. Тогда, совместная плотность по правилу Джеффри равна $\pi(\mu,\sigma^{2})\propto\sigma^{-3}$. Заметим, что полученное значение отлично от того, которое можно  получить при применении правила Джеффри  отдельно для $\mu$ и $\sigma^{2}$, поскольку $\pi(\mu)\propto{c}$ и $\pi(\sigma^{2})\propto\sigma^{-2}$ дает $\pi(\mu)\pi(\sigma^{2})\propto\sigma^{-2}$. 

Правило Джеффри  можно рассматривать как метод получения априорного распределения, когда отсутствуют другие способы. Однако, в литературе до сих пор нет согласия, дает ли правило Джеффри неинформативные априорные предположения и в каком смысле неинформативные. Вместе с тем, как следует из рассмотренного выше примера, априорная вероятности Джеффри может быть несобственной, и ее использование может дать несобственное апостериорное распределение.

\subsubsection*{Сопряженное априорное распределение}

Когда указывается собственное априорное распределение, которое может быть как информативным, так и размытым, удобно выбрать функциональную форму априорного распределения так, чтобы аналитическое выражение для апостериорной плотности с учетом данных было представимо в простой форме, как, например (13.7).

Как правило, аналитическое решение наиболее часто существует, если выборка и априорное распределение образуют натуральную сопряженную пару, что означает принадлежность выборки, априорного и апостериорного распределений к одному классу. В этом случае априорное распределение называется сопряженным априорным распределением. В Разделе 13.2.2 рассматривается пример, где из нормальности распределения данных и нормальности априорного распределения для среднего следует нормальность апостериорного распределения.

Семейство экспоненциальных распределений является в принципе единственным, для которого существует естественное сопряженное распределение. Однопараметрическая функция из семейства экспоненциального распределения для одного наблюдения может быть записана как

\begin{equation}
f(y|\theta)=\exp\lbrace{a\theta+b(y)+c(\theta)u(y)}\rbrace,
\end{equation}

\[
{\propto}\exp\lbrace{a\theta+c(\theta)u(y)}\rbrace,
\]

где разные функции $a(\cdot), c(\cdot)$ и $u(\cdot)$ позволяют получить разные плотности в семействе распределений, и $b(\cdot)$ нормализующая константа. Например, при $c(\theta)=\mu/\sigma^{2}, a(\theta)=-\mu^{2}/2\sigma^{2}$, и $u(y)=y$ получим ядро нормального распределения $N[\mu,\sigma^{2}]$ (для известного параметра $\sigma^{2}$). Отметим, что $u(y)=y$ означает принадлежность к семейству линейных экспоненциальных распределений, более подробно рассмотренных в Разделе 5.7.3. В общем случае, если $\theta$ вектор, тогда $c(\theta)u(y)$ заменяется на $c(\theta)'u(y)$, где, как правило, $u(\cdot)$ имеет такую же размерность, как $\theta$.

Для случайной выборки размера $N$, распределение которой относится к экспоненициальному семейству, плотность будет равна

\begin{equation}
L(y|\theta){\propto}\exp\lbrace{Na(\theta)+c(\theta)t(y)}\rbrace,
\end{equation}

где $t(y)=\sum_{i}u(y_i)$. Рассмотрим следующее априорное распределение для $\theta$:

\begin{equation}
\pi(\theta|\beta,\alpha){\propto}\exp\lbrace{\beta{a}(\theta)+\alpha{c}(\theta)}\rbrace,
\end{equation}

где $\alpha$ и $\beta$  --- параметры априорного распределения и функции $a(\cdot)$ и $c(\cdot)$ те же, что и в (13.10). Полученная плотность относится к экспоненциальному семейству для $\theta$ в случае постоянного $\alpha$.

\begin{tabular}{ccc}
Распределение & Выборочная плотность & Сопряженное априорное \\ 
\hline 
Нормальное & $N[\theta,\sigma^2]$& $\theta{\sim}N[\mu,\tau^{2}]$\\ 
Нормальное & $N[\mu,1/\theta^2]$& $\theta{\sim}G[\alpha,\beta]$\\ 
Биномиальное & $B[N,\theta]$& $\theta{\sim}Beta[\alpha,\beta]$\\ 
Пуассона & $P[\theta]$& $\theta{\sim}G[\alpha,\beta]$\\ 
Гамма& $G[\nu,\theta]$& $\theta{\sim}G[\alpha,\beta]$\\ 
Мультиномиальное & $MN[\theta_1,\ldots ,\theta_k]$& $\theta_1,\ldots ,\theta_k{\sim}Dirichlet[\alpha_1,\ldots ,\alpha_k]$\\ 
\hline 
\end{tabular} 

После применения теоремы Байеса и упрощения имеем
\begin{equation}
p (\theta|y){\propto}L(y|\theta)\pi(\theta|\beta,\alpha)
\end{equation}

\[
{\propto}\exp \lbrace(\beta+N)a(\theta)+(\alpha+t(y))c(\theta)\rbrace,
\]

что подтверждает, что апостериорное распределение имеет то же ядро, что и исходное априорное распределение в (13.12). Сравнение апостериорной плотности с выборочной показывает, что априорное распределение рассматривается как источник дополнительных $\beta$ наблюдений $y_p$, к примеру, $t(y_p)=\alpha$.

В таблице 13.2 представлены некоторые виды стандартных сопряженных семейств, соответствующие им плотности даны в Приложении B. Гамма-распределение включает экспоненциальное и хи-квадрат распределения как частные случаи. Отрицательное биномиальное, равномерное распределения и распределение Парето также имеют сопряженные априорные распределения.

Преимущество сопряженного априорного распределения заключается в простоте расчетов и аналитических выводов. Тем не менее, использование сопряженных априорных распределений накладывает дополнительные ограничения, которые в настоящее время, с развитием вычислительных средств, становятся всё менее оправданными.

Другим преимуществом построения апостериорного распределения, относящегося к тому же классу, что и априорное, состоит в том что первое может использоваться как новое (учитывающее специфику данных) априорное распределение для дальнейшего анализа. Если априорное распределение интерпретировать как <<полученную информацию>>, апостериорное распределение одного исследования может использоваться как априорное в следующем.


\subsubsection*{Иерархические априорные распределения}

Иерархические априорные распределения возникают в случае, когда предполагается, что  параметры априорного распределения в свою очередь распределены по некоторому закону. Параметры подобных <<распределений над распределениями>>, называются гиперпараметрами. 

Пусть данные имеют совместную плотность распределения $L(y|\theta)$ как в разделе 13.2.1, но теперь априорное распределение $\theta$ зависит от параметра $\tau$, значение которого случайно. Таким образом, априорное распределение $\theta$ определяется как $\pi(\theta|\tau)$, где, в свою очередь, параметры $\tau$ имеют априорное распределение $\pi(\tau)$. Совместное априорное распределение выражается как $\pi(\theta,\tau)=\pi(\theta|\tau)\pi(\tau)$ и, согласно байесовскому правилу, совместное апостериорное распределение может быть  представлено как

\begin{equation}
p(\theta|y){\propto}L(y|\theta)\pi(\theta|\beta,\alpha)
\end{equation}

\[
{\propto}\exp\lbrace{(\beta+N)a(\theta)+(\alpha+t(y))c(\theta)}\rbrace,
\]

\[
p(\theta,\tau|y){\propto}L(y|\theta)\pi(\theta|\tau)\pi(\tau).
\]

Как правило, в литературе основное внимание сосредоточено на построении частной апостериорной плотности $\theta$, которая рассчитывается через интегрирование совместной апостериорной плотности по $\tau$. Параметры априорного распределения $\pi(\tau)$ называются гипермараметрами. Как вариант, эти параметры могут быть параметрами для следующего шага, следовательно выражение для совместной плотности можно записать $\pi(\theta|\tau)\pi(\tau|\phi)\pi(\phi)$ и так далее. Новые вычислительные методы байесовского анализа, в частности, алгоритм Гиббса, хорошо подходят для  иерархического априорного распределения поскольку оно имеет рекурсивную структуру.

Иерархическое априорное распределение можно рассматривать как аналог классических моделей со случайными коэффициентами. Например, для независимых одинаково распределенных дискретных данных можно предположить, что $y_i{\sim}P(\theta_i)$, где параметр распределения Пуассона является случайным. Целесообраpно предположить, что $\theta_i$ имеет распределение, сопряженное с пуассоновским, т.е. гамма-распределение, $\theta_i{\sim}G[\alpha,\beta]$. В классическом подходе $\alpha$ и $\beta$ оцениваются методом максимального правдоподобия. В неиерархическом байесовском методе значения параметров $\alpha$ и $\beta$ заранее определены и используются для построения апостериорного распределения $\theta_i$. В иерархической байесовской модели задается априорное распределение параметров $\alpha$ и $\beta$, например, сопряженное гамма-распределение, и на первом шаге строится совместное апостериорное распределение для $\theta_i$, $\alpha$ и $\beta$ для дальнейшего построения частного апостериорного распределения $\theta_i$.

Иерархическое априорное распределение естественным образом появляется при рассмотрении иерархических моделей, также известных как многоуровневые. Многоуровневые модели широко используются в классических методах, но с использованием специального программного обеспечения (Брик и Рауденбуш, 1992, 2002). Одной из первых по иерархическим моделям была написана работа Линдли и Смита (1972). Авторы рассматривают иерархические модели регрессии с помощью байесовского подхода. Потребность в иерархическом моделировании появляется, если данные разбиты на страты, группы или слои и параметры зависимости могут варьироваться по группам. Например, массив данных состоит из оценок учащихся разных школ. Для моделирования оценок за тест могут потребоваться индивидуальные характеристики, априори отличающиеся для каждого ученика, характеристики класса, зависящие от принадлежности к классу и школьные характеристики, меняющиеся от школы к школе. Поскольку эти данные предполагают кластеризацию наблюдений, иерархическое моделирование также рассмотрено в главе 24. Кроме того, многоуровневые модели и модели панельных данных со случайным эффектом имеют много общего.


Например, предположим, что все данные разбиты н $J$ групп и для каждой группы среднее значение $y$ уникально. Для индивида $i$ из группы $j$ предположим, что $y_{ij}{\sim}N[\theta_j,\sigma^2]$, где для простоты дисперсия $\sigma^2$ известна. Тогда среднее значение для $j$-ой группы равно $\overline{y}_j{\sim}N[\theta_j,\sigma^{2}/N_j]$, где $N_j$ обозначает количество индивидов в группе; предполагается, что наблюдения не зависят друг от друга. В иерархической модели определяется априорное распределение $\theta_i$, например, $\theta_{j}{\sim}N[\mu, \tau^2]$, где для параметров более высокого порядка $\mu$ и $\tau^2$ также задано априорное распределение.


\subsubsection*{Анализ чувствительности}


Частотный подход  допускает оценку модели при разном количестве априорных ограничений, накладываемых на модель. Например, можно оценить модель с одним или несколькими ограничениями, и далее можно сравнить результаты, чтобы определить чувствительность оценок к изменению априорных ограничений.


Аналогичный подход используется в байесовском анализе. Не обязательно считать, что априорное распределение является верным, можно провести анализ чувствительности, прослеживая изменения апостериорного распределения в зависимости от изменений в априорном. Кроме того, можно следить за тем как меняется апостериорное распределение, меняя предположения о процессе порождающем данные. 

\subsection{Апостериорное распределение: плотность и иные характеристики}

Байесовский анализ построен на апостериорном распределении. Для удобства, как правило, используют только общую статистику, например, моменты апостериорного распределения, квантили или частные распределения составляющих компонент вектора параметров $\theta$. Однако, апостериорное распределение также используется для построения прогнозов и вероятностных утверждений, о чем подробно пойдет речь в этом разделе, и для сравнения моделей, которое обсуждается в разделе 13.8.

Квантили некоторых уровней занимают особое место в байесовском анализе.

\subsubsection*{Частное апостериорное распределение}

В общем случае $\theta$ является многомерным вектором, т.е. $\theta'=(\theta_1,\ldots ,\theta_q)$ и предметом исследования может быть апостериорное распределения отдельных компонент $\theta$. Плотность частного апостериорного распределения $k$-того параметра $\theta_k$ рассчитывается путем интегрирования совместной плотности апостериорного распределения остальных $(q-1)$ компонента вектора $\theta$. Частная функция плотности обозначается через $p(\theta_k|y)$ и рассчитывается $(q-1)$-кратным интегрированием.

\begin{equation}
p(\theta|k|y)=\int{p(\theta_1,\ldots ,\theta_p|y)d\theta_1..d\theta_{k-1}d\theta_{k+1}..d\theta_q}
\end{equation}

\[
=\int{p(\theta|y)d\theta_{-k}},
\]

где в компактной записи во второй строке через $\theta_{-k}$ обозначены все компоненты вектора $\theta$, кроме $\theta_k$. Частное апостериорное распределение, как правило, асиметрично и не обязательно унимодально, в то время как асимптотическое нормальное распределение для классических оценок симметрично и унимодально. В связи с этим полезным является графический анализ апостериорного распределения, особенно в тех случаях, когда оно существенно отличается от симметричного унимодального.

\subsubsection*{Моменты апостериорного распределения}

Классическое представление результатов оценивания регрессии включает значения оценок параметров и стандартные ошибки. При байесовском оценивании также могут быть рассчитаны среднее или медиана и стандартное отклонение частного апостериорного распределения для каждого параметра.

\subsubsection*{Точечные оценки}

В рамках классического подхода задача состоит в получении хорошей точечной оценки неизвестного  параметра $\theta_0$, от которого зависит процесс порождающий данные $f(y|\theta_0)$. В байесовском анализе, в отличие от классического, задача состоит в нахождении закона распределения $\theta$, который определяется как  $\theta_0$, так и априорными предположениями о  $\theta_0$.

Таким образом, в байесовском подходе точечной оценке уделяется намного меньше внимания. Для удобства среднее или медиана апостериорного распределения принимаются за точечные оценки. Оптимальная точечная оценка параметра может быть рассчитана на основе функции потерь (см. Раздел 13.2.7).

\subsubsection*{Апостериорные доверительные интервалы}

Найденное апостериорное распределение может быть использовано для построения вероятностных утверждений аналогично частотному подходу. В частности, по методу Байеса могут быть построены доверительные интервалы и области. 

Для $k$-го параметра $100(1-\alpha)$-процентным апостериорным доверительным интервалом $R(\theta_k)$  является любой интервал, в который значения $\theta_k$ попадают  с апостериорной вероятностью $\alpha$, т.е.

\begin{equation}
1-\alpha=\Pr[\theta_k{\epsilon}R(\theta_k)|y]=\int_{R(\theta_k)p(\theta_k|y)d\theta}.
\end{equation}

Существует множество областей, которые соответствуют этой вероятности. Наиболее простой апостериорный доверительный интервал --- между квантилями $\alpha/2$ и $1-{\alpha}/2$, т.е. между 2.5 и 97.5 перцентилями. Более сложным является построение интервала наивысшей плотности апостериорного распределения (highest probability density, HPD), который удовлетворяет (13.15) и   дополнительному условию, согласно которому ни в одной из точек интервала $R(\theta)$ значение плотности распределения не ниже, чем ни в одной из точек за его пределами. Этот интервал необязательно должен быть непрерывныным, если апостериорное распределение полимодально, и в общем случаем он не совпадает с простым интервалом, исключение составляет случай симметричнго унимодального апостериорного распределения.

Построенные интервалы могут быть представлены как области. Например, $100(\alpha)$-процентная область наивысшей плотности апостериорного распределения $R(theta)$ задается соотношением 

\begin{equation}
1-\alpha=\Pr[\theta{\epsilon}R(\theta)|y]=\int R(\theta) p(\theta|y)d\theta.
\end{equation}

Преимущество байесовского подхода заключается в том, что апостериорные интервалы намного проще интерпретировать, чем доверительные интервалы в частотном анализе. Если границы $95$-ти процентного апостериорного интервала для $\theta_k$ равны $(1,4)$, можно утверждать, что значение $\theta_k$ лежит между 1 и 4 с вероятностью 0.95. В случае частотного анализа для $95$-ти процентного доверительного интервала для $\theta_k$ с границами $(1,4)$ можно лишь утверждать, что в случае построения множества доверительных интервалов для различных выборок $95$ процентов этих доверительных интервалов будут включать истинные значения $\theta_k$.

\subsubsection*{Тестирование гипотез}

Тестированию гипотез уделяется незначительное внимание в теме байесовских методов. Как отмечалось при рассмотрении точечных оценок, расчет истинного значения $\theta_0$ не является основной целью байесовских методов. Основной интерес составляет определении области значений, которые может принять параметр $\theta$ при имеющихся наблюдениях и заданном априорном распределении. Для сравнения моделей см. Раздел 13.8.

\subsubsection*{Условная плотность апостериорного распределения}

Условная плотность апостериорного распределения параметра $\theta_k$, при заданных значениях $\theta_j$, может быть рассчитана на основе сведений о совместном и частном апостериорных распределениях из соотношения

\begin{equation}
p(\theta_k|\theta_j,\theta_j \in \theta_{-k},y)=\dfrac{p(\theta_k,\theta_j|y)}{p(\theta_j|y)}.
\end{equation}

Особое внимание уделяется множеству $q$ условных плотностей распределения $p(\theta_k|\theta_{-k}), k=1,\ldots ,q$, также известных как множество полных условных распределений. Эти распределения играют важную роль для современных вычислительных методов, применяемых для расчета параметров совместного апостериорного распределения, которое будет обсуждаться далее.

Определения частной и условной апостериорной плотностей в (13.15) и (13.17) могут быть расширены для блоков параметров. 

\subsubsection*{Предельная функция правдоподобия}

Частная вероятность или частная функция правдоподобия равна знаменателю в формуле Байеса и определяется по следующей формуле: 

\begin{equation}
f(y)=\int{L(y|\theta)}\pi(\theta)d\theta.
\end{equation}

Значение этого выражения равно условному математическому ожиданию функции правдоподобия, $\E[L(y|\theta)]$ по априорной плотности. Частная функция правдоподобия лежит в основе байесовских выводов (см Раздел 13.8), поскольку эта функция включает информацию о носителе априорного распределения.

\subsubsection*{Апостериорная плотность для прогнозов}

Рассмотрим точечный прогноз вне выборки $y^p$, плотность распределения которого равна $f(y^p|\theta)$, где значение $\theta$ неизвестно. Значение предиктивной апостериорной плотности $y^p$ определяется взвешиванием плотности распределения с помощью апостериорного  распределения $\theta$

\begin{equation}
f^{p}(y^p)=\int{f(y^p|\theta)p(\theta|y)d\theta}.
\end{equation}

Если  функция правдоподобия, как регрессионная модель, содержит регрессоры, то они также учитываются при расчете условных плотностей. 

\subsection{Поведение апостериорной плотности на больших выборках}

С ростом выборки влияние априорного распределения на апостериорное, даже при условии информативности первого, снижается, как это показано в примере 13.2.2. Этот факт позволяет утверждать, что асимптотически информация выборки доминирует над априорной информацией или, другими словами, вес априорного распределения  стремится к нулю с ростом выборки.

Поскольку обращаться с апостериорным распределением трудно, существенным является вопрос его асимптотической аппроксимации, поскольку такая аппроксимация может использоваться вместо истинного апостериорного распределения конечной выборки. Приближенные значения достаточно легко получить, поскольку апостериорное распределение асимптотически сходится к функции правдоподобия. Для более подробного изучения см. Гельман и др. (1995).

Для простоты предположим, что наблюдения независимы и одинаково распределены. Тогда логарифм апостериорного распределения представим в виде

\begin{equation}
\sum^{N}_{i=1}\ln {p(\theta|y_i)}=\ln \pi(\theta)+\sum^{N}_{i=1}\ln {f(y_i,\theta)}.
\end{equation}

При таком представлении очевидно, что для больших выборок апостериорное распределение в большей степени определяется вкладом функции правдоподобия, поскольку влияние априорного распределения на апостериорное фиксировано, в то время как влияние выборочного распределения на апостериорное растет с ростом размера выборки $N$. 

Предположим, что апостериорная функция $p(\theta|y)$ унимодальна и асимптотически симметрична. Рассмотрим асимптотические свойства моды апостериорного распределения, $\hat{\theta}$, которая является как локальным, так и глобальным максимумом функции плотности апостериорного распределения.

Для изучения состоятельности оценки $\hat{\theta}$ заметим, что значение апостериорной моды сходится к ММП оценке при $N{\rightarrow}\infty$, поскольку  второе слагаемое в (13.20) доминирует. Таким образом, мода апостериорного распределения состоятельна, если состоятельна ММП-оценка. Тогда, $\hat{\theta}{\rightarrow}^{p}\theta_0$ если  процесс порождающий данные $y$ имеет плотность $f(y|\theta_0)$ и выполняются стандартные условия регулярности для ММП-оценок. 

Для того, чтобы получить асимптотическое распределение $\hat{\theta}$, рассмотрим разложение в ряд Тейлора второго порядка логарифмической апостериорной плотности в окрестности моды $\hat{\theta}$. Тогда

\begin{equation}
\ln {p(\theta|y){\simeq}}\ln {p(\hat{\theta}|y)}+\dfrac{1}{2}(\theta-\hat{\theta})'\left[\dfrac{\partial^{2}\ln {p(\theta|y)}}{\partial\theta\partial\theta'}{\mid}_{\theta=\hat{\theta}}\right](\theta-\hat{\theta}), 
\end{equation}

Так как для моды апостериорного распределения выполняется условие $\partial{\ln }p(\theta|y)/\partial\theta=0$ и асимптотически  производные третьего и высшего порядков по $\theta$ игнорируются, выражение может быть упрощено. Обозначим через

\[
I(\hat(\theta))=-\dfrac{\partial^{2}\ln {p(\theta|y)}}{\partial\theta\partial\theta'}|_{\theta=\hat{\theta}}
\]

полученную информацию, основанную на апостериорной плотности $\ln {p(\theta|y)}$ в точке моды апостериорного распределения. Тогда потенцирование (13.21) даст

\[
p(\theta|y){\propto}\exp\left(-\dfrac{1}{2}(\theta-\hat{\theta})'I(\hat{\theta})(\theta-\hat{\theta})\right), 
\]

и это выражение задает ядро многомерного нормального распределения со средним $\hat{\theta}$ и ковариационной матрицей $I(\hat{\theta})^{-1}$. Из  этого следует, что апостериорное распределение имеет вид

\[
\theta|y \overset{a}{\sim} N[\hat{\theta},I(\hat{\theta})^{-1}].
\]

С ростом размера выборки $N$ составляющая правдоподобия становится доминирующей в апостериорном распределении и влияние априорного распределения становится незначимым. В таком случае, мода оценки $\hat{\theta}$ может быть заменена ММП-оценкой, которая является модой правдоподобия. В результате получим так называемую байесовскую центральную предельную теорему (Гамерман, 1997). Асимптотически, результаты частотного и байесовского выводов будут основаны на одном и тоже предельном многомерном нормальном распределении, и, таким образом, между ними не должно возникать существенного несоответствия. 

В литературе этот результат был обозначен как теорема Бернштейна-фон Мизеса; для более подробного изучения трех компонент этой теоремы, см. Трейн (2003, глава 12). Эти компоненты включают (1) вывод о том, что апостериорное среднее сходится по вероятности к  оценке максимального правдоподобия, (2) онон имеет нормальное распределение и (3) предельное распределение апостериорного среднего совпадает с распределением  оценки максимального правдоподобия. Эти результаты неявно фигурируют в байесовской центральной предельной теореме. Эта теорема является ключевой при использовании принципов правдоподобия для оценивания и формулирования выводов. Вся сила байесовской центральной предельной теоремы в полном объеме будет видна после обсуждения применения численных методов для аппроксимации апостериорного распределение. 

Следует ли из приведенных аргументов, что применение байесовского метода и метода максимального  правдоподобия приводит к существенно аналогичным результатам? Определяется ли выбор между двумя подходами удобством расчетов? Точные ответы на поставленные вопросы отсутствуют. Однако в литературе приводится ряд примеров, в которых не только показано, что эти методы дают схожие результаты, но также, что байесовские методы как правило более эффективны с точки зрения трудоемкости вычислений.

\subsection{Принятие решений в байесовском подходе}

При данном апостериорном распределении $p(\theta|y)$, какая из точечных оценок $\theta$ должна быть выбрана? Этот вопрос рассмотрен в Разделе 4.2 с точки зрения выбора наилучшего прогнозного значения $y$, например, с помощью  квадратичной функции потерь. В данном разделе рассматривается наилучшая оценка параметра $\theta$, с точки зрения, например, с помощью квадратичной функции потерь.

Допустим $L(\theta,\hat{\theta})$ --- заданная функция потерь, где $\hat{\theta}$ оценка неизвестного параметра $\theta$. Величина потери неизвестна, поскольку зависит от неизвестного параметра $\theta$. Тем не менее, возможно найти ожидаемое значение потери, поскольку при байесовском анализе, в отличии от классического подхода, известно распределение $\theta$. Оптимальная оценка, $\hat{\theta}_{OPT}$, это оценка параметра $\hat{\theta}$, которая минимизирует величину ожидаемых апостериорных потерь

\begin{equation}
\underset{\hat{\theta}}{\min}E[L(\theta,\hat{\theta})]=\underset{\hat{\theta}}{\min}\int{L}(\theta,\hat{\theta})p(\theta|y)d\theta,
\end{equation}

Потери, которые зависят от различных значений $(\theta,\hat{\theta})$ взвешиваются по апостериорной вероятности  $p(\theta|y)$.

Можно показать, что апостериорное среднее является оптимальной оценкой для квадратичной функции потерь, $L(\theta,\hat{\theta})=(\theta-\hat{\theta})'(\theta-\hat{\theta})$. Если использовать абсолютное значение ошибки, $L(\theta,\hat{\theta})=|\theta-\hat{\theta}|$, то оптимальной оценкой будет медиана апостериорного распределения. Поскольку апостериорное распределение известно эти оптимальные оценки можно рассчитать аналитически или численно.

При выполнении соответствующих предпосылок, можно показать, что минимизация ожидаемой апостериорной ошибки  эквивалентна минимизации ожидаемого апостериорного риска. Функция риска усредняет возможные потери по гипотетической выборке $y$ из генеральной совокупности, т.е. 

\[
R(\theta,\hat{\theta})=\int{L}(\theta,\hat{\theta})f(y|\theta)dy.
\]

Для того, чтобы избежать путаницы с обозначением для функции потерь и функции правдоподобия, в данном и последующем блоках уравнений, $f(y|\theta)$ используется для обозначения функции правдоподобия $L((y|\theta)$. Ожидаемый апостериорный риск  усредняет риск для различных параметров $\theta{\epsilon}\Theta$ путем присваивания каждому значению веса в соответствии со значением плотности апостериорного распределения, и 

\begin{equation}
E[R(\theta,\hat{\theta})]=\int_{\Theta}\left\lbrace\int{L}(\theta,\hat{\theta})f(y|\theta)dy \right\rbrace{p}(\theta|y)d\theta
\end{equation}

\[
=\int{ \left\lbrace\int_{\Theta}L(\theta,\hat{\theta})p(\theta|y)d\theta\right\rbrace}f(y|\theta)dy
\]

\[
=\int{E[L(\theta,\hat{\theta})]f(y|\theta)dy},
\]

где в первом выражении границы внешнего интеграла равны области определения параметра $\theta$, во втором равенстве порядок интегрирования меняется и в третьей строке записано итоговое выражение. Для данных преобразований предполагаются выполненными соответствующие  ограничения на $L(\theta,\hat{\theta})$ и $p(\theta,y)$. Например, $p(\theta|y)$ должна быть собственной функцией плотности и функция потерь должна быть интегрируема. Таким образом, значение ожидаемого риска будет ограничено и её минимизация --- это корректная операция.

Мы получили широко известный и важный результат, что байесовская оценка является допустимой в том смысле, что она минимизирует  ожидаемый риск для некоторой функции потерь.

\section{Байесовский анализ линейной регрессии}

Поскольку анализ линейных регрессий --- знакомая тема, он является полезным началом перехода к классу нелинейных моделей. Предположим, что данные порождены стандартной линейной регрессионной моделью

\[
y=X\beta+u,
\]

где $X$ обозначает $N \times K$ матрицу полного ранга слабо экзогенных регрессоров. Кроме того, делается предпосылка о независимости, гомоскедастичности и нормальном распределении ошибок, $u \sim N[0,\sigma^{2}I_{N}]$. Таким образом, выборочная условная плотность равна $y|X, \beta, \sigma^{2} \sim N[X\beta,\sigma^{2}I_{N}]$. Мы следуем изложению Зеллнера (1971).

Далее рассмотрим  неинформативное и информативное априорное распределение. В обоих случаях после длинных выкладок возможно получить аналитическое выражение для апостериорного распределения. Для информативного  априорного распределения моменты апостериорного распределения являются взвешенной функцией от выборочного и априорного среднего.

В последующих разделах представлено описание методов для более сложных моделей, тем не менее, анализ этих методов упрощается, если результаты, полученные в этом разделе, могут быть частично применены к некоторым компонентам модели.

\subsection{Неинформативное априорное распределение}

В качестве неинформативного априорного распределения мы используем априорную вероятность Джеффри. Из Раздела 13.2.4 следует, что для $y{\sim}N[\mu,\sigma^{2}]$ априорная плотность для $\mu$ (при заданном значении $\sigma^{2}$) постоянна, в то время, как априорная плотность для $\sigma^{2}$ (при заданном значении $\mu$) пропорциональна $\sigma^{2}$. При рассмотрении регрессии постоянной плотность будет для $\beta_j, j=1,\ldots ,K$, т.е. $\pi(\beta_j)\propto{c}$ и априорная плотность для $\sigma^{2}$ равна $\pi(\sigma^{2})\propto{1/\sigma^{2}}$. Априорная информация о $\beta_j$ рассматривает значения параметра как равновероятные, в тоже время считается, что малые значения $\sigma^{2}$ более вероятны, чем большие. Предполагая независимость $\beta$ и $\sigma^{2}$ совместная априорная плотность равна

\[
\pi{(\beta,\sigma^{2})\propto{1/\sigma^{2}}}.
\]

Функция правдоподобия равна

\begin{equation}
L(\beta,\sigma^{2}|y,X)=(2\pi\sigma^{2})^{-N/2}\exp\left\lbrace-\dfrac{1}{2\sigma^{2}}(y-X\beta)'(y-X\beta)\right\rbrace
\end{equation}

\[
{\propto}(\sigma^{2})^{-N/2}\exp\left(-\dfrac{1}{2\sigma^{2}}\lbrace{\hat{u}'\hat{u}+(\beta-\hat{\beta})'X'X(\beta-\hat{\beta})}\rbrace\right) 
\]

\[
{\propto}(\sigma^{2})^{-N/2}\exp\left(-\dfrac{1}{2\sigma^{2}}(N-K)s^2+(\beta-\hat{\beta})'X'X(\beta-\hat{\beta}))\right),
\]

где $\hat{\beta}=(X'X)^{-1}X'y$ и $\hat{u}=y-X\hat{\beta}$; для перехода во второй строке используется равенство $y-X\beta=\hat{u}-X(\beta-\hat{\beta})$ и $X'\hat{u}=0$; и для перехода в третьей строке --- $s^2=\hat{u}'\hat{u}/(N-K)$.

Совмещая функцию  правдоподобия, (13.25), и априорную функцию плотности, получим апостериорную плотность

\begin{multline}
p(\beta,\sigma^{2}|y,X) \\
\propto\left(\frac{1}{\sigma^2}\right)^{N/2}\exp\left(-\dfrac{1}{2\sigma^{2}}\lbrace(N-K)s^2+(\beta-\hat{\beta})'X'X(\beta-\hat{\beta}))\rbrace\right)\dfrac{1}{\sigma^2} \\
\propto\left(\frac{1}{\sigma^2}\right)^{N/2+1}\exp\left(-\dfrac{1}{2\sigma^{2}}\lbrace(N-K)s^2+(\beta-\hat{\beta})'X'X(\beta-\hat{\beta}))\rbrace\right) \\
\propto \left(\frac{1}{\sigma^2}\right)^{K/2}\exp\left(-\dfrac{1}{2}(\beta-\hat{\beta})'(\sigma^{2}(X'X)^{-1})^{-1}(\beta-\hat{\beta})\right)  \\ 
\times \left(\dfrac{1}{\sigma^2}\right)^{(N-K)/2+1}\exp\left(-\dfrac{(N-K)s^2}{2\sigma^2}\right)  .
\end{multline}

Условное апостериорное распределение $p(\beta|\sigma^2,y,X)$ параметра $\beta$, при заданных значениях $\sigma^2$ и данных $y,X$, является $K$-мерным нормальным со средним $\hat{\beta}$ и дисперсией $\sigma^{2}(X'X)^{-1}$, поскольку $\beta$ присутствует только в первой строке последнего выражения. Условная апостериорная плотность распределения $\sigma^2$ при заданном значении $\beta$ сложнее рассчитать, поскольку $\sigma^2$ присутствует в обеих строках.

Частная апостериорная плотность распределения $\beta$, значения которой получены интегрированием по $\sigma^2$, имеет большее значение для апостериорных выводов о  $\beta$. Проинтегрируем вторую строчку (13.26), используя замену переменной $z=1/\sigma^2$ и равенство $\int^{\infty}_{0}z^{c}\exp(-az)dz=\Gamma(c+1)/a^{c+1}$ верное для произвольных констант $a>0, c>-1$, где $c=N/2+1$ и $a=\lbrace\cdot\rbrace$ --- длинное выражение в фигурных скобках. В результате получим ядро функции частного апостериорного распределения

\begin{equation}
p(\beta|y,X){\propto}\lbrace(N-K)s^2+(\beta-\hat{\beta})'X'X(\beta-\hat{\beta}))\rbrace^{-N/2}
\end{equation}

\[
{\propto}\lbrace{1+(\beta-\hat{\beta})'(s^{2}(N-K)(X'X)^{-1})^{-1}(\beta-\hat{\beta})}\rbrace^{-(N-K+K)/2},
\]
где из Раздела 13.3.5 следует, что это выражение является ядром многомерного t-распределения Стьюдента с центром в точке $\hat{\beta}$ и $N-K$ степенями свободы и ковариационной матрицей $s^2(X'X)^{-1}$ домноженной на $(N-K)/(N-K-2)$. Таким образом,

\begin{equation}
\beta{\sim}t_{k}(\hat{\beta},s^2(X'X)^{-1}).
\end{equation}

Отдельный элемент вектора $\beta$ имеет одномерное t-распределение Стьюдента.

Частная функция апостериорной плотности для $\sigma^2$ получается легче. Интегрируем последнее выражение в (13.26) по $\beta$, заметим, что $\beta$ присутствует только в первой строке. В результате получаем ядро нормальной плотности распределения $N[\hat{\beta},\sigma^2(X'X)^{-1}]$ и интеграл плотности равен 1. Следовательно, функция для частного апостериорного распределения $\sigma^2$ равна

\begin{equation}
p(\sigma^2|y,X){\propto}(\sigma^2)^{-(N-K+1)/2}\exp\left(-\dfrac{(N-K)s^2}{2\sigma^2}\right). 
\end{equation}

Это выражение является ядром для обратного к квадратному корню из гамма распределения. Иными словами, это плотность случайной величины, обратной к квадратному корню из гамма-распределенной случайной величины с $(N-K)$ степенями свободы. Этот результат идентичен результату применения частотного подхода к построению распределения $\hat{\beta}$.

Для нормальной линейной регрессии, применение байесовского анализа с неинформативным  априорным распределением на конечных выборках дает качественно схожие выводы со стандартным частотным анализом. Условное распределение параметра $\beta$ по $\sigma^2$ равно $N[\hat{\beta},\sigma^{2}(X'X)^{-1}]$, а безусловное распределение $\beta$ есть многомерное $t$-распределение. 

Тем не менее, интерпретация результатов довольно отличается, поскольку здесь мы видим распределение  неизвестного параметра $\beta$ со средним значением $\hat{\beta}$, а не оценки $\hat{beta}$ с неизвестным средним $\beta$. Например, $95$-ти процентный интервал наивысшей плотности для $\beta_j$ равен $\hat{\beta}_j{\pm}t_{.025,N-K}{\times}se[\hat{\beta_j}]$, где $se[\hat{\beta_j}]=(s^{2}(X'X)^{jj})^{1/2}$. Из Раздела 13.2.5 следует, что $\beta_j$ принадлежит интервалу наивысшей плотности с апостериорной вероятностью 0.95.

\subsection{Информативное априорное распределение}

Наиболее понятным байесовский анализ модели нормальной линейной регрессии с информативным априорным распределением будет, когда используется предпосылка о независимости сопряженных распределений $\beta$ и $\sigma$. Из Раздела 13.2.4 сопряженное распределение для $\beta$ является нормальным, а для $1/\sigma^2$ -- гамма. Мы получаем плотность нормального-гамма распределения

\begin{equation}
\pi(\beta,1/\sigma^2)=\pi_{N}(\beta|1/\sigma^2)\pi_{\gamma}(1/\sigma^2),
\end{equation}

где $\pi_{N}(\beta|1/\sigma^2)$ --- это плотность нормального распределения $N[\beta_0,\sigma^2{\Omega}^{-1}_0]$, с известными значениями $\beta_0$ и $\Omega_0$ и ядром 

\begin{equation}
\pi_{N}(\beta|1/\sigma^2){\propto}\sigma^{-K}\exp[-\dfrac{(\beta-\beta_0)'{\Omega_0}(\beta-\beta_0)}{2\sigma^2}],
\end{equation}

и $\pi_{\gamma}(1/\sigma^2)$ имеет плотность $G[\nu_0,s^2_0]$, где $\nu_0$ и $s^2_0$ известны и 

\begin{equation}
\pi_{\gamma}(1/\sigma^2)=\sigma^{-(\nu_0+1)}\exp\left[-\dfrac{\nu_{0}s^2_0}{2\sigma^2}\right].
\end{equation}

Отметим, что априорная плотность распределения для параметра  $\beta$ зависит от плотности параметра масштаба $\sigma$. Это имеет смысл, поскольку $\sigma$ отражает масштаб измерения $y$ и, таким образом, должно влиять на значение $\beta$. При заданных априорной плотности и  функции правдоподобия в (13.25), апостериорная плотность имеет нормальный-гамма вид. После математических преобразований получим, что:

\begin{multline}
p(\beta,1/\sigma \mid y, X) \propto (\sigma^2)^{-N/2} 
\exp\left[-\frac{s^2(N-K)}{2\sigma^2}\right]
\exp\left[-\frac{(\beta-\hat{\beta})'X'X(\beta-\hat{\beta})}{2\sigma^2}\right] \\
\times (\sigma^2)^{-K/2} 
\exp\left[-\frac{(\beta-\beta_0)'\Omega_0(\beta-\beta_0)}{2\sigma^2}\right] \\
\times (\sigma^2)^{-\nu_0/2-1} 
\exp\left[-\frac{\nu_0 s_0^2}{2\sigma^2}\right]\\
\propto (\sigma^2)^{(\nu_0/2+N)/2-1-K/2} 
\exp\left[-\frac{s_1^2}{2\sigma^2}\right]\\
\times \exp\left[-\frac{(\beta-\bar{\beta})'\Omega_1(\beta-\bar{\beta})}{2\sigma^2}\right]
\end{multline}
где $\bar{\beta}$ и $\Omega_1^{-1}$ обозначают апостериорное среднее и дисперсию $\beta$, и $s_1^2$ обозначает апостериорное среднее $\sigma^2$:

\begin{multline}
\bar{\beta}=(\Omega_0+X'X)^{-1}(\Omega_0\beta_0+X'X\hat{\beta}) \\
\Omega_1=(\Omega_0+X'X) \\
s_1^2=s_0^2+\hat{u}'\hat{u}+(\beta-\bar{\beta})'[\Omega_0^{-1}+(X'X)^{-1}](\beta-\bar{\beta})
\end{multline}

Апостериорное среднее $\bar{\beta}$ получается путем использование матричного варианта дополнения до полного квадрата. А именно, для векторов $\beta$, $\bar{\beta}$, $\beta_0$ и $\hat{\beta}$ размера $K\times 1$ и симметричных квадратных матриц $A$ и $B$ размера $K\times K$ можно показать, что

\begin{multline}
(\beta-\beta_0)'A(\beta-\beta_0)+(\beta-\hat{\beta})'B(\beta-\hat{\beta}) \\
= (\beta-\bar{\beta})'(A+B)(\beta-\bar{\beta})+(\beta_0-\bar{\beta})'AB(A+B)^{-1}(\beta_0-\bar{\beta})
\end{multline}
где $\bar{\beta}=(A+B)^{-1}(A\beta_0+B\hat{\beta})$.

Совместная частная плотность апостериорного распределения $\beta$ и $\sigma^2$ имеет такую же нормальную-гамма форму как и априорное распределение. 

Условное апостериорное распределение $\beta$ при заданном значении $\sigma^2$ имеет среднее $\overline{\beta}$, средневзвешенное матричное априорного среднего $\beta_0$ и выборочного среднего $\hat{\beta}$.

В общем случае, применение сопряженного априорного распределения математически эквивалентно увеличению количества данных за счет выборки с аналогичным распределением. В этом случае, априорная плотность нормального гамма распределения эквивалентна дополнительной выборке с оценкой параметра $\beta_0$, матрицей $X'X$ равной $\Omega_0$, количеством степеней свободы $\nu_0$ и суммой квадратов ошибок $\nu_{0}s^2_0$. Поскольку значение матрицы $\Omega_0$ фиксировано, $\Omega_0/N{\rightarrow}0$ при $N{\rightarrow}\infty$, в то же время $X'X/N$ сходится к матрице констант. Таким образом, $\overline{\beta} \to\hat{\beta}$, подтверждает что при большой выборке ММП-оценка равна апостериорному среднему. Значение апостериорной дисперсии $\Omega^{-1}_1$ пропорционально $(\Omega_0+X'X)^{-1}$. Более подробно данный вопрос изложен у Лимера (1978). 

Частная апостериорная плотность $\beta$ может быть получена интегрированием совместной плотности по $\sigma^2$. Это даст нам выражение

\begin{equation}
p(\beta|y,X){\propto}\left[s^2_1+(\beta-\overline{\beta})'(\Omega_0+X'X)(\beta-\overline{\beta})\right]^{-(\nu_1+K/2)};
\end{equation}

таким образом частная апостериорная плотность является многомерным $t$-распределением Стьюдента с центром точке $\overline{\beta}$, а не в $\hat{\beta}$ как в случае с неинформативным априорным распределением.

Поскольку сопряженная априорная плотность эквивалентна наличию дополнительной выборки, использование выборочной и априорной информации происходит симметрично, не смотря на то, что информация, полученная из двух источников, может противоречить друг другу. 
Таким образом, за удобство использования сопряженного распределения нужно платить. 
В случае, если априорная и выборочная информация противоречат друг другу, можно предположить, что апостериорное распределение будет бимодальным с модальными значениями равным выборочному и априорному средним, соответственно. 
Одна из возможностей реализовать эту особенность --- предположить, что $\beta$ имеет априорное многомерное $t$-распределение Стьюдента независимое от $1/\sigma^2$ и $1/\sigma^2$ имеет априорное гамма распределение независимое от $X\beta$. Эта априорное распределение называется <<априорным распределение Дики>> (Лимер, 1978, стр. 79). При данной предпосылке частное  апостериорное распределение получается путем перемножения двух плотностей многомерного $t$-распределения Стьюдента; это произведение может также быть выражено как смесь двух  $t$-распределений. Это распределение может обладать свойством бимодальности. Лимер (1978) приводит более подробный анализ этой ситуации.

\subsection{Смешенное оценивание}

В данном разделе рассмотрено как можно представить байесовский анализ линейной регрессии через частотный подход к вероятности. 

В частотном анализе часто учитывают априорную информацию с помощью ограничений в виде равенств. Такую априорную информацию можно считать предельным случаем байесовского анализа, если значения параметров дисперсии устремить к бесконечности. В частотном анализе также может быть использована недетерминистическая априорная информация с помощью  смешенного оценивания. Применение этого анализа не требует сложной математики и содержит интуитивное понимание того, как байесовский подход может объединять в себе априорную и выборочную информацию. 

Рассмотрим нормальную линейную регрессионную модель. Предположим, что в соответствии с априорной информацией регрессионный параметр имеет нормальное распределение, $\beta \sim N[0,\sigma^2_{\nu}I_K]$, обобщение на случай ненулевого математического ожидания не связано с существенными трудностями. Таким образом, априорная информация может быть записана следующим образом

\[
\beta=0+v,
\]

где $v$ --- вектор ошибок размера  $K \times 1$  и $v \sim N[0,\sigma^2_{\nu}I_{K}]$. Внесем это дополнительную априорную информацию в выборку $y=X\beta+u$, и запишем расширенную регрессионную модель

\[
\begin{bmatrix}
y\\0
\end{bmatrix}
=
\begin{bmatrix}
X\\I_{K}
\end{bmatrix}
\beta+
\begin{bmatrix}
u\\{-v}
\end{bmatrix}
\]

Замена параметров даст следующие результаты:

\[
\begin{bmatrix}
y\\0
\end{bmatrix}
=
\begin{bmatrix}
X\\\dfrac{\sigma}{\sigma_\nu}I_{K}
\end{bmatrix}
\beta+
\begin{bmatrix}
u\\-\dfrac{\sigma}{\sigma_\nu}{v}
\end{bmatrix}
=
\begin{bmatrix}
X\\{\lambda}I_{K}
\end{bmatrix}
\beta+
\begin{bmatrix}
u\\{v^{*}}
\end{bmatrix}
\]

где $\lambda=\sigma/\sigma_{\nu}$ и  замена $v^{*}=-\lambda{v}$ была сделана, чтобы все ошибки имели одинаковую дисперсию $\sigma^2$.

Метод оценки, основанный на расширенном наборе данных, называется расшеренным или смешанным МНК. При фиксированном $\lambda$ оценка смешенным МНК равна

\begin{equation}
\hat{\beta}_{\lambda}=[X'X+\lambda^{2}I_{K}]^{-1}X'y
\end{equation}

\[
=[X'X(I_{K}+\lambda^{2}(X'X)^{-1})]^{-1}X'y
\]

\[
=[I_{K}+\lambda^{2}(X'X)^{-1}]^{-1}(X'X)^{-1}X'y
\]

\[
=A_{\lambda}\hat{\beta},
\]

где $A_{\lambda}=[I_{K}+\lambda^{2}(X'X)^{-1}]^{-1}$ и $\hat{\beta}=(X'X)^{-1}X'y$ неограниченная МНК-оценка.

Этот подход, который впервые был введен Хёрлем и Кеннардом (1970) для устранения проблемы мультиколлинеарности при оценке на малых выборках получил название ридж-регрессии (ridge regression). Этот подход относится к задачам регуляризации, в которых дополнительно накладывается условие регулярности. Оценка называется сжимающей оценкой (shrinkage estimator), так как она приближается к априорному среднему, в данном случае к нулевому вектору. Данный подход оправдан, например, для конечных выборок с сильно коррелированными данными, где $t$-статистики близки к нулю, что затрудняет возможность отличать переменные, чьи коэффициенты действительно близки к нулю, от переменных, чьи коэффициенты кажутся близкими к нулю. В пределе сближение оценки коэффициента с нулем означает выкидывание регрессора.

Следует отметить некоторые особенности $\hat{\beta}_{\lambda}$: (1) при фиксированном $\lambda$ оценка $\hat{\beta}_{\lambda}$ является средним апостериорного распределения $\beta$ (2) оценка является матричным взвешенным нулевого вектора $0$ и вектора $\hat{\beta}$ (3) вычисления практически не изменятся, если мы будем стягивать оценку к ненулевому $\beta$, скажем к $\beta_0$. Тогда, результирующая оценка будет матричным взвешенное векторов $\beta_0$ и $\hat{\beta}$.

Симметричная взвешенная матрица $A_{\lambda}=[I_{K}+(\lambda^{2}/N)(N^{-1}X'X)^{-1}]\rightarrow{I_K}$ при $N\rightarrow{\infty}$ поскольку $\lambda^{2}/N{\rightarrow}0$. Поэтому,

\[
\hat{\beta}_{\lambda}{\rightarrow}\hat{\beta}, \text{ при } N\rightarrow\infty
\]

и влияние  априорного распределения на апостериорное среднее становится незначительным при увеличении размера выборки.

Аналогично, условная апостериорная дисперсия параметра $\hat{\beta}_\lambda$ равна

\[
V[\hat{\beta}_{\lambda}]=A_{\lambda}V[\hat{\beta}]A_{\lambda}
\]

\[
=\sigma^2 A_{\lambda}(X'X)^{-1}A_{\lambda},
\]

и при $N \rightarrow \infty$ оказывается, что $V[\hat{\beta}_{\lambda}] \rightarrow \sigma^{2}(X'X)$.

Для конечных выборок, при фиксированных  $\lambda$ и $\sigma^2$, условное апостериорное распределение $\hat{\beta}_{\lambda}$ может быть записано в виде

\begin{equation}
\hat{\beta}_{\lambda}|\lambda,\sigma^2\sim
N[A_{\lambda}\hat{\beta},\sigma^2 A_{\lambda}(X'X)^{-1}A'_{\lambda}].
\end{equation}

Частное апостериорное распределение $\hat{\beta}_{\lambda}$ можно получить с помощью интегрирования по $\lambda$ и $\sigma^2$. 
Предполагая, что $\lambda$ задано, а также, что $\sigma^2$ имеет неинформативное априорное распределение, можно проинтегрировать по $\sigma^2$ как показано в Разделе 13.3.1. 
Этот интеграл можно посчитать в явном виде и в результате получим, что частное  апостериорное распределение параметра $\hat{\beta}_{\lambda}$ является многомерным $t$-распределением Стьюдента. 
Кроме того, возможно задать априорное распределение $\lambda$, например, выбрать в качестве априорного  гамма распределение, поскольку $\lambda>0$ и, далее, перейти к интегрированию. Однако, $\lambda$ входит в  условное апостериорное распределение в сложном виде и аналитически посчитать интеграл невозможно. Следовательно, необходимо воспользоваться численным интегрированием. Численное интегрирование позволяет получить байесовскую трактовку модели.

\subsection{Иерархическая априорная информация}

Рассмотрим трех-уровневую линейную регрессионную модель, которая является иерархической по регрессионным параметрам, но не по параметрам дисперсии. 

На первом уровне линейная регрессия имеет вид $y=X_{1}\beta_{1}+u$, где индекс 1 добавляется для того, чтобы отличить параметры и регрессоры первого и второго уровней. Параметры $\beta_1$ случайны и зависят как от параметров, так и от данных, т.е. $\beta_1=X_{2}\beta_{2}+v$. Например, первый уровень модели описывает результаты отдельных студентов на экзамене, а второй уровень модели описывает характеристики школ. Предполагается, что ошибки нормально распределены. Также предполагается, что параметры второго уровня $\beta_2$ неизвестны и задано их  априорное распределение. Априорное распределение также задано для $\sigma^2$ на первом уровне модели.

Предполагая нормальное распределение ошибок и используя сопряженное априорное распределение, получим следующую модель

\begin{equation}
y|X_{1},\beta_1,\sigma^{2}_1 \sim N[X_{1}\beta_1,\sigma^2_{1}I_N],
\end{equation}

\begin{equation}
\beta_1|X_2,\beta_2,\Sigma_2 \sim N[X_2\beta_2,\Sigma_2],
\end{equation}

\begin{equation}
\beta_2 \sim N[\beta^{*},\Sigma^{*}],
\end{equation}

\begin{equation}
\sigma^{-2}_1|\nu^{*},\sigma^{*2} \sim G[\nu^{*}/2,\nu^{*}\sigma^{*2}/2],
\end{equation}

где векторы имеют следующие размеры: $X_1$ --- $N{\times}K$, $X_2$ --- $K\times M$, $\beta_1$ --- $K{\times}1$, $\beta_2$ --- $M{\times}1$, $\Sigma_2$ --- $K{\times}K$, $\beta^{*}$ --- $M{\times}1$ и $\Sigma^{*}$ --- $M{\times}M$. Во второй строке выписана априорное распределение для  регрессионного параметра $\beta_1$, и в третьей строке выписано априорное распределение второго уровня или априорное распределение для априорного распределения $\beta_2$. При этом предполагается, что $\Sigma_2$ известна. Параметры $(\beta^*,\Sigma^*)$ часто называют гиперпараметрами. Что касается параметров дисперсии, в четвертой строке записано априорное распределение для параметра дисперсии $\sigma^2_1$, где $\nu^{*}$ и $\sigma^{*2}$ известны. Нововведением является условие (13.40).

Отметим, что возможно сократить количество уровней и анализировать двухуровневую модель. В частности, мы можем записать двухуровневую модель с информативным априорным распределением одним из следующих способов:

\[
y|X_1,\beta_1,\sigma^2_1{\sim}N[X_1\beta_1,\sigma^2_1I_N],
\]

\[
\beta_1|X_2,\Sigma_2{\sim}N[X_2\beta^*,\Sigma_2+X_2\Sigma^{*}X'_2]
\]

или

\[
y|x_1,X_2,\beta_2,\Sigma_2,\sigma^2_1{\sim}N[X_1X_2\beta_2,\sigma^2_1I_N+X_1\Sigma_2X'_1],
\]

\[
\beta_2{\sim}N[\beta^*,\Sigma^*].
\]

Если $\sigma^2_1$ известно, данная постановка задачи соответствует условно сопряженному с нормальным априорному распределением. Используя полученные ранее результаты мы можем получить  выражения для средних значений апостериорного распределения $\beta_1$ и $\beta_2$, которые соответственно равны матричному средневзвешенному  $\beta^*$ и $\hat{\beta}_1$ или $\beta^*$ и $\hat{\beta}_2$.

Предпосылка о нормальности используется только для примера. Широкое применение на практике получили иерархические модели для обобщенных линейных моделей, в которых используется экспоненциальное семейство распределений (Альберт, 1988).

В иерархических моделях иногда невозможно получить полное апостериорное вероятностное распределение параметров первого уровня, к примеру, $\beta_1$ в аналитическом виде. К счастью, современные симуляционные методы,  представленные в следующем разделе, прекрасно подходят к моделям с иерархической структурой.

Другой подход, являющийся приложением эмпирического байесовского метода, включает оценку параметров априорного распределения верхнего уровня модели, и похоже на метод максимального правдоподобия. В этом подходе, к примеру, не предполагается, что $\Sigma_2$ и $\Sigma^*$ известные матрицы.

\subsection{Многомерное $t$-распределение и распределение Уишарта}

В байесовском анализе за основу можно брать разные распределения. Далее рассмотрено применение байесовского анализа для оценки линейной регрессии на базе двух многомерных распределений при выполнении предпосылки о нормальности.

Многомерное $t$-распределение это многомерный вариант одномерного $t$-распределения Стьюдента. Это распределение аналогично многомерному нормальному распределению, за исключением того, что хвосты $t$-распределения могут быть значительно тяжелее. Для байесовского анализа хвосты распределения более тяжелые, поскольку предельное апостериорное распределение сопряжено с нормальным апостериорным распределением (см. Раздел 13.3.2) или может использоваться непосредственно для $\beta$ в случае, если хвосты распределения тяжелее нормального больше, чем ожидалось. Для $q \times 1$ $t$ случайных величин, имеющих многомерное $t$-распределение Стьюдента, $\nu$ степеней свободы, математическое ожидание $\mu$ и параметр дисперсий $\Sigma$, совместная плотность распределения имеет вид

\[
f_t(t|\nu,\mu,\Sigma)=
\frac{\Gamma((\nu+1)/2)}{\Gamma(\nu/2)(\pi\nu)^{1/2}|\Sigma|^{1/2}} 
\times \left\lbrace 1+\frac{1}{\nu}(t-\mu)'\Sigma^{-1}(t-\mu)\right\rbrace^{-(\nu+q)/2},
\]

где $\Gamma(\cdot)$ это гамма-функция. Распределения симметрично с модой $\mu$, средним $\mu$, если $\nu>1$ и дисперсия равна $[\nu/(\nu-2)]\Sigma$, если $\nu>2$. Хвосты распределения могут быть намного тяжелее, чем у нормального распределения (например, дисперсия равна $3\Sigma$, если $\nu=3$) и нормальное распределение получается при $\nu{\rightarrow}\infty$. Если $z{\sim}N[0,1]$ и $s{\sim}\chi^2(\nu)$, тогда $t=\mu+\Sigma^{\-1/2}z/\sqrt{s/\nu}$ имеет описанное здесь многомерное $t$-распределение. Этот факт помогает получить псевдо-случайную выборку простым способом.

Распределение Уишарта --- многомерный аналог одномерного хи-квадрат распределения, или, в более общем виде, аналог гамма-распределения. В байесовском анализе распределение Уишарта используется в качестве сопряженного априорного распределения для обратной ковариационной матрицы многомерного нормального распределения. Распределение положительно-определенной случайной матрицы $W$ размера  $q\times q$  называется распределением Уишарта со степенями свободы  $\nu{\geq}q$ и матрицей масштаба $S$, если функция плотности равна

\begin{multline}
f_{W}(W|\nu,S)=2^{{\nu}q/2}\pi^{q(q-1)/4}\Pi^{q}_{j=1}\Gamma\left(\dfrac{\nu+1-j}{2}\right) \\
\times|S|^{-{\nu}/2}|W|^{(\nu-q-1)/2}\exp(-\tr(S^{-1}W)/2), 
\end{multline}

где $\Gamma(\cdot)$ --- это гамма функция и $\tr(\cdot)$ обозначает след матрицы. Это распределение имеет математическое ожидание $\nu S$. Выборочная ковариационная матрица для независимых  нормально распределенных векторов имеет  распределение Уишарта. В общем случае, при фиксированном  $\nu(q)$ и независимых векторах размера $q{\times}1$ с $x_j{\sim}N[0,S], j=1,\ldots ,\nu$, сумма $\sum^{\nu}_{j=1}x_{j}x'_j$ будет иметь распределение Уишарта. Если матрица $W^{-1}$  распределена по Уишарту и плотность её распределения равна $f_{W}(W^{-1}|\nu,S)$, тогда $W$ имеет обратное распределение Уишарта  с плотностью

\[
f_{IW}(W|\nu,S)
\]

\[
=2^{{\nu}q/2}\pi^{q(q-1)/4}\Pi^{q}_{j=1}\Gamma\left(\dfrac{\nu+1-j}{2}\right)|S|^{\nu/2}|W|^{-(\nu+q+1)/2}\exp(-\tr(S^{-1}W)/2). 
\]

\subsection{Монте-Карло интегрирование}

Во многих ситуациях невозможно аналитически записать апостериорное распределение параметров. В таких случаях необходимо использовать численные методы для оценки либо всех параметров апостериорного распределения, либо некоторых ключевых характеристик этого распределения, к примеру, апостериорного среднего.

В этом разделе будет рассмотрен расчет основных характеристик апостериорного распределения, без получения самого апостериорного распределения полностью. Для этого возможно использование методов из Главы 12 и с меньшим количеством расчетов, поскольку интеграл необходимо рассчитать всего один раз, а не для каждого отдельного наблюдения на каждой итерации. В следующем Разделе рассмотрены методы генерирования выборки из апостериорного распределения.

\subsection{Сэмплирование по важности}

Предположим, что задача состоит в оценке апостериорной  моментной функции $\E[m(\theta|y)]$, где математическое ожидание рассчитывается по функции плотности $p(\theta|y)$. Необходимо рассчитать значение выражения

\begin{equation}
\E[m(\theta)]=\int_{R(\theta)}m(\theta)p(\theta|y)d\theta.
\end{equation}

Например, апостериорное среднее $k$-го параметра  равно $\E[\theta_k]=\int{\theta_{k}}p(\theta|y)d\theta$. Другие примеры включают  расчет апостериорного стандартного отклонения,  апостериорной частной плотности, апостериорных доверительных интервалов, а также апостериорного математического ожидания заданной функции от параметров.


Из Главы 12 следует, что прямая Монте-Карло оценка интеграла $\E[m(\theta)]$ равна $\hat{E}[m(\theta)]=S^{-1}\sum_{s}m(\theta^s)$, где $\theta^s, s=1,\ldots ,S$ --- это $S$ значений параметра $\theta$, сгенерированных согласно  апостериорному распределению $p(\theta|y)$. Однако эту оценку невозможно получить в текущей байесовской постановке задачи, если не существует аналитического решения для плотности апостериорного распределения (13.1), поскольку при отсутствии аналитической записи плотности невозможно сгенерировать выборку из $p(\theta|y)$. Поэтому мы будем использовать сэмплирование по важности, упомянутое в Разделе 12.7.2. Интеграл в (13.42) может быть записан также в виде:

\begin{equation}
E[m(\theta)]=\int_{R(\theta)}\left(\dfrac{m(\theta)p(\theta|y)}{g(\theta)}\right)g(\theta)d\theta, 
\end{equation}

где $g(\theta)>0$ известная функция плотности, с таким же носителем как и плотность $p(\theta|y)$, выборку из которой легко генерировать. Оценка интеграла методом Монте-Карло соответственно равна

\[
\hat{E}[m(\theta)]=\dfrac{1}{S}\sum^S_{s=1}\dfrac{m(\theta^s)p(\theta^s|y)}{g(\theta^s)},
\]

где $\theta_s, s=1,\ldots ,S$ --- это $S$ значений $\theta$ сгенерированных вспомогательной плотности $g(\theta)$, а не из целевой плотности $p(\theta|y)$. Следует отметить, что требование о совпадении областей определения $p(\theta|y)$ и $g(\theta)$ может вызвать затруднение, если $p(\theta|y)$ зависит от дополнительных параметров или если известна функциональная форма условной плотности, но неизвестна форма частной апостериорной плотности.

При работе с апостериорной плотностью также необходимо разобраться с  константой интегрирования в знаменателе формулы (13.1). Предположим, что $p^{ker}(\theta|y)$ обозначает ядро апостериорной плотности распределения, т.е. $p^{ker}(\theta|y)=L(y|\theta)\pi(\theta)$ или кратно значению этого выражения. Для простоты обозначений далее мы опустим зависимость от $y$. Тогда апостериорная плотность будет равна

\[
p(\theta)=\dfrac{p^{ker}(\theta)}{\int{p^{ker}(\theta)d\theta}},
\]

и, соответственно, математическое ожидание равно

\[
E[m(\theta)]=\int{m(\theta)\left(\dfrac{p^{ker}(\theta)}{\int{p^{ker}(\theta)d\theta}}\right)d\theta}
\]

\[
=\frac{\int m(\theta)p^{ker}(\theta)d\theta}{{\int}p^{ker}(\theta)d\theta}
\]

\[
=\dfrac{{\int}(m(\theta)p^{ker}(\theta)/g(\theta))g(\theta)d\theta}{{\int}(p^{ker}(\theta)/g(\theta))g(\theta)d\theta}.
\]

С помощью сэмплирования по важности мы получаем оценку для $\E[m(\theta)]$ равную

\begin{equation}
\hat{E}[m(\theta)]=\dfrac{\dfrac{1}{S}\sum^{S}_{s=1}m(\theta)p^{ker}(\theta^s)/g(\theta^s)}{\dfrac{1}{S}\sum^{S}_{s=1} p^{ker}(\theta^s)/g(\theta^s)},
\end{equation}

где $\theta^s, s=1,\ldots ,S$ --- это $S$ значений $\theta$, сгенерированных из вспомогательной плотности сэмплирования по важности $g(\theta)$.

Этот метод впервые предложили Клоэк и Ван Дейк (1978). Гевеке (1989) доказал состоятельность и асимптотическую нормальность оценок при выполнении некоторых условий регулярности. 
Эти условия включают следующие предпосылки: $g(\theta)>0$ на носителе $R(\theta)$ плотности $p(\theta)$; $\E[m(\theta)]<\infty$, что означает существование апостериорных моментов; и ${\int}p(\theta|y)d\theta=1$, т.е. апостериорная плотность является собственной. Как ранее отмечалось, обычно мы работаем с ядром  плотности, равным $p^{ker}(\theta|y)=L(y|\theta)\pi(\theta)$, которое может не давать единицу при интегрировании. 
Априорное распределение $\pi()\theta$ может быть несобственным, но для равенства $p(\theta|y)d\theta=1$ требуется выполнение условия ${\int}\pi(\theta)d\theta<\infty$.

Сэмплирование по важности просто в применении, но на практике возникают нюансы, рассмотренные  у Гевеке (1989). 
Основное требование заключается в том, что у плотности $g(\theta)$ должны быть более толстые хвосты, чем у $p(\theta|y)$, чтобы веса сэмплирования по важности $w(\theta)=p(\theta|y)/g(\theta)$ оставались ограниченными. 
Из-за асимптотической нормальности логарифма апостериорной плотности, можно выбрать в качестве $g(\theta)$  многомерное $t$-распределение, со средним равным моде апостериорного распределения, с ковариационной матрицей пропорциональной обращенной матрице Гессе для логарифма апостериорной плотности. При этом количество степеней свободы должно быть достаточно мало для обеспечения тяжести хвостов распределения $g(\theta)$. Также Гевеке (1989) ввел меру относительной вычислительной эффективности, которая равна отношению количества симуляций, необходимых для достижения заданного уровня точности оценивания $\hat{E}[m(\theta)]$, при использовании $g(\theta)$, к количеству симуляций, которое бы потребовалось, если бы можно было делать симуляции непосредственно из плотности  $p(\theta|y)$. Из Главы 12 следует что для интегралов высокой размерности необходимо большее количество симуляций, чтобы достичь хорошей аппроксимации. Также можно использовать методы ускорения симуляций, рассмотренные в Главе 12, такие как антитетическое сэмплирование.

Сэмплирование по важности использует сгенерированные значения $\theta^s$ из плотности $g(\theta)$ с равной вероятностью. Более эффективным было бы взвешивать сгенерированные значения согласно близости  $g(\theta^s)$ к $p(\theta|y)$. Это можно достичь с помощью повторного сэмплирования по важности (см. Гельман и др., 1995).

Также, сэмплирование по важности может быть использовано для расчета различных характеристик апостериорного распределения, которые рассмотрены в Разделе 13.2.5. Среди этих характеристик  можно выделить такие, как оценки квантилей и перцентилей апостериорного распределения, что позволяет построить $95\%$ апостериорный интервал и нарисовать график плотности для $\theta_k$.

\section{Алгоритм Монте-Карло по схеме марковской цепи}

Современная идея байесовского анализа заключается в том, чтобы вместо оценки основных характеристик апостериорного распределения (см. предыдущий раздел), построить большую выборку из апостериорного распределения. Далее с помощью описательных статистик для построенной выборки апостериорного распределения можно будет получить информацию о моментных характеристиках  оценок, а также о других характеристиках, к примеру, о частном распределении параметров или их функций. Например, имея выборку из $S$ значений из апостериорного распределения, можно рассчитать $\E[\theta_k]$ через $s^{-1}\sum_{s}\theta^{s}_k$.

Задача состоит в том, чтобы сгенерировать значения из апостериорного распределения, когда отсутствует аналитическая форма записи для апостериорной плотности. Если существует распределение, подходящее для расчета апостериорных моменты с помощью сэмплирования по важности, тогда оно подходит и для генерирования выборки из апостериорного распределения с помощью алгоритма принятия-отбрасывания, который был рассмотрен в Разделе 12.8. Однако, применение этого метода может быть неэффективно, поскольку большой процент значений может быть отброшен.

Вместо этого генерируются последовательные значения так, что при большой длине последовательности распределение  генерируемых значений сходится к стационарному, совпадающему с целевой  апостериорной плотностью $p(\theta|y)$. Этот подход называется алгоритмом Монте-Карло по схеме марковской цепи (Markov Chain Monte Carlo, MCMC), поскольку предполагает сочетание симулирования (Монте-Карло) и последовательности, являющейся Марковской цепью. После того, как последовательность сойдется, выборка из $S$ симуляций может быть использована для расчета характеристик апостериорного распределения, например, $\E[\theta]$ можно оценить с помощью $\hat{E}[\theta_k]=S^{-1}\sum_{s}\theta^{s}_k$. Полученные симуляции имеют положительную корреляцию, следовательно, точность оценки снижается, поскольку дисперсия будет больше обычного, т.е. больше, чем $(S-1)^{-1}\sum_{s}(\theta^{s}_k-\hat{E}[\theta_k])^2$.

Генерируемая последовательность является цепью Маркова. Для построения широко применяются два алгоритма, алгоритм Гиббса и алгоритм Метрополиса-Хастингса, первый практически является частным случаем последнего, см. Хастингс (1970). Великолепное детальное описание приводят Гельман и др. (1995), Гамерман (1997) и Роберт и Казелла (1999). Мы приводим только краткую схему. 

\subsection{Марковские цепи}

Перед тем как перейти к рассмотрению алгоритма Гиббса и алгоритма Метрополиса-Хастингса необходимо дать основные определения и идеи, которые используются в литературе по MCMC. Нижеприведенные определения даны в контексте дискретной модели. Эти определения могут также распространятся и на непрерывные модели, которые относятся к случаям, когда апостериорное распределение непрерывно по параметрам.

Марковская цепь определяется как последовательность случайных величин $x_n (n=0,1,2,\ldots )$, где $x_n$ принимает значения в конечном множестве $A$ с ядром перехода $K(\cdot)$, которое определяет вероятность того, что $x_n$ будет равно определенному значению, при заданном предыдущем значении $x_{n-j}$. Марковские цепи определяется свойством

\begin{equation}
\Pr[x_{n+1}=x|x_n,x_{n-1},\ldots ,x_0]=\Pr[x_{n+1}=x|x_n],
\end{equation} 

это означает, что распределение $x_{n+1}$ полностью зависит только от предыдущего значения $x_n$. Ядро перехода представляет собой матрицу перехода $T$ с элементами

\begin{equation}
t_{xy}=\Pr[x_{n+1}=y|x_n=x],
\end{equation}

что равно вероятности перехода из $x$ в $y$. Для марковской цепи с конечным числом состояний множество $A$ значений (или состояний)  $x_n$ конечно и состоит из $m$ элементов, тогда $T$ можно записать как



\[ T=
\begin{bmatrix}
t_{11} & \dots & t_{1m} \\
& \vdots & \ddots & \vdots\\
t_{m1} & \dots & t_{mm}
\end{bmatrix}
\]

где $\sum^{m}_{j=1}t_{ij}=1, i=1,\ldots ,m$.

Далее рассмотрим переход из $x$ в $y$ за $n$ шагов. Вероятность указанного перехода $T^n$ равна произведению $n$-раз матриц $T$. Элементы в строках матрицы $T^n$ --- это вероятности   $m$ состояний на $n$-том шаге, и  $j$-ая строка $t^{(n)}_j=(t^{(n)}_{j1},\ldots ,t^{(n)}_{jm})$ представляет собой вероятности перехода из состояния $j$ в другие состояния на шаге $n$. Если начальное распределение вероятностей отдельных состояний $t^{(0)}$, тогда распределение на $n$-ом шаге имеет вид $t^{(n)}=t^{(0)}T^n=t^{(n-1)}T$. Таким образом, распределение вероятностей на $n$-том шаге определяется только начальным распределением и матрицей перехода.

В контексте симуляций, основное внимание сосредоточено на асимптотическом <<поведении>> цепи Маркова при $n{\rightarrow}\infty$. Говорят, что цепь имеет стационарное распределение или инвариантное распределение при вероятности перехода $t_{xy}$, если 

\begin{equation}
\sum_{x \in A}t_{x}t_{xy}=t_y \text{ для } \forall y \in A,
\end{equation}

где $t_{xy}$ --- вероятность перехода от $x$ к $y$. Домножение на матрицу перехода не приводит к изменениям стационарных вероятностей. Существование и единственность стационарного распределения является важным вопросом.

Если стационарное распределение существует и, если $lim_{n{\rightarrow}\infty}t^{(0)}T^{n}_{x,y}=t^*$, тогда цепь будет асимптотически сходится к $t^*$ независимо от начального распределения. В таком случае $t^*$ является предельным распределением. Несмотря на то, что в этой главе стационарное распределение определено для марковских цепей с конечным числом состояний, MCMC методы могут быть также использованы и для бесконечного количества состояний; см. Гилкс, Ричардсон и Шпигельхальтер (1996, стр. 60-61).

Состояние $y$ может быть возвратным (рекуррентным) или невозвратным (транзиентным). Рекуррентное состояние будет повторно посещено с вероятностью 1, а транзиентное состояние имеет положительную вероятность никогда больше не быть посещенным. 

В рамках байесовского подхода цель заключается в генерировании  значений из апостериорного  распределения, $p(\theta)$. При использовании цепей Маркова для генерирования этих значений, начальные значения вектора параметров, $\theta^{(0)}$, которые аналогичны распределению состояний, часто генерируются с помощью переходного ядра.  Используя подходящий метод для генерирования псевдо-случайных чисел, получим новый вектор значений $\theta^{(1)}$ с помощью ядра перехода, оцененного в точке $\theta^{(0)}$, т.е. $K(\theta^{(0)})$. На $n$-том шаге значения генерируются с помощью переходного ядра $K(\theta^{(n-1)})$. Используемая марковская цепь такова, что при $n{\rightarrow}\infty$ апостериорную плотность  стремится к $p(\theta)$. Когда сходимость к предельному распределению произошла, все последующие значения генерируются из этого распределения, хотя они и являются коррелированными.

Эти идея являются интуитивном объяснением  MCMC алгоритмов, с помощью которых возможно восстановить байесовское апостериорное распределение для многих разных, в том числе многомерных, моделей, к примеру, линейной иерархической модели, рассмотренной в разделе 13.3.4. Предполагая, что задано ядро перехода $K(\theta^{n-1},\cdot)$ с помощью которого можно генерировать значения $\theta$ и внутри которого заложено предельное распределение, целевое апостериорное распределение может быть получено в смысле сколь угодно близкого приближения.

Текущее описание имеет общий характер. На практике, не существует единого выбора ядра перехода и возможно построение разных цепей Маркова. Некоторые варианты могут быть более удачными, чем другие с точки зрения сходимости к предельному распределению. Если сходимость происходит слишком медленно и необходимо производить много расчетов, возможно имеет смысл построить другую цепь Маркова. Также стоит отметить, что нужны критерии сходимости, чтобы определить насколько близко распределение к целевому распределению на $n$-том шаге. 

\subsection{Алгоритм Гиббса}

Вначале рассмотрим алгоритм Гиббса, подход, который относится к MCMC классу, может быть легко описан и применим.

Допустим, что вектор $\theta=[\theta_{1} \theta_2]^{\prime}$ имеет апостериорную плотность $p(\theta)=p(\theta_{1},\theta_{2})$, где для простоты обозначений мы опускаем зависимость от $y$. Если известны все условные плотности, то генерируя  значений цепи поочередно то из $p(\theta_{1}|\theta_{2})$, то  из $p(\theta_{2}|\theta_{1})$,  мы получим в пределе значения из распределения  $p(\theta_{1},\theta_2)$. Для расчета требуется знание обеих плотностей, $p(\theta_{1}{|}\theta_{2})$ и $p(\theta_{2}|\theta_{1})$, данное условие не всегда выполнено на практике.

\subsubsection*{Пример}

В качестве простой иллюстрации рассмотрим двумерные нормально распределенные данные  с равномерной априорной плотностью для среднего и известной ковариационной матрицей. Допустим, что $y=(y_1,y_2){\sim}N[\theta,\Sigma]$, где $\theta=[\theta_{1},\theta_{2}]'$ и диагональные элементы матрицы $\Sigma$ равны 1, а внедиагональные элементы --- $\rho$. Тогда с учетом заданной априорной плотности  $\theta$ возможно показать что апостериорное распределение является двумерным нормальным распределением, $\theta|y{\sim}N[\overline{y},N^{-1}\Sigma]$. Поскольку условные плотности известны


\[
\theta_1|\theta_2, y{\sim}N[(\overline{y}_1+\rho(\theta_2-\overline{y}_2)),(1-\rho^2)/N],
\]

\[
\theta_2|\theta_1, y{\sim}N[(\overline{y}_2+\rho(\theta_1+\overline{y}_1)),(1-\rho^2)/N],
\]

мы можем делать итерации, генерируя значения из условного нормального распределения, используя обновленные значения $\theta_1$ и $\theta_2$. Если цепь достаточно длинная, тогда распределение будет сходиться к двумерному нормальному. В этом примере легко напрямую сгенерировать значения из совместного распределения $\theta|y$, используя разложение Холецкого, которое дано в Разделе 12.8, однако в других примерах возможно генерирование  значений только из условного, а не совместного распределения.

\subsubsection*{Алгоритм Гиббса}

В более общем случае, рассмотрим $q$-мерное целевое распределение $p(\theta)$, где обозначение зависимости от данных опускается. Предположим, что вектор параметров $\theta$ разбит на $d$ блоков. К примеру, в линейной регрессионной модели $\theta'=[\beta {\sigma}^{2}]'$. Допустим, что $\theta_k$ обозначает $k$-ый блок и $\theta_{-k}$ все компоненты вектора $\theta$, которые не входят в $\theta_k$. Предположим, что условное распределение, $p(\theta_{k}|{\theta}_{-k}), k=1,\ldots ,d$ известно. Тогда последовательное сэмплирование может быть организовано согласно следующей процедуре:

1. Возьмем начальные значения $\theta$, $\theta^{(0)}=(\theta^{(0)}_1,\ldots ,\theta^{(0)}_d)$.

2. Следующая итерация состоит в пошаговом изменении всех компонент вектора $\theta$, что дает в результате вектор $\theta^{(1)}=(\theta^{(1)}_1,\ldots ,\theta^{(1)}_d)$, значения которого сгенерированы последовательно с помощью $d$ генерирований из $d$ условных распределений:

\[
p(\theta^{(1)}_1|\theta^{(0)}_2,\ldots ,\theta^{(0)}_d)
\]

\[
p(\theta^{(1)}_2|\theta^{(1)}_1,\theta^{(0)}_3\ldots ,\theta^{(0)}_d)
\]
\[
\vdots
\]
\[
p(\theta^{(1)}_d|\theta^{1}_1,\theta^{(1)}_2,\ldots ,\theta^{(1)}_{d-1}).
\]

3. Возвращаемся на шаг 1, в качестве стартового вектора $\theta$ берём $\theta^{(1)}$ и делаем шаг 2 для того, чтобы построить новый вектор $\theta^{(2)}$. Повторяем шаги 1 и 2 до тех пор, пока не будет достигнута сходимость.

В работе Гилкса др. (1996, стр.7) представлено краткое доказательство того, что стационарное распределение данной цепи является апостериорным. После того как сходимость будет достигнута, значения формируются из целевого совместного апостериорного распределения. Геман и Геман (1984) показали, что стохастическая последовательность ${\theta^{n}}$ является марковской цепью с нужным стационарным распределением. Гельфанд и Смит (1990) показали, что, при выполнении некоторых условий, цепь сходится к стационарному апостериорному распределения при количестве циклов формирования значений из полного множества условных распределений стремящемся к бесконечности. Также см. Таннер и Вонг (1987). По достижении сходимости, можно сгенерировать большую выборку и использовать её для подсчета выборочных аналогов апостериорных моментов частного или совместного распределений.

В вышеупомянутых работах не оговаривается необходимое количество циклов, которое требуется произвести, чтобы достичь сходимости, поскольку это количество может варьироваться в зависимости от модели. Важно убедиться в том, что было произведено необходимое количество циклов для достижения сходимости цепи. Существует множество тестов для проверки  сходимости. Поскольку оценки апостериорных моментов должны быть основаны на значениях, сформированных из апостериорного распределения, как правило первые полученные значения цепи отбрасываются, эти значения называются периодом прожига (burn-in phase).

Алгоритмы последовательной симуляции могут быть изменены таким образом, что каждое значение будет зависеть не только от последнего сформированного значения, но также от более ранних значений. Основное требование заключается в том, что вероятность улучшения текущей аппроксимации апостериорного распределения должна быть положительна и желательно высока. Привлекательность более ограничивающего марковского подхода заключается в более простых доказательствах сходимости  распределения к целевому апостериорному распределению.

Для байесовского анализа алгоритм Гиббса является полезным инструментом, когда для совместного апостериорного распределения нет аналитической формы записи, а для всех условных распределений она есть. Во многих приложениях в значительной степени используются изобретательность и знание о сопряженных плотностях и других байесовских результатах, многие  из которых были получены до распространения симуляционных методов. Эти результаты нужны для того, чтобы специфицировать априорные плотности распределения для которых возможно рассчитать полные условные распределения. 

Далее рассмотрим два примера применения MCMC методов.

\subsubsection*{Пример линейной регрессии}

В Разделе 13.3.2 мы проанализировали апостериорное распределение нормальной линейной гомоскедастичной регрессионной модели при нормальной и гамма сопряженных априорных плотностях. Ранее было показано, что $\beta$ при заданном $\sigma^{-2}$ имеет многомерное нормальное условное апостериорное распределение и условное распределение $\sigma^{-2}$ равно гамма-распределению. Хотя можно получить совместное апостериорное  распределение  в явной форме (см. (13.32)), гораздо проще применить алгоритм Гиббса для формирования большой выборки из совместного апостериорного распределения. Цепь получается путем последовательного генерирования  значений, полученных из нормального условного распределения при фиксированном  параметре точности $\sigma^{-2}$ и из гамма распределения при фиксированном $\beta$.

Структура алгоритма аналогична структуре, которая будет использована далее, в Разделе 13.6 для более сложного случая, когда вместо одного уравнения берется система из двух внешне не связанных уравнений регрессии.

Во многих случаях естественно работать с блоками параметров. Например, в случае множественной линейной регрессионной модели многих уравнений с недиагональной одновременной ковариационной матрицей, параметры условного среднего $(\beta_1,\beta_2,\ldots )$ формируют один блок  и $\Sigma$ формирует второй. Условные распределения можно записать в следующей форме $\beta_1,\beta_2,\ldots |data,\Sigma$ и $\Sigma|data,\beta_1,\beta_2,\ldots $. Чиб и Гринберг (1996, стр. 418-419) рассмотрели применение алгоритма Гиббса для этого случая.

\subsubsection*{Пример иерархического априорного распределения}

Схема Гиббса широко применяется для анализа моделей иерархического априорного распределения. С учетом структуры линейной иерархической модели, которая задана уравнениями (13.39)-(13.41), легко построить марковскую цепь на базе полного множества условных распределений. Аналогичный общий подход может быть расширена для случая нелинейной иерархической априорной модели, при этом может потребоваться осуществление дополнительных шагов если нелинейность появляется в сочетании с  латентной переменной (Альберт, 1988).

\subsection{Алгоритм Метрополиса}

Алгоритм Гиббса --- наиболее известный MCMC алгоритм. Алгоритм Гиббса имеет ограничения в применения, ввиду необходимости создания выборки из  условного распределения, которое может быть неизвестно. Возможно расширить применение MCMC методов с помощью алгоритма Метрополиса и алгоритма Метрополиса-Хастингса. В своей работе Чиб и Гринберг (1995) дают введение и рекомендуют  другие источники. Ниже дано краткое описание алгоритмов.

Согласно алгоритму Метрополиса строится последовательность $\left\lbrace\theta^{(n)}, n=1,2,\ldots \right\rbrace$, распределение значений которой сходится к целевому апостериорному распределению и предполагается, что значение этого распределения можно посчитать с точностью до нормализующей константы.

Для простоты обозначений вновь опустим зависимость функции плотности $p(\theta|y)$ от $y$. Алгоритм включает следующие шаги:

1. Формируем стартовые значение $\theta^{(0)}$ из первичной аппроксимации апостериорного распределения, при этом должно выполняться условие $p(\theta^{(0)})>0$. Например, можно сгенерировать значения из многомерного $t$-распределения с центром равным моде частного апостериорного распределения.


2. Положим $n=1$. Сгенерировать предложение $\theta^*$ из симметричного вспомогательного распределения $J(\theta^{(1)}\mid \theta^{(0)})$, которое для любой пары $(\theta^a,\theta^b)$ обладает свойством $J(\theta^{a}\mid \theta^{b})=J_1(\theta^b\mid \theta^a)$. Примером может служить $\theta^{(1)}\mid \theta^{(0)} \sim N[\theta^{(0)},V]$ при фиксированной матрице $V$. Симметричность распределения предложения упрощает вычисления, но некритична.

3. Рассчитать отношение плотностей $r=p(\theta^*)/p(\theta^{(0)})$

4. Положить $\theta^{(1)}$ равной 

\[
\theta^{(1)}= \begin{cases}
\theta^* \text{ с вероятностью } \min(r,1) \\
\theta^{(0)} \text{ с вероятностью } (1-\min(r,1))
\end{cases}
\]


Это означает, что $\theta^{(1)}$ --- смесь распределений $\theta^*$ и  $\theta^{(0)}$.

5. Вернуться к шагу 2, увеличить счетчик шагов и повторить последующие шаги.

6. После большого количества шагов провести тесты на сходимость. Если тесты не отвергают сходимость, можно считать, что найдено апостериорное распределение.



Этот алгоритм может быть рассмотрен как итерационный метод максимизации $p(\theta)$. Если  $\theta^{*}$ увеличивает $p(\theta)$, тогда обязательно $\theta^{(n)}=\theta^{*}$, если же  $\theta$ уменьшает значение $p(\theta)$, тогда $\theta^{(n)}=\theta^*$ с вероятностью $r<1$. 

Этот алгоритм --- аналог подхода принятия-отказа (см. Раздел 12.8), на этот раз не существует ограничения, что функция плотности предложения домноженная на константу должна накрывать апостериорную функцию плотности.

Алгоритм Метрополиса позволяет создать марковскую цепь, которая имеет свойства обратимости, неприводимости и рекуррентности по Харрису, что гарантирует сходимость к стационарному распределению. Гельман и др. (1995) показали, что стационарное распределение есть ожидаемое апостериорное распределение $p(\theta)$. Допустим, что $\theta_a$ и $\theta_b$ две точки такие, что $p(\theta_b){\geq}p(\theta_a)$. Если $\theta^{(n-1)}=\theta_a$ и $\theta^{*}=\theta_b$, тогда с уверенностью можно сказать, что $\theta^{(n)}=\theta_b$ и $\Pr[\theta^{(n)}=\theta_b,\theta^{(n-1)}=\theta_a]=J_{n}(\theta_b|\theta_a)p(\theta_a)$. Если поменять порядок и задать, что $\theta^{(n-1)}=\theta_b$ и $\theta^*=\theta_a$, тогда $\theta^{(n)}=\theta_a$ с вероятностью $r=p(\theta_a)/p(\theta_b)$ и $\Pr[\theta^{(n)}=\theta_{a}, \theta^{(n-1)}=\theta_b]=J_{n}(\theta_a|\theta_b)p(\theta_b)[p(\theta_a)/p(\theta_b)]=J_{n}(\theta_a|\theta_b)p(\theta_a)=J_{n}(\theta_b|\theta_a)p(\theta_a)$, предполагая, что распределение предложения симметрично. Таким образом, можно сделать вывод, что частные плотности  $\theta^{(n)}$ и $\theta^{(n-1)}$ одинаковы, поскольку их совместное распределение симметрично, таким образом,  $p(\theta)$ является симметричным стационарным распределением марковской цепи.



\subsection{Алгоритм Метрополиса-Хастингса}

Результаты применения алгоритма Метрополиса могут меняться в зависимости от выбранного начального приближения  и выбора распределения предложения. Возможной проблемой является медлительность алгоритма Метрополиса, которая может возникнуть, если замена текущих значений параметров новыми не происходит с нужной частотой, что приводит к замедлению движения цепи. Выполнение алгоритма может быть ускорено путем использования несимметричных предлагающих распределений.

Процедура алгоритма Метрополиса-Хастингса аналогична процедуре алгоритма Метрополиса, за исключением того, что на шаге 2 предлагающее распределение не обязательно должно быть симметричным и на шаге 3 вероятность принятия решений $r$ для всех значений $n$ становится равной 

\[
r_n=\dfrac{p(\theta^{*})/J_{n}(\theta^{*}|\theta^{(n-1)})}{p(\theta^{(n-1)})/J{n}(\theta^{(n-1)}|\theta^*)}=\dfrac{p(\theta^*)J_{n}(\theta^{(n-1)}|\theta^*)}{p(\theta^{(n-1)})J_{n}(\theta^*|\theta^{(n-1)})}.
\]

Оставшиеся шаги выполняются в соответствии с этим изменением. Отметим, что если в $p(\cdot)$ или в $J(\cdot)$ есть нормализующая константа, то в $r_n$ нормализующие константы сокращаются. Таким образом, значение обеих плотностей, апостериорной и предлагающей, могут быть рассчитаны до  константы пропорциональности. См. Хастингс (1970).

\subsection{Примеры алгоритма Метрополиса-Хастингса}

Отличные друг от друга предлагающие распределения приводят к различным алгоритмам с разной степенью эффективности, в том смысле, что отличается количество симуляций, необходимых для генерирования значений из апостериорного распределения. Далее рассмотрим несколько примеров. Следует отметить, что существуют разные рекомендации  для выбора предлагающего распределения, за исключением рекомендации применять алгоритм Гиббса, когда это возможно.  

Алгоритм Гиббса можно рассматривать как частный случай алгоритма Метрополиса-Хастингса. Если вектор $\theta$ разбит на $d$ блоков, тогда на $n$-том шаге алгоритма будет сделано $d$ шагов Метрополиса. Предлагающее распределение --- это  условное распределение, данное  в Разделе 13.5.2. Можно показать, что вероятность принятия здесь всегда равна единице. % здесь есть английский синоним, но в русском его нет

Возможно использовать несколько стратегий, применяя различные переходные ядра  для разных подмножеств параметров. Например, шаг алгоритма Метрополиса-Хастингса может применяться совместно со схемой Гиббса, которую удобно использовать, если возможно непосредственно генерировать значения из соответствующего условного распределения.

В алгоритме Метрополиса-Хастингса с независимыми предложениями все значения предлагающего распределения генерируются  из фиксированной плотности $g(\theta)$, в таком случае вероятность принять упрощается и становится равной выражению $r_n=w(\theta^{*})/w(\theta^{(n-1)})$, где веса значимости равны $w(\theta)=p(\theta)/g(\theta)$. Алгоритм Метрополиса-Хастингса со случайным блужданием генерирует предложения по правилу $\theta^*=\theta^{(n-1)}+\varepsilon$, где $\varepsilon$ генерируется согласно плотности $g(\varepsilon)$.

Гельман и др. (1995, стр. 334) рассматривают симулирование $q$-мерного нормального распределения с ковариационной матрицей $\Sigma$. Для алгоритма Метрополиса со предлагающим  распределением $\theta^{*}|\theta^{(n-1)}{\sim}N[\theta^{(n-1)},c^{2}\Sigma]$ при выборе $c{\simeq}2.4/\sqrt{q}$ дает наибольшую эффективность по сравнению с прямым генерированием из $q$-мерного нормального распределения. Эффективность достигает примерно $0.3$, по сравнению с алгоритмом Гиббса, когда эффективность равна $1/q$, если $\Sigma=\sigma^{2}I_q$.

\section{Пример MCMC: алгоритм Гиббса для внешне не связанных уравнений}

Проиллюстрируем применение алгоритма Гиббса, чтобы проанализировать модель внешне не связанных уравнений. Этот пример  немного более сложный, чем применение к регрессии с одним уравнением, поскольку учитывается, что ошибки уравнений взаимосвязаны между собой.

Рассмотрим пример модели с двумя уравнениями для $i$-го наблюдения

\[
y_{1i}=x'_{1i}\beta_1+\epsilon_{1i},
\]

\[
y_{2i}=x'_{2i}\beta_2+\epsilon_{2i},
\]

где $(\varepsilon_1,\varepsilon_2)$ имеют двумерное нормальное распределение со средним значением ноль и ковариационной матрицей 

\[
\Sigma=
\begin{bmatrix} \sigma_{11} \sigma_{12}\\ \sigma_{21} \sigma_{22} \end{bmatrix}.
\]

Комбинируя эти два уравнения, получим, что для $i$-го наблюдения 

\[
y_{i}=x'_{i}\beta+\epsilon_{i},
\]

где $\varepsilon{\sim}N[0,\Sigma]$. В итоге получим следующее выражение для процесса порождающего данные

\[
y_{i}|x_{i},\beta,\Sigma,\sim N[x'_{i}\beta,\Sigma]
\]

и основная цель состоит в  оценке апостериорного среднего регрессионного параметра $\beta$ и ковариационной матрицы $\Sigma$, при заданных значениях $y,X$.

Рассмотрим независимые априорные распределения, где

\[
\beta \sim N[\beta_{0},B^{-1}_0],
\]

\[
\Sigma^{-1} \sim Wishart[\nu_{0},D_0],
\]

$B_0$ определяет точность, значение которой обратно к ковариационной матрице априорного распределения и обратное распределение Уишарта, описанное в Разделе 13.3.5, является обобщением обратного гамма-распределения. Альтернативный способ, который не рассматривается здесь, заключается в том, чтобы использовать зависимые априорные распределения, по аналогии с тем, как это было сделано в Разделе 13.3.2, что даст $\beta|\Sigma  \sim N[\beta_0,w_{0}\Sigma]$ для заданного $w_0$.

Проведя математические преобразования, получим следующие условные апостериорные распределения

\[
\beta|\Sigma,y,X{\sim}N\left[C_{0}\left(B_{0}\beta_{0}+\sum^{N}_{i=1}x'_{i}\Sigma^{-1}y\right)_{i},C_0\right], 
\]

\[
\Sigma^{-1}|\beta,y,X{\sim} Wishart \left[\nu_{0}+N,\left(D^{-1}_0+\sum^{N}_{i=1}u'_{i}u_{i}\right)^{-1}\right], 
\]

где $C_0=(B_{0}+\sum^{N}_{i=1}x'_{i}\Sigma^{-1}x_i)^{-1}$ и $u_{i}=y_{i}-x'_{i}\beta$. Возможно использовать алгоритм Гиббса, поскольку известны условные апостериорные распределения, а  сэмплирование из них не представляет трудностей.

Таблица 13.3. Алгоритм Гиббса: пример внешне не связанных уравнений

\begin{tabular}{p{4cm}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}}
\hline 
Параметр априорного распределения, $\tau$ & $\tau$=10 & $\tau=1$ & $\tau=1/10$ & $\tau=10$ & $\tau=10$ \\ 
Размер выборки, $N$ & 1,000 & 1,000 & 1,000 & 1,000 & 10,000 \\ 
Репликации алгоритма Гиббса & 50,000 & 50,000 & 50,000 & 100,000 & 100,000 \\ 
\hline 
$\beta_{11}$ (своб-ый член ур-ия 1) &  0.971 (0.0310) & 1.013 (0.0312) & 0.983 (0.0316) & 1.020 (0.0324) & 1.010 (0.0100) \\ 
$\beta_{12}$ (коэф-нт наклона ур-ия 1) & 1.026 (0.0265) & 0.9835 (0.0271) & 1.006 (.0265) & 1.006 (.0268) & 1.015 (0.0086) \\ 
$\beta_{21}$ (своб-ый член ур-ия 2) & 1.016 (0.0309) & 0.972 (0.0325) & 0.993 (0.0322) & 1.017 (0.0326) & 0.991 (0.0100) \\ 
$\beta_{22}$ (коэф-нт наклона ур-ия 2) & 0.983 (0.0256) & 0.992 (0.0285) & 0.979 (0.0272) & 1.005 (0.0277) & 1.007 (0.0085) \\ 
$\sigma_{11}$ (дисперсия ошибок ур-ия 1) & 0.960 (0.0429) & 0.969 (0.0434) & 1.012 (0.0453) & 1.043 (0.0466) & 1.010 (0.0143) \\ 
$\sigma_{12}$ (ковариация ошибок) & -0.499 (0.0340) & -0.507 (0.0358) & -0.519 (0.0368) & -0.576 (0.0379) & -0.515 (0.0113) \\ 
$\sigma_{22}$ (дисперсия ошибок ур-ия 2) & 0.950 (0.425) & 1.066 (0.0476) & 1.049 (0.0467) & 1.062 (0.0472) & 1.002 (0.0141) \\ 
\hline 
\end{tabular} 

Для проверки сходимости, например, можно рассчитать выборочные среднее и стандартное отклонение итоговых значений и посмотреть меняются ли  или остаются неизменными. В случае, если изменения незначительны, к примеру, менее чем 0,1 при 10,000 симуляций, тогда делается вывод о сходимости. Также возможно рассмотреть несколько цепей Маркова. Симуляции всегда будут коррелированы между собой, важно лишь, какова скорость сходимости автокорреляционной функции к нулю. Иногда высокая автокорреляция является внутренним свойством алгоритма. Тогда можно взять каждое десятое или сотое значение для уменьшения автокорреляции. 

Для проверки сходимости распределения алгоритма Гиббса к стационарному апостериорному распределению мы рассчитали  20-ть коэффициентов автокорреляции для симуляций апостериорного распределения после большого количества значений. Наличие автокорреляции в коэффициентах после прожига может свидетельствовать об отсутствии сходимости в целевом распределении. Когда количество симуляций мало, к примеру, 1,000, коэффициент автокорреляции иногда может превышать 0,06. Однако, если количество симуляций более 50,000, практически отсутствует автокорреляция до 20-го порядка и с увеличением количества симуляций автокорреляция уменьшается. В большинстве случаев, оценка коэффициентов корреляции не превышает 0.005. Можно легко проверить, что для N=1,000  априорный параметр $\tau$ практически не оказывает влияния на апостериорное распределение.


\subsection{Пополнение данных}

В некоторых случаях алгоритм Гиббса может использоваться для более широкого ряда моделей путем введения вспомогательных переменных. В частности, он подходит для моделей с латентными переменными, например, модели дискретного выбора,  модели урезанных и цензурированных данных и модели  смеси распределений, рассматриваемых в последних главах.

В скалярном случае значения латентной зависимой переменной $y^{*}$ ненаблюдаемы; наблюдаемы только значения $y=g(y^{*})$ для  некоторой функции $g$. % здесь опечатка в английском тексте
 Например, для логит и пробит моделей (см. Главу 14) известен только знак $y^{*}$, т.е. $y=1$, если $y^{*}>0$ и $y=0$, если $y^{*}{\leq}0$. 

Байесовский подход к анализу моделей скрытых переменных и, в частности, использование алгоритма Гиббса, хорошо дополняется заменой скрытых переменных на их оцененные  значения. 
Этот шаг доступен, если возможно записать прогнозную плотность скрытых переменных через наблюдаемые переменные. Процедура добавления предполагаемых значений как если бы эти значения были известны, называется пополнением данных. В качестве примера, в разделе 10.3.7 был рассмотрен EM алгоритм. В своей работе Таннер и Вонг (1987) продемонстрировали, что иногда генерировать выборку из апостериорного распределения затруднительно, в то время как после пополнения данных можно использовать алгоритм Гиббса.

Допустим, что апостериорную плотность распределения можно выразить через наблюдаемую переменную $y$ и ненаблюдаемую переменную $y^{*}$,

\begin{equation}
p(\theta|y)=\int_{y^{*}}p(\theta|y,y^{*})f(y^{*}|y)dy^{*},
\end{equation}

где интеграл в правой части может быть проинтерпретирован как усреднение по $y^{*}$. 

Аналогично с EM алгоритмом, метод пополнения данных предполагает повторение цикла из двух шагов: I --- imputation, пополнение данных, P --- posterior, уточнение апостериорного распределение. 


На первом шаге генерируются значения из плотности условного распределения $y^{*}$. При этом происходит интегрирование по $\phi$, которое появляется в распределении,  связывающем $y^{*}$ и $y$. Прогнозное распределение равно 

\begin{equation}
f(y^*|y)=\int_{\phi}f(y^{*}|y,\phi)f(\phi|y)\,d\phi.
\end{equation}

Имея сгенерированное значение согласно $p(\theta|y)$ мы можем сгенерировать значение из $f(y^{*},y)$, повторяя оба шага $m$ раз мы получим  вектор $y_{i}, i=1,\ldots,m$. Это завершает I-шаг.

В результате пополнения данных на $I$-ом шаге, $P$-шаг реализуется путем обновления текущего приближения к апостериорному распределению; а именно:

\begin{equation}
\text{Updated } p(\theta|y)=\frac{1}{m}\sum_{i=1}^{m}p(\theta|y,y_i^*).
\end{equation}

Далее, алгоритм возвращается на $I$-ый шаг.

При $m=1$, процедура состоит в оценке интеграла  (13.49) по алгоритму Гиббса. Применение метода пополнения данных для устранения проблемы пропущенных наблюдений подробно рассмотрено в главе 26.

\section {Байесовский выбор моделей}

В 7-ой и 8-ой главах были рассмотрены  вопросы тестирования гипотез, спецификации модели и сравнения моделей с  помощью частотного подхода к понятию вероятности. Этот раздел посвящен   изучению байесовских факторов ---  основного инструмента, используемого в байесовском анализе для оценивания степени уверенности в верности  гипотезы,  альтернативы тестирования нулевой гипотезы. Также байесовские  факторы могут  быть критерием выбора одной из моделей вне зависимости от того, являются ли они вложенными или нет. В эконометрической литературе, одним из первых исследователей, кто рассматривал байесовские факторы как критерий выбора модели, стал Зеллнер (1971, 1978). Изложение материала  построено на обзорной статье Касс и Рафтери (1995).

Обозначим вектор данных буквой $y$  и с помощью $H_1$ и $H_2$ --- две гипотезы, возможно невложенные, априорные вероятности равны $\Pr[H_1]$ и $\Pr[H_2]$, соответственно. Процессы Вероятности получения данных определенных значений обозначим через $\Pr[y|H_1]$ и $\Pr[y|H_2]$. % здесь в английском оригинале написана чушь :)
В соответствии с информацией из выборки  априорные вероятности преобразуются в апостериорные. Согласно теореме Байеса :

\begin{equation}
\Pr[H_k|y]=\dfrac{\Pr[y|H_k]\Pr[H_k]}{\Pr[y|H_1]\Pr[H_1]+\Pr[y|H_2]\Pr[H_2]}, k=1,2,
\end{equation}

и апостериорное отношение шансов равно

\begin{equation}
\dfrac{\Pr[H_{1}|y]}{\Pr[H_2|y]}=\dfrac{\Pr[y|H_1]\Pr[H_1]}{\Pr[y|H_2]\Pr[H_2]}{\equiv}B_{12}\dfrac{\Pr[H_1]}{\Pr[H_2]},
\end{equation}
 
где $B_{12}=\Pr[y|H_1]/\Pr[y|H_2]$ --- это байесовский фактор. Гипотеза 1 предпочитается, если апостериорное отношение шансов больше 1. Правая часть выражения (13.53) это апостериорное отношение шансов, которое равно произведению байесовского фактора и априорного отношения шансов. Если априорные вероятности равны между собой, т.е. $\Pr[H_1]=\Pr[H_2]$, тогда байесовский фактор равен отношению шансов в пользу $H_1$. При рассмотрении нескольких гипотез байесовский фактор рассчитывается для всех пар гипотез. Байесовский фактор определен даже если гипотезы не являются вложенными. 

Значение байесовского фактора рассчитывается как отношение фукнций правдоподобия. Значение фактора зависит от неизвестных параметров, обозначенных за $\theta_1$ и $\theta_2$, которые исчезают при усреднении или интегрировании  по априорной вероятности так, что

\begin{equation}
\Pr[y|H_k]=\int{\Pr[y|\theta_k,H_k]\pi(\theta_k|H_k)d\theta}, k=1,2,
\end{equation}

Согласно выводам раздела 13.2.5 значение выражения (13.54) равно частной или прогнозной вероятности данных при заданном априорном распределении.

При нахождении значения выражения могут возникнуть трудности из-за зависимости подынтегрального выражения от всех констант функции правдоподобия. Значения констант можно опустить при расчете апостериорной вероятности, но для расчета байесовских факторов значения факторов важны. Может потребоваться численный расчет подынтегрального выражения в (13.54) при отсутствии решения в явном виде, например, с помощью сэмплирования по важности. Подробный обзор литературы по расчету байесовского фактора представлен в работе Касс и Рафтери (1995). Вместе с тем, существуют асимптотические приближения для расчета байесовского фактора, которые легко рассчитываются пакетами, решающими задачу максимального правдоподобия.

Таблица 13.4. Интерпретация байесовских факторов 

\begin{tabular}{p{4cm}p{3cm}p{3cm}}
\hline 
Байесовский фактор $B_{12}$ & $2\ln B_{12}$ & Свидетельства в пользу $H_1$ \\
от 1 до 3 & от 0 до 2 & слабые \\
от 3 до 20 & от 2 до 6 & умеренные \\
от 20 до 150 & от 6 до 10 & сильные \\
больше 150 & больше 10 & очень сильные 
\end{tabular}


Проста интерпретация байесовского фактора через аргументы в пользу гипотезы $H_1$. «Байесовский фактор отражает в численном виде свидетельства, содержащиеся в данных, в пользу той или иной теории, выраженной с помощью статистической модели» (Касс и Рафтери, 1995, стр. 777). В частотном подходе к вероятности часто используют удвоенное значение логарифмического отношения правдоподобия. По аналогии, в байесовском подходе часто приводят удвоенный логарифм  байесовского фактора. В своей работе Рафтери и Касс приводят деление степени уверенности в гипотезе $H_1$ на несколько групп,  см.  таблицу 13.4.

Предположим, что поставлена задача сравнить две вложенные модели. Обозначим за $H_0$ модель с ограничениями и за $H_1$ модель без ограничений. Как было показано ранее для парного сравнения двух моделей через показатель апостериорного отношения шансов требуется расчет байесовского фактора. Для нулевой гипотезы байесовский фактор рассчитывается как:

\[
B_{01}=\frac{m(y|H_0)}{m(y|H_1)},
\]

где $m(y|H_j)$ частная функция правдоподобия соответствующая гипотезе $H_j$. Если модели $H_0$ и $H_1$ вложены, для расчета байесовских факторов можно использовать подход отношения  плотностей Сэвиджа-Дики (см. Вердинелли и Вассерман, 1995).

Важный вклад в методику расчета байесовского фактора сделал Чиб (1995), предложив существенно более простой метод, применимый не зависимо от того, являются ли модели вложенными. В основе подхода Чиба (1995) лежат две, связанные между собой идеи. Первая состоит в записи предельной плотности $m(y)$ для модели $H_k$ как отношения:

\[
m(y)=\dfrac{f(y|\theta)\pi(\theta)}{\pi(\theta|y)},
\]

где числитель равен произведению правдоподобия, включая константу, и априорной вероятности, а знаменатель --- апостериорная плотность $\theta$.
%%%%%%%%%% here!
Данное выражение получается в результате перегруппировки величин в (13.1), где $f(y)$ (ранее $\Pr[y|H_k]$) обозначена $m(y)$. Смысл его всего лишь в том частная плотность --- это  нормализующая константа. Во-вторых, успешное применение алгоритма MCMC, позволит получить оценку апостериорной плотности $\pi(\tilde{\theta}|y)$ в точке $\tilde{\theta}$. Следовательно,

\begin{equation}
\ln \hat{m}y=\ln {f(y|\tilde{\theta})}+\ln {\pi(\tilde{\theta})}-\ln {\pi(\tilde{\theta}|y)}
\end{equation}

Таким образом, если известны значения выражений в правой части уравнения, частную плотность можно рассчитать используя алгоритм Гиббса. Этот подход расширили Чиб и Желязков (2001), а именно, вместо алгоритма Гиббса авторы использовали алгоритм Метрополиса-Хастингса.

В сложных и сильно-параметризованных моделях, расчет байесовского фактора может вызвать затруднения. Вместе с тем,  критерий Шварца, также известный как байесовский информационный критерий (BIC) (см. раздел 8.5), может дать грубое приближение логарифма байесовского фактора. Формула для расчета BIC: $BIC=-2\ln {L}(\hat{\theta}_{ML})+\ln {N_q}$. BIC легко рассчитать, если известно значение логарифма функции правдоподобия.

Из выражения (13.52) следует, что отношение априорных вероятностей играет важную роль при выборе модели. Как правило, исследователь может самостоятельно выбрать эти вероятности. Данный вопрос  рассматривается в литературе, посвященной чувствительности байесовского фактора к выбору априорных вероятностей.

\section{Практические соображения}

В современной литературе по байесовским методам основное внимание уделяется марковским цепям. Построение марковских цепей требует большого количества вычислений, что требует хорошего программного обеспечения. На момент написания данной работы, последняя версия пакета BUGS (Bayesian inference Using Gibbs Sampling) --- WinBUGS -- была признана лучшей, особенно для иерархических моделей и решения проблем пропущенных данных. Последнюю версию BUGS можно найти на сайте. Подробная информация о программных пакетах, предназначенных для работы с байесовскими методами, содержится в работе Гамерманf (1997, Раздел 5.6).

В настоящее время актуальным остается вопрос длины марковской цепи. Разработаны способы проверки сходимости цепей, однако эти способы не универсальны. В своей работе Каппе и Роберт (2000) обозначили основные проблемы использования критериев сходимости, в том числе правила остановки. Важную роль здесь играет сложность условного распределения. Для скалярных параметров сходимость удобно анализировать графически, хотя есть  и формальные тесты (Гевеке, 1992). Вместе с тем,  Гельман и Рубин (1992) предлагают  использовать несколько цепей, где начальное значение для каждой цепи своё, что позволит проверить сходимость к одному и тому же апостериорному распределению. Зеллнер и Мин (1995) предложили несколько критериев сходимости, которые применимы, если апостериорное распределение может быть представлено явно.

\section{Библиографические заметки}

В настоящее время существует много книг, посвященных применению современных методов рассчета к байесовскому анализу, в том числе работы Гамермана (1997), а также Гельмана и др. (1995). Достаточно доступными являются книги Джилла (2002), Купа (2003) и Ланкастера (2004). Куп рассматривал применение байесовских методов ко многим стандартным нелинейным моделям и панельным данным. Также до сих пор актуальны результаты более ранних работ Зеллнера (1971) и Лимера (1978).

13.2 Работа Байеса (1764) доступно изложена в статье Стиглера (1961). Вначале Байес излагает некоторые свойства вероятности, например, $\Pr[A|B]=\Pr[A\cap B]/\Pr[B]$. Полученный результат Байес использует для определения апостериорной вероятности $\Pr[a<\theta<b |y]$, где $a$ и  $b$ --- границы, $y$ --- количество успешных испытаний по схеме Бернулли и $\theta$ искомая вероятность каждого успешного испытания. Байес использовал равномерное распределение, следовательно для апостериорной вероятности справедливо $f(\theta|y){\propto}f(y|\theta)$. Пример, рассмотренный Байесом, довольно трудный, он не смог аккуратно посчитать апостериорную вероятность, для этого требовалось неполное гамма-распределение, которое было изучено только в 20-м веке. Более популярным оказался подход Лапласа и других, получивший название метода обратной вероятности,  для которого также справедливо $f(\theta|y){\propto}f(y|\theta)$. Метод Байеса и метод обратной вероятности были заменены  методом максимального правдоподобия, разработанным Фишером (1992), чья работа прямо критиковала байесовский подход и метод обратной вероятности. 

Условия регулярности для сходимости к нормальному распределению апостериорной  оценки  изложены в работе Хейда и Джонстона (1979). Теорема Бернштейна-фон Мизеса  менее формально  но понятно изложена  в статье Трейна (2003).

13.3

Байесовский анализ линейных регрессий подробно рассмотрен у Зеллнера (1971) и Лимера (1978).

13.4

Интегрирование методом Монте-Карло хорошо представлено у Гевеке (1989) и Гевеке и Кина (2001).

13.5

Вводное описание алгоритма Гиббса приводят Казелла и Джордж (1992). Многочисленные статьи Чиба и совторов, а также Гевеке и соавторов, охватывают многие темы из микроэконометрики. Чиб и Гринберг (1996, раздел 3) приводят приложения MCMC, в частности к внешне не связанным регрессионным моделям, тобит и пробит моделям. Вместе с тем, авторы показали, что количество расчетов можно сократить, если соединить алгоритм Гиббса и метод пополнения данных. 
Пополнение данных необходимо для того, чтобы описать латентные переменные, которые используются для решения проблемы скрытых переменных в цензурированных моделях и моделях дискретного выбора. Чиб (2001), учитывая все новые достижения, провел детальный анализ применения MCMC для актуальных линейных и нелинейных моделей. Гевеке и Кин (2000) сосредоточили основное внимание на методах интегрирования как байесовских, так и небайесовских.


\subsubsection*{Упражнения}

21-1 Покажите, что если $\beta|\lambda{\sim}N[\mu,\lambda^{-1}\Sigma]$ и $\lambda{\sim}Gamma[\alpha/2,\alpha/2]$, то безусловное распределение $\beta$ является многомерным $t$-распределением с параметрами $(\mu,\Sigma,\alpha)$.

21-2 (Адаптировано из работы Чиба, 1992). Рассмотрим цензурированную регрессию или тобит-модель (см. Раздел 16.3), где $y^{*}=x'\beta+\varepsilon, \varepsilon$ независимо и одинаково распределены ${N[0,\sigma^{2}]}$, и значения $y$ наблюдаемо, если $y^{*}>0$ и не определено, если $y^{*}{\leq}0$. Допустим, что количество цензурированных наблюдений $y$ равно $N_0$ и $y_0$ их обозначает. Введем латентную переменную $z$, которая соответствует цензурированным наблюдениям, т.е. $z_{i}<0$ для $i$-го цензурированного наблюдения. Для генерирования латентных переменных $z_i$ может использоваться метод пополнения данных. Латентные переменные здесь представляют собой множество независимых случайных величин и имеют усеченное нормальное распределение на интервале $(-\infty,0)$ и плотность распределения  равна $\phi(z_i|y_i,\beta,\sigma^2)/(1-\Phi(x'_{j}\beta/\sigma))$, $-\infty<z_i<0$, где $\phi$ и $\Phi$ являются функцией плотности и функцией распределения  нормальной величины. Допустим, что $\beta$ имеет нормальное априорное распределение, а $\sigma^{-2}$ гамма априорное распределение.

(a) Покажите, что возможно явно получить условные распределения для $z_i,\beta$ и $\sigma^{-2}$.

(b) Используя результаты части (a) кратко опишите алгоритм Гиббса для симуляции значений $z_i, \beta$ и $\sigma^{-2}$.

(c) Покажите как могут быть получены разумные стартовые значения $\beta$ и $\sigma^{-2}$. 


