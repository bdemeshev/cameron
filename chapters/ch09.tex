
\chapter{Полупараметрические методы}
\section{Введение}

В этой главе мы рассматриваем методы для анализа данных, которые требуют меньше от спецификации модели, чем методы, описанные в предыдущих главах.

Начнём с непараметрического оценивания. Для него требуются минимальные предположения относительно процесса, порождающего данные. Одним из наиболее ярких примеров является оценивание непрерывной функции плотности с помощью ядерной оценки. 
Привлекательность этого способа состоит в том, что он позволяет получить более гладкую оценку, чем гистограмма. Вторым ярким примером является непараметрическая регрессия на скалярный регрессор, например, ядерная регрессия. Она накладывает гибкую кривую на диаграмму рассеяния в координатах $(x,y)$. В данном случае нет никаких параметрических ограничений на форму кривой. Непараметрические оценки имеют множество применений, включая описание исходных данных, анализ данных и оценённых остатков из регрессионной модели, а также описание оценок параметров, полученных с помощью метода Монте-Карло.

Эконометрический анализ уделяет особое внимание многомерной регрессии скалярного $y$ на многомерный регрессор $x$. Теоретически возможно применять непараметрические методы для множественной регрессии на очень больших выборках, но на практике это затруднительно, поскольку данные должны быть разделены по нескольким измерениям, что приводит к слишком малому количеству точек в каждой группе.

По этой причине эконометристы уделяли больше внимания полупараметрическим методам. Они сочетают параметрическую компоненту, которая значительно уменьшает размерность, и непараметрическую компоненту. Одним из важных применений этих методов является то, что они позволяют получать более гибкие модели условного математического ожидания. Например, условное математическое ожидание $\E[y|x]$ может быть параметризовано в одноиндексную форму $g(x'\beta)$, где функциональная форма для $g(\cdot)$ не специфицирована. Вместо этого она непараметрически оценена вместе с неизвестными параметрами $\beta$. Другое важное применение позволяет ослабить предположения о распределении, которые при неверной спецификации приводят к получению несостоятельных оценок параметров. Например, мы хотим получить состоятельные оценки $\beta$ в линейной регрессионной модели $y = x'\beta + \e$, когда данные $y$ усечены или ограничены (см. главу 16). Но в то же время мы не хотим указывать конкретное распределение $\e$.

Асимптотическая теория для непараметрических методов отличается от теории для более параметрических методов. Оценки получаются путём разделения данных на очень маленькие части при $N \rightarrow \infty$. Оценивание локального поведения происходит отдельно для каждой части. Так как при оценивании каждой части используются не все $N$ наблюдений, скорость сходимости меньше, чем в предыдущих главах. Тем не менее, в простейших случаях непараметрические оценки по-прежнему имеют асимптотически нормальное распределение. В некоторых основных случаях полупараметрической регрессии оценки параметров $\beta$ имеют стандартные свойства сходимости со скоростью $N^{-1/2}$, поэтому масштабирование на $\sqrt{N}$ приводит к получению предельного нормального распределения, в то время как непараметрическая компонента модели сходится с меньшей скоростью $N^{-r}$, где $r < 1/2$.

Так как непараметрические методы --- это методы локального усреднения, различный выбор локальности приводит к различным результатам на конечных выборках. В некоторых случаях при наличии ограничений существуют правила или методы для определения ширины окна, которые используются для локального усреднения. Эти правила аналогичны правилам для определения количества столбцов в гистограмме с учётом количества наблюдений. Кроме того, обычной практикой является использование ненаучных методов для выбора ширины окна. Эти методы предлагают использовать график, который на глаз выглядит достаточно гладким, но в то же время обнаруживает особенности интересующей зависимости.

Описание непараметрических методов занимает основную часть этой главы, потому что они представляют отдельный интерес и потому что они необходимы для полупараметрических методов, которые изложены в первую очередь в главах, посвящённых моделям с дискретными и ограниченным зависимыми переменными. Ядерным методам уделяется особое внимание, так как они относительно просты, для того чтобы их представлять. Как говорил Хэрдл (1990, с. xi): <<Считается, что все методы сглаживания в асимптотическом смысле по сути эквивалентны ядерному сглаживанию>>.

Раздел 9.2 содержит примеры непараметрического оценивания плотности и примеры непараметрических регрессий, которые проводятся на реальных данных. Ядерное оценивание плотности представлено в разделе 9.3. Локальные регрессии рассмотрены в разделе 9.4, чтобы отразить логику использования ядерной регрессии, которая рассмотрена в разделе 9.5. Раздел 9.6 содержит непараметрические регрессионные методы, отличные от ядерных методов. Далее в разделе 9.7 представлена тема полупараметрической регрессии.

\section{Непараметрический пример: почасовая заработная плата}

В качестве примера рассмотрим почасовую заработную плату и образование для 175 женщин в возрасте 36 лет, которые работали в 1993 году. Данные взяты из Мичиганского панельного опроса динамики доходов (Michigan Panel Survey of Income Dynamics). Легко обнаружить, что распределение почасовой заработной платы скошено вправо. По этой причине мы моделируем $\ln wage$ --- натуральный логарифм почасовой заработной платы.

Мы приведём всего один пример непараметрического оценивания плотности и пример одной непараметрической регрессии, а также проиллюстрируем важную роль выбора ширины окна. В разделах 9.3 --- 9.6 описана лежащая в основе теория.

\subsection{Оценивание функции плотности с помощью непараметрических методов}

Гистограмма натурального логарифма заработной платы приведена на графике 9.1. Уточним, что на гистограмме изображены 30 столбцов, ширина каждого столбца составляет примерно 0.20. Это необычно узкая ширина столбцов для 175 наблюдений, но многие детали не видны при большей ширине столбцов. Логарифмированные данные по заработной плате выглядят довольно симметричными, хотя они, возможно, немного скошены влево.

\vspace{5cm}

График 9.1: Гистограмма натурального логарифма почасовой заработной платы. Данные для 175 американских женщин в возрасте 36 лет, которые работали в 1993.

Стандартная сглаженная непараметрическая оценка плотности --- это ядерная оценка плотности, которая определена в (9.3). В данном случае мы используем ядро Епанечникова, которое определено в таблице 9.1.

Важным решением на практике является выбор ширины окна. Для этого примера оценка Сильвермана, определённая в (9.13), даёт $h = 0.545$. Тогда ядерная оценка является средневзешенным тех наблюдений, у которых значение логарифма заработной платы находится в пределах 0.21 единиц логарифма заработной платы в текущей точке оценивания. Наибольший вес имеют наблюдения  ближайшие к текущей точке оценивания. График 9.2 отражает три ядерные оценки плотности с шириной окна 0.273, 0.545 и 1.091 соответственно. Они соответствуют ширине окна, равной половине оценки, оценке Сильвермана и оценке, умноженной на два. Очевидно, что наименьшая ширина окна слишком мала, так как она приводит к слишком негладкой оценке плотности. Наибольшая ширина окна наоборот слишком сглаживает данные. Средняя ширина окна, равная 0.545, кажется наилучшим выбором, так как она даёт достаточно гладкую оценку плотности.

\vspace{5cm}

График 9.2: Ядерная оценка плотности для логарифма заработной платы для трёх различных вариантов ширины окна, используя ядро Епанечникова. Выбранная ширина окна равна $h = 0.545$. Используются те же данные, что и для графика 9.1.

Что можно сделать с этой ядерной оценкой плотности? Один вариант состоит в том, чтобы сравнить эту плотность с плотностью нормального распределения путём наложения плотности нормального распределения с математическим ожиданием, равным выборочному среднему, и дисперсией, равной выборочной дисперсии. Здесь не приведён график, но он показывает, что ядерная оценка плотности с предпочтительной шириной окна 0.545 является более остроконечной, чем для нормального распределения. Второй вариант заключается в том, чтобы сравнить ядерную оценку плотности логарифма заработной платой для различных подгрупп, например, по уровню образования или по полному или неполному рабочему дню.


\subsection{Непараметрическая регрессия}

Рассмотрим взаимосвязь между логарифмом заработной платы и образованием. Непараметрический метод, который здесь используется, --- локальный линейный регрессионный метод $LOWESS$, локальная средневзвешенная оценка (см. уравнение (9.16) и раздел 9.6.2).

Линия локально взвешенной регрессии в каждой точке $x$ получается с помощью центрированного подмножества наблюдений, которое включает  по умолчанию ближайшие $0.8N$ наблюдений. Здесь $N$ --- размер выборки, а веса снижаются по мере удаления от $x$. Для значений $x$, близких к краям, используется нецентрированное подмножество меньшего размера.

График 9.3 отражает диаграмму рассеяния логарифма заработной платы и образования, а также три регрессионные кривые $LOWESS$ для ширины окна, равной 0.8, 0.4 и 0.1. Первые два варианта дают аналогичные кривые. Зависимость кажется квадратичной, но это может быть иллюзией, так как данных для низких уровней образования довольно мало. Тех, кто учился в школе менее 10 лет, менее 10\% выборки. Для большей части данных линейная зависимость тоже может хорошо подходить. Для простоты мы не приводим 95\% доверительные интервалы, хотя это можно сделать.

\vspace{5cm}

График 9.3: Непараметрическая регрессия логарифма заработной платы на образование для трёх различных вариантов ширины окна, используя локально-линейную регрессию $LOWESS$. Используются те же данные, что и для графика 9.1.

\section{Ядерное оценивание плотности}

Непараметрические оценки плотности могут быть полезны для сравнения закона распределения между различными группами и для сравнения с эталонной плотностью такой, как плотность нормального распределения. По сравнению с гистограммой они имеют то преимущество, что они обеспеченивают более гладкую оценку плотности. Ключевое решение, аналогичное выбору количества столбцов на гистограмме, --- выбор ширины окна. Мы концентрируемся на стандартной непараметрической оценке плотности, ядерной оценке плотности. Мы представляем подробное описание, так как для оценки плотности можно более просто получить результаты, которые также релевантны для регрессии.


\subsection{Гистограмма}

Гистограмма --- это оценка плотности, которая получается с помощью разделения $x$ на одинаковые по размеру интервалы и расчёта доли выборки в каждом интервале.

Мы рассматриваем более формальное представление гистограммы, то, которое можно расширить до более гладкой ядерной оценки плотности. Рассмотрим оценивание плотности $f(x_0)$ скалярной непрерывной случайной величины $x$, которую мы хотим оценить в точке $x_0$. Так как плотность --- производная функции распределения $F(x_0)$ (т.е., $f(x_0) = d F(x_0)/dx$), мы получаем
\[
f(x_0) = \underset{h \rightarrow 0}{\lim} \frac{F(x_0 + h) - F(x_0 - h)}{2h} = \underset{h \rightarrow 0}{\lim} \frac{\Pr[x_0 - h < x < x_ 0 + h]}{2h}.
\]
Для выборки $\{x_i, i = 1, \dots, N\}$ размера $N$ это означает использование оценки
\begin{equation}
\hat{f}_{HIST}(x_0) = \frac{1}{N} \sum_{i=1}^N \frac{{\bf{1}} (x_0 - h < x < x_ 0 + h)}{2h},
\end{equation}
где функция-индикатор
\[
{\bf{1}}(A) =  
\begin{cases} 
1 \hspace{0.2cm} \text{если событие А случается,} \\ 
0 \hspace{0.2cm} \text{в противном случае.} 
\end{cases}
\]

Оценка $\hat{f}_{HIST}(x_0)$ --- это высота столбика диаграммы, центрированного относительно $x_0$ с шириной $2h$, она равняется доле выборки, которая лежит в промежутке от $x_0 - h$ до $x_0 + h$. Если оценивать $\hat{f}_{HIST}(x_0)$ с постоянным шагом по $x$, через каждые $2h$ единиц, то можно получить гистограмму. 

Оценка $\hat{f}_{HIST}(x_0)$ присваивает одинаковый вес всем наблюдениям в $x_0 \pm h$. Это становится ясно, если переписать (9.1) как
\begin{equation}
\hat{f}_{HIST}(x_0) = \frac{1}{Nh} \sum_{i=1}^N \frac{1}{2} \times {\bf{1}} \left( \left|\frac{x_i - x_0}{h}\right| < 1 \right).
\end{equation}
Это приводит к оценке плотности, которая является ступенчатой функцией, даже если лежащая в основе плотность непрерывная. Более гладкие оценки могут быть получены с использованием взвешивающих функций, отличных от функции-индикатора, которая выбрана здесь.

\subsection{Ядерная оценка функции плотности}

Ядерная оценка плотности, введённая Росенблаттом (1956), обобщает оценку гистограммы (9.2) с помощью альтернативной взвешивающей функции. Таким образом,
\begin{equation}
\hat{f}(x_0) =  \frac{1}{Nh} \sum_{i=1}^N  K \left( \frac{x_i - x_0}{h} \right).
\end{equation}
Взвешивающая функция $K(\cdot)$ называется ядерной функцией и удовлетворяет ограничениям, которые приведены в следующем разделе. Параметр $h$ --- параметр сглаживания, который называется шириной окна, и $h$, умноженное на два, --- ширина окна, умноженная на два. Некоторые источники называет шириной окна именно $2h$. Плотность оценивается путём оценивания $\hat{f}(x_0)$ на более широком диапазоне значений $x_0$, чем диапазон значений, используемый для формирования гистограммы. В стандартном случае оценивание производится на выборочных значениях $x_1, \dots, x_N$. Это также позволяет получать оценку плотности, которая является более гладкой, чем оценка гистограммы.

\subsection{Ядерные функции}

Ядерная функция $K(\cdot)$ --- непрерывная функция, симметричная относительно нуля, которая имеет единичный интеграл и удовлетворяет дополнительным условиям ограниченности. Как и Ли (1996), мы предполагаем, что ядро удовлетворяет следующим условиям:
\begin{enumerate}
\item $K(z)$ симметрична относительно нуля и непрерывна.
\item $\int K(z)dz = 1$, $\int zK(z)dz = 0$ и $\int |K(z)|dz < \infty$.
\item Или (а) $K(z) = 0$, если $|z| \geq z_0$ для некоторых $z_0$, или (б) $|z|K(z) \rightarrow 0$ при $|z| \rightarrow \infty$.
\item $\int z^2K(z)dz = k$, где $k$ --- константа.
\end{enumerate}

На практике ядерные функции работают лучше, если они удовлетворяют условию (3a), а не только более слабому условию (3б). Тогда будет рассматриваться интервал $[-1,1]$, а не $[- z_0, z_0]$. Этот интервал является нормированием для удобства, и, как правило, $K(z)$ ограничена интервалом $z \in [-1,1]$.

Некоторые часто используемые ядерные функции приведены в таблице 9.1. Равномерное ядро использует те же веса, что и гистограмма с шириной столбцов $2h$, но оно даёт гистограмму, которая оценивается на серии точек $x_0$, а не с использованием фиксированных столбцов. Гауссово ядро удовлетворяет (3б), а не (3а), потому что оно не ограничивает $z \in [-1,1]$. Ядро порядка $p$ --- ядро, первый ненулевой момент которого является $p$ моментом. Первые семь ядер имеют второй порядок и удовлетворяют второму условию из пункта 2. Последние два ядра имеют четвёртый порядок. Ядра более высокого порядка могут увеличить скорость сходимости, если $f(x)$ дважды и более дифференцируема (см. раздел 9.3.10). Однако они могут принимать отрицательные значения. В таблице 9.1 для некоторых ядер также представлен параметр $\delta$, который определён в (9.11) и который используется в разделе 9.3.6 для выбора ширины окна.

Если есть $K(\cdot)$ и $h$, то легко найти оценку. Если ядерная оценка оценивается в $r$ различных значениях $x_0$, то вычисление ядерной оценки требует максимум $Nr$ операций, когда ядро имеет неограниченный носитель. На практике можно значительно уменьшить число необходимых операций, см., например, Хэрдл (1990, стр. 35).


\begin{table}[h]
\begin{center}
\caption{\label{tab:pred} Ядерные функции: наиболее часто используемые примеры}
\begin{minipage}{16.5cm}
\begin{tabular}[t]{lll}
\hline
\hline
\bf{Ядро}\footnote{Константа $\delta$ определена в (9.11). Она используется для получения оценки Сильвермана, которая приведена в (9.13).} & \bf{Ядерная функция $K(z)$} & \bf{$\delta$} \\
\hline
Равномерное (или прямоугольное) & $\frac{1}{2} \times {\bf{1}} (|z| < 1)$ & 1.3510 \\
Треугольное (или треугольник) & $(1 - |z|) \times {\bf{1}} (|z| < 1)$ & -- \\
Епанечникова (или квадратичное) & $\frac{3}{4} (1 - z^2) \times {\bf{1}} (|z| < 1)$ & 1.7188 \\
Квартическое (или биквадратичное) & $\frac{15}{16} (1 - z^2)^2 \times {\bf{1}} (|z| < 1)$ & 2.0362 \\
Триквадратичное & $\frac{35}{32} (1 - z^2)^3 \times {\bf{1}} (|z| < 1)$ & 2.3122 \\
Трикубическое & $\frac{70}{81} (1 - |z|^3)^3 \times {\bf{1}} (|z| < 1)$ & -- \\
Гауссово (или нормальное) & $(2\pi)^{-1/2}\exp(-z^2/2)$ & 0.7764 \\
Гауссово четвёртого порядка & $\frac{1}{2}(3-z)^2(2\pi)^{-1/2}\exp(-z^2/2)$ & -- \\
Квартическое четвёртого порядка & $\frac{15}{32}(3-10z^2+7z^4)\times {\bf{1}} (|z| < 1)$ & -- \\
\hline
\hline
\end{tabular}
\end{minipage}
\end{center}
\end{table}

\subsection{Пример ядерной плотности}

Выбор ширины окна $h$ уже был проиллюстрирован на графике 9.2.

Здесь мы проиллюстрируем выбор ядра, используя сгенерированные данные --- случайную выборку, которая состоит из 100 наблюдений и имеет нормальное распределение $\mathcal{N}[0,25^2]$. Для этой выборки выборочное среднее равно 2.81, а выборочное стандартное отклонение равно 25.27.

График 9.4 показывает эффекты  использования различных ядер. Для Епанечникова, Гауссова, квартического и равномерного ядер оценка Сильвермана, приведённая в (9.13), даёт ширину окна, которая равна 0.545, 0.246, 0.246 и 0.214 соответственно. Полученные ядерные оценки плотности очень похожи, даже равномерное ядро, которое порождает скользящую гистограмму. Различие оценок плотности для разных ядер гораздо меньше, чем для разной ширины окна. Это видно на графике 9.2.

\vspace{5cm}

График 9.4: Ядерные оценки плотности для логарифма заработной платы для четырёх различных ядер с использованием оценки Сильвермана для ширины окна. Используются те же данные, что и для графика 9.1.

\subsection{Статистические выводы}

Мы рассмотрим распределение ядерной оценки плотности $\hat{f}(x)$ при заданных $K(\cdot)$ и $h$, предполагая, что данные $x$ независимы и одинаково распределены. Оценка $\hat{f}(x)$ является смещённой. Это смещение стремится асимптотически к нулю, если ширина окна $h \rightarrow 0$ при $N \rightarrow \infty$, поэтому $\hat{f}(x)$ является состоятельной оценкой. Тем не менее, параметр смещения необязательно исчезает в асимптотическом нормальном распределении $\hat{f}(x)$. Это затрудняет статистические выводы.

\begin{center}
Математическое ожидание и дисперсия
\end{center}

Получение математического ожидания и дисперсии $\hat{f}(x)$ описано в разделе 9.8.1, в предположении, что вторая производная $f(x)$ существует и  ограничена. Также предполагается, что ядро удовлетворяет условию $\int zK(z)dz = 0$, которое является вторым свойством ядра из раздела 9.3.3.

Ядерная оценка плотности смещена с параметром смещения $b(x_0)$, который зависит от ширины окна и кривизны истинной плотности и используемого для оценки ядра:
\begin{equation}
b(x_0) = \E[\hat{f}(x_0)] - f(x_0) = \frac{1}{2}h^2 f''(x_0) \int z^2K(z)dz.
\end{equation}
Ядерная оценка смещена на порядок малости $O(h^2)$, где мы используем обозначение порядка малости, т.е. функция $a(h)$ имеет порядок малости $O(h^k)$, если $a(h)/h^k$ конечно. Смещение исчезает асимптотически, если $h \rightarrow 0$ при $N \rightarrow \infty$.

В предположении, что $h \rightarrow 0$ и $N \rightarrow \infty$, дисперсия ядерной оценки плотности имеет вид:
\begin{equation}
\V[\hat{f}(x_0)] = \frac{1}{Nh} f(x_0) \int K(z)^2dz + o\left( \frac{1}{Nh} \right),
\end{equation}
где функция $a(h)$ имеет порядок малости $O(h^k)$ при $a(h)/h^k \rightarrow 0$. Дисперсия зависит от размера выборки, ширины окна, истинной плотности и ядра. Дисперсия исчезает, если $Nh \rightarrow \infty$. Для этого необходимо, чтобы при $h \rightarrow 0$ дисперсия уменьшалась с меньшей скоростью, чем при $N \rightarrow \infty$.

\begin{center}
Состоятельность
\end{center}

Ядерная оценка поточечно соcтоятельна, то есть она состоятельна в конкретной точке $x = x_0$, если и смещение, и дисперсия исчезают. Это происходит в случае, если $h \rightarrow 0$ и $Nh \rightarrow \infty$.

Можно показать, что для оценивания $f(x)$ для всех значений $x$ может действовать более сильное условие равномерной сходимости $\sup_{x_0} |\hat{f}(x_0) - f(x_0)| \stackrel{p}{\rightarrow} 0$ при $Nh/\ln N \rightarrow \infty$. В этом случае $h$ должно быть больше, чем в случае поточечной сходимости.

\begin{center}
Асимптотическая нормальность
\end{center}

Предыдущие результаты показывают, что асимптотически $\hat{f}(x_0)$ имеет математическое ожидание $f(x_0) + b(x_0)$ и дисперсию $(Nh)^{-1}f(x_0)\int K(z)^2dz$. Отсюда следует, что если можно применить центральную предельную теорему, то ядерная оценка плотности имеет предельное распределение
\begin{equation}
\sqrt{Nh}(\hat{f}(x_0) - f(x_0) - b(x_0)) \stackrel{d}{\rightarrow} \mathcal{N} \left[ 0, f(x_0) \int K(z)^2dz \right].
\end{equation}
Тот вариант центральной предельной теоремы, который применяется в данном случае, не является стандартным и требует выполнения условия (4), см., например, Ли (1996, стр. 139) или Пагана и Улла (1999, стр. 40).

Важно отметить наличие параметра смещения $b(x_0)$, который был определён в (9.4). Для типичных вариантов ширины окна этот параметр не исчезает, усложняя получение доверительных интервалов (они представлены в разделе 9.3.7).

\subsection{Выбор ширины окна}

Выбор ширины окна $h$ гораздо более важен, чем выбор ядерной функции $K(\cdot)$. Существует разница между выбором небольшого значения $h$, чтобы уменьшить смещённость, и выбором большого значения $h$, чтобы обеспечить гладкость. Стандартный способ в данном случае --- использовать среднеквадратичную ошибку $(MSE)$, которая равна сумме квадрата смещения и дисперсии.

Из (9.4) смещение имеет порядок малости $O(h^2)$, и из (9.5) дисперсия имеет порядок малости $O((Nh)^{-1})$. Интуитивно $MSE$ минимизируется с помощью выбора $h$ так, чтобы квадрат смещения и дисперсия имели один и тот же порядок. Таким образом, $h^4 = (Nh)^{-1}$. Это означает, что оптимальная ширина окна $h = O(N^{-0.2})$ и $\sqrt{Nh} = O(N^{0.4})$. Теперь мы приведём более формальное описание, которое включает практическую оценку для $h$.
 
\begin{center}
Cредняя интегрированная квадратическая ошибка
\end{center} 

Локальный показатель эффективности ядерной оценки плотности в точке $x_0$ --- это $MSE$ 
\begin{equation}
MSE[\hat{f}(x_0)] = \E[(\hat{f}(x_0) - f(x_0))^2],
\end{equation}
где ожидание берётся от функции плотности $f(x)$. Так как $MSE$ равняется сумме дисперсии и квадрата смещения, (9.4) и (9.5) дают $MSE$ ядерной оценки плотности 
\begin{equation}
MSE[\hat{f}(x_0)] \simeq \frac{1}{Nh} f(x_0) \int K(z)^2dz + \Bigl\{ \frac{1}{2}h^2 f''(x_0) \int z^2K(z)dz \Bigr\}^2.
\end{equation}

Чтобы получить глобальный показатель эффективности во всех значениях $x_0$, мы начнём с определения интегрированной квадратической ошибки $(ISE)$
\begin{equation}
ISE(h) = \int (\hat{f}(x_0) - f(x_0))^2 dx_0,
\end{equation}
непрерывный аналог суммирования квадратов ошибок по всем $x_0$ в дискретном случае. Это можно записать как функцию от $h$, чтобы подчеркнуть зависимость от ширины окна. Затем мы устраняем зависимость $\hat{f}(x_0)$ от значений $x$, отличных от $x_0$, взяв математическео ожидаемое $ISE$ от плотности $f(x)$. Это даёт среднюю интегрированную квадратическую ошибку $(MISE)$,
\[
MISE(h) = \E[ISE(h)] = \E \left[ \int (\hat{f}(x_0) - f(x_0))^2 dx_0 \right] = \int \E \left[(\hat{f}(x_0) - f(x_0))^2 dx_0 \right] = \int MSE[\hat{f}(x_0)]dx_0,
\]
где $MSE[\hat{f}(x)]$ определено в (9.8). Из предыдущих алгебраических вычислений $MISE$ равно интегрированной среднеквадратической ошибке $(IMSE)$.

\begin{center}
Оптимальная ширина окна
\end{center}

Оптимальная ширина окна является минимумом $MISE$. Дифференцируя $MISE(h)$ по $h$ и приравнивая производную к нулю, можно получить оптимальную ширину окна
\begin{equation}
h^* = \delta \left( \int f''(x_0)^2 dx_0 \right)^{-0.2} N^{-0.2},
\end{equation}
где $\delta$ зависит от используемой ядерной функции,
\begin{equation}
\delta = \left( \frac{\int K(z)^2dz}{(\int z^2K(z)dz)^2} \right)^{0.2}.
\end{equation}
Этот результат получен Сильверманом (1986).

Так как $h^* = O(N^{-0.2})$, мы имеем $h^* \rightarrow 0 $ при $N \rightarrow \infty$ и $Nh^* = O(N^{0.8}) \rightarrow \infty$, что требуется для состоятельности.

Смещение в $\hat{f}(x_0)$ имеет порядок малости $O(h^{*2}) = O(N^{-0.4})$, но оно исчезает при $N \rightarrow \infty$. Для оценки гистограммы можно показать, что $h^* = O(N^{-0.2})$ и $MISE(h^*) = O(N^{-2/3})$ уступает  $MISE(h^*) = O(N^{-4/5})$ для ядерной оценки плотности.

Оптимальная ширина окна зависит от кривизны плотности, и $h$ меньше, если $f(x)$ может сильно меняться.

\begin{center}
Оптимальное ядро
\end{center}

Оптимальная ширина окна зависит от выбора ядра (см. (9.10) и (9.11)). Можно показать, что $MISE(h^*)$ мало отличается для  разных ядер при условии, что различные оптимальные $h^*$ используются для различных ядер (график 9.4 это иллюстрирует). Можно показать, что оптимальное ядро --- ядро Епанечникова, хотя его превосходство незначительно.

Выбор ширины окна гораздо более важен, чем выбор ядра, и из (9.10) он зависит от ядра.

\begin{center}
Оценка ширины окна 
\end{center}

Оценка ширины окна --- это простая формула для $h$, которая зависит от размера выборки $N$ и стандартного отклонения $s$.

Стоит начать с предположения, что данные имеют нормальное распределение. Тогда  \\ $\int f''(x_0)^2dx_0 = 3/(8\sqrt{\pi}\sigma^5) = 0.2116/\sigma^5$. В таком случае (9.10) можно преобразовать как
\begin{equation}
h^* = 1.3643\delta N^{-0.2}s,
\end{equation}
где $s$ --- выборочное стандартное отклонение $x$ и $\delta$ приведены в таблице 9.1 для различных ядер. Для Епанечникова ядра $h^* = 2.345N^{-0.2}s$, а для Гауссова ядра $h^* = 1.059N^{-0.2}s$. Для нормального ядра ширина окна гораздо меньше, потому что в отличие от большинства ядер нормальное ядро даёт некоторый вес $x_i$, даже если $|x_i - x_0| > h$. На практике используют оценку Сильвермана
\begin{equation}
h^* = 1.3643\delta N^{-0.2}\min(s,iqr/1.349),
\end{equation}
где $iqr$ --- выборочное межквартильное отклонение. При этом $iqr/1.349$ используется в качестве альтернативной оценки $\sigma$. Она защищает от выбросов, которые могут увеличить $s$ и привести к слишком большому значению $h$.

Эти оценки $h$ хорошо работают на практике, особенно для симметричных одномодальных плотностей, даже если $f(x)$ не является плотностью нормального распределения. Тем не менее, для проверки всегда следует попробовать различные варианты ширины окна такие, как дважды и половина оценки Сильвермана.

Для примера на графиках 9.2 и 9.4 мы имеем $177^{-0.2} = 0.3551$, $s = 0.8282$ и $iqr/1.349 = 0.6459$, поэтому (9.13) даёт $h^* = 0.3173\delta$. Для ядра Епанечникова, например, это получается $h^* = 0.545$, так как $\delta = 1.7188$ из таблицы 9.1.

\begin{center}
Кросс-валидация
\end{center}

Из (9.9) $ISE(h) = \int \hat{f}^2(x_0)dx_0 - 2\int \hat{f}(x_0)f(x_0)dx_0 + \int f^2(x_0)dx_0$. Третье слагаемое не зависит от $h$. Альтернативный подход позволяет оценить первые два члена $ISE(h)$ с помощью
\begin{equation}
CV(h) = \frac{1}{N^2h} \sum_i \sum_j K^{(2)} \left( \frac{x_i - x_j}{h} \right) - \frac{2}{N} \sum_{i=1}^N \hat{f}_{-i}(x_i),
\end{equation}
где $K^{(2)}(u) = \int K(u - t)K(t)dt$ --- свёртка $K$ с самим собой, а $\hat{f}_{-i}(x_i)$ --- ядерная оценка $f(x_i)$ с выбрасыванием одного наблюдения. Вывод можно посмотреть у Ли (1996, стр. 137) или Пагана и Улла (1999 стр.51). Оценка $h_{CV}$, полученная с помощью кросс-валидации, выбирается так, чтобы она минимизировала $\widehat{CV}(h).$ Можно показать, что $h_{CV} \stackrel{p}{\rightarrow} h^*$ при $N \rightarrow \infty$, но скорость сходимости очень низкая.

Вычислить $h_{CV}$ довольно тяжело, потому что необходимо вычислить $ISE(h)$ для ряда значений $h$. Часто нет необходимости проводить кросс-валидацию для ядерного оценивания плотности, так как оценка Сильвермана обычно является хорошей отправной точкой.

\subsection{Доверительные интервалы}

Ядерные оценки плотности, как правило, представлены без доверительных интервалов, но можно построить точечные доверительные интервалы для $f(x_0)$, где <<точечные>> означает построенные для определённого значенияы $x_0$. Простая процедура состоит в том, чтобы получить доверительные интервалы в небольшом количестве оцениваемых точек $x_0$, например, в десяти точках. Разумно взять эти точки на равных расстояниях по всему диапазону $x$. Потом надо нанести эти доверительные интервалы на график вместе с оценкой  плотности.

Результат (9.6) приводит к следующему 95\% доверительному интервалу для $f(x_0)$:
\[
f(x_0) \in \hat{f}(x_0) - b(x_0) \pm 1.96 \times \sqrt{\frac{1}{Nh}\hat{f}(x_0)\int K(z)^2dz}.
\]

Для большинства ядер $\int K(z)^2dz$ можно легко получить с помощью аналитических методов.

Ситуация усложняется из-за наличия параметра смещения, который нельзя игнорировать при конечных выборках, хотя асимптотически $b(x_0) \stackrel{p}{\rightarrow} 0$. Это так, потому что при оптимальной ширине окна $h^* = O(N^{-0.2})$ смещение нормированной случайной величины $\sqrt{Nh}(\hat{f}(x_0) - f(x_0))$, которая представлена в (9.6), не исчезает, так как $\sqrt{Nh^*} \times O(h^{*2}) = O(1)$. Можно оценить смещение, используя (9.4) и ядерную оценку $f''(x_0)$, но на практике оценка $f''(x_0)$ имеет много шумов. Вместо этого стандартный метод
состоит в уменьшении смещения при вычислении доверительного интервала, а не самой $\hat{f}(x_0)$. Это делается с помощью недосглаживания, то есть выбора $h < h^*$ такого, что $h^* = o(N^{-0.2})$. Другие подходы включают использование ядер более высокого порядка таких, как ядра четвёртого порядка, которые приведены в таблице 9.1, или использование метода бутстреп (см. раздел 11.6.5).

Можно также вычислить доверительные интервалы для $f(x)$ для всех возможных значений $x$. Они будут шире, чем точечные доверительные интервалы для каждого значения $x_0$.

\subsection{Оценивание производных плотности}

В некоторых случаях необходимо получить оценки производных плотности. Например, оценивание параметра смещения $\hat{f}(x_0)$, который приведён в (9.4), требует оценить $f''(x_0)$.

Для простоты мы приводим оценки первой производной. Разностный подход использует $\hat{f}'(x_0) = [\hat{f}(x_0 + \Delta) - \hat{f}(x_0 - \Delta)]/2\Delta$. Альтернативный подход вместо этого подразумевает взятие первой производной $\hat{f}(x_0)$ из (9.3), что даёт $\hat{f}'(x_0) = - (Nh^2)^{-1} \sum_i K'((x_i - x_0)/h)$.

Интуитивно ясно, что необходимо использовать большую ширину окна для оценивания производных, которые могут
меняться быстрее, чем $f(x_0)$. Смещение $\hat{f}^{(s)}(x_0)$ сходится так же, как и раньше, но дисперсия сходится медленнее, что приводит к оптимальной ширине окна $h^* = O(N^{-1/(2s+2p+1)})$, если $f(x_0)$ дифференцируема $p$ раз. Для ядерного оценивания первой производной нам необходимо, что $p \geq 3$.

\subsection{Многомерная ядерная оценка плотности}

Предыдущее описание рассматривало ядерное оценивание плотности для скалярного $x$. Многомерная ядерная оценка плотности для плотности $k$-мерной случайной величины $x$ имеет вид: 
\[
\hat{f}(x_0) = \frac{1}{Nh^k} \sum_{i=1}^N K\left( \frac{x_i - x_0}{h} \right),
\]
где $K(\cdot)$ --- $k$-мерное ядро. Обычно $K(\cdot)$ является ядром-произведением, произведением одномерных ядер. Также можно использовать многомерные ядра такие, как плотность многомерного нормального распределения или сферические ядра, пропорциональные $K(z'z)$. Ядро $K(\cdot)$ удовлетворяет свойствам, аналогичным свойствам для одномерного случая, см. Ли (1996, стр. 125).

Аналитические результаты и выражения аналогичны тем, которые были раньше. Только дисперсия $\hat{f}(x_0)$ убывает со скоростью $O(Nh^k)$. При $k > 1$ она убывает медленнее, чем $O(Nh)$ в одномерном случае. Тогда

\[
\sqrt{Nh^k}(\hat{f}(x_0) - f(x_0) - b(x_0)) \stackrel{d}{\rightarrow} \mathcal{N}\left[ 0, f(x_0)\int K(z)^2dz \right].
\] 

Оптимальная ширина в данном случае $h = O(N^{-1/(k+4)})$, что больше, чем $O(N^{-0.2})$ в одномерном случае. Тогда получается, что $\sqrt{Nh^k} = O(N^{2/(k+4)})$. Оценка Сильвермана и кросс-валидация могут быть расширены и на многомерный случай. Для нормального ядра-произведения  оценка ширины окна Скотта для $j$-той компоненты $x$ $h_j = N^{-1/(k+4)}s_j$, где $s_j$ --- выборочное стандартное отклонение $x_j$. 

Проблема малочисленности данных, скорее всего, возникает в случае многомерного ядра. Это называется проклятием размерности, так как меньшее число наблюдений в непосредственной близости от $x_0$ получают существенный вес, когда $x$ имеет большую размерность. Даже когда в этом нет проблемы, изображение даже двумерной ядерной оценки плотности возможно лишь на трёхмерном графике. А его, возможно, будет трудно интерпретировать.

Одно из возможных применений многомерной ядерной оценки плотности состоит в том, чтобы сделать возможным оценивание условной плотности. Так как $f(y|x) = f(x,y)/f(x)$, оценка $\hat{f}(y|x) = \hat{f}(x,y)/\hat{f}(x)$ является естественной оценкой условной плотности. Здесь $\hat{f}(y|x)$ и $\hat{f}(x)$ являются двумерной и одномерной ядерными оценками плотности.

\subsection{Ядра высшего порядка}

В предыдущем анализе предполагается, что $f(x)$ дважды дифференцируема. Это является необходимой предпосылкой для получения параметра смещения из (9.4). Если $f(x)$ более чем дважды дифференцируема, то использование ядер более высокого порядка (см. раздел 9.3.3 примеры ядер четвёртого порядка) уменьшает величину смещения, что приводит к меньшему значению $h^*$ и более высокой скорости сходимости. Стандартное утверждение состоит в том, что если $x$ является $k$-мерным, $f(x)$ дифференцируема $p$ раз и используется ядро порядка $p$, то ядерная оценка $f(x_0)$, $\hat{f}(x_0)$, имеет оптимальную скорость сходимости $N^{-p/(2p+k)}$, когда $h^* = O(N^{-1/(2p+k)})$.

\subsection{Альтернативные непараметрические оценки плотности}

Ядерная оценка плотности является стандартной непараметрической оценкой. Другие оценки плотности представлены, например, у Пагана и Улла (1999). Такого рода оценки часто основываются на подходах таких, как методы ближайших соседей, и чаще используются в непараметрической регрессии. Они кратко описаны в разделе 9.6.

\section{Непараметрическая локальная регрессия}

Мы рассматриваем регрессию скалярной зависимой переменной $y$ на скалярный регрессор $x$. Регрессионная модель имеет вид:
\begin{equation}
y_i = m(x_i) + \e_i, i = 1, \dots, N,
\end{equation}
\[
\e_i \sim iid[0, \sigma_{\e}^2].
\]
Сложность заключается в том, что функциональная форма $m(\cdot)$ не задана. По этой причине невозможно оценивать модель с помощью НМНК.

В этом разделе приводится общее описание непараметрической регрессии с использованием локальных средневзвешенных. Описание ядерной регрессии приведено в разделе 9.5, а другие широко применяемые методы локально взвешенных методов представлены в разделе 9.6.

\subsection{Локальные средневзвешенные}

Предположим, что одному значению регрессора, например, $x_0$, соответствует несколько значений $y$, например, $N_0$ наблюдений. Тогда очевидная оценка $m(x_0)$ --- выборочное среднее значение $N_0$ $y$-ков. Обозначим эту оценку как $\tilde{m}(x_0)$. Отсюда следует, что $\tilde{m}(x_0) \sim [m(x_0), N_0^{-1}\sigma_{\e}^2]$, так как это среднее значение $N_0$ наблюдений, которые в силу (9.15) независимы и одинаково распределены с математическим ожиданием $m(x_0)$ и дисперсией $\sigma_{\e}^2$.

Оценка $\tilde{m}(x_0)$ несмещена, но она необязательно состоятельна. Для состоятельности необходимо, чтобы $N_0 \rightarrow \infty$ при $N \rightarrow \infty$ так, чтобы $\V[\tilde{m}(x_0)] \rightarrow 0$. При дискретных регрессорах эта оценка может иметь сильные шумы в конечных выборках, потому что $N_0$ может быть маленьким. Более того, для непрерывных регрессоров может быть только одно наблюдение, в котором $x_i$ принимает конкретное значение $x_0$, даже когда $N \rightarrow \infty$.

Проблема малочисленности данных может быть преодолена с помощью усреднения наблюдаемых значений $y$, когда $x$ близок к $x_0$ в дополнение к $x$ в точности равному $x_0$. Прежде всего отметим, что оценку $\tilde{m}(x_0)$ можно выразить как средневзвешенное зависимой переменной $\tilde{m}(x_0) = \sum_i w_{i0}y_i$, где веса $w_{i0}$ равны $1/N_0$, если $x_i = x_0$, и равны 0, если $x_i \not= x_0$. Таким образом, вес варьируется в зависимости от оцениваемой точки $x_0$ и выборочных значений регрессоров.

Мы рассматриваем локально средневзвешенную оценку
\begin{equation}
\hat{m}(x_0) = \sum_{i=1}^N w_{i0,h}y_i,
\end{equation}
где веса
\[
w_{i0,h} = w(x_i, x_0, h)
\]
в сумме равны единице, то есть $\sum_i w_{i0,h} = 1$. Веса так заданы, чтобы они увеличивались при приближении $x_i$ к $x_0$.

Дополнительный параметр $h$ является общим обозначением для параметра ширины окна. Он задан таким образом, что меньшие значения $h$ приводят к меньшей ширине окна, а также больший вес придаётся тем наблюдениям $x_i$, которые близки к $x_0$. Конкретно для ядерной регрессии $h$ --- это ширина окна. Другие методы, которые приведены в разделе 9.6, имеют альтернативные параметры сглаживания, которые играют ту же роль, что $h$ в данном случае. С уменьшением $h$ $\hat{m}(x_0)$ становится менее смещённой, так как используются только наблюдения, близкие к $x_0$. Но в то же время разброс её значений становится больше, так как используется меньше наблюдений.

Оценка МНК для линейной регрессионной задачи --- средневзвешенное $y_i$, так как после некоторых алгебраических выражений мы получаем
\[
\hat{m}_{OLS}(x_0) = \sum_{i=1}^N \Bigl\{\frac{1}{N}+\frac{(x_0-\bar{x})(x_i-\bar{x})}{\sum_j (x_j-\bar{x})^2} \Bigr\} y_i.
\]
Однако веса МНК могут увеличиваться с увеличением расстояния между $x_0$ и $x_i$, если, например, $x_i > x_0 > \bar{x}$. Вместо этого локальная регрессия использует веса, которые уменьшаются в $|x_i - x_0|$. 

\subsection{Пример: метод $k$ ближайших соседей}

Мы рассмотрим простой пример, невзвешенное среднее значений $y$, которые соответствуют ближайшим $(k - 1)/2$ наблюдениям $x$, меньшим $x_0$, и ближайшим $(k - 1)/2$ наблюдениям $x$, большим $x_0$.

Надо проранжировать наблюдения по мере увеличения значений $x$. Тогда оценивание в точке $x_0 = x_i$ даёт
\[
\hat{m}_k(x_i) = \frac{1}{k}(y_{i-(k-1)/2} + \dots + y_{i+(k-1)/2}),
\]
где для простоты $k$ нечётно, а также игнорируются возможные мофицикации из-за совпадающих значений и значений $x_0$, близких к крайним значениям $x_1$ и $x_N$. Эта оценка может быть представлена как частный случай (9.16) с весами
\[
w_{i0,k} = \frac{1}{k} \times {\bf{1}} \left( |i - 0| < \frac{k-1}{2} \right), x_1 < x_2 < \cdots < x_0 < \cdots < x_N.
\]

Эту оценку можно называть по-разному. Мы называем её (симметризованной) оценкой $k$ ближайших соседей ($k$ -- NN, Nearest Neighbors), которая определена в разделе 9.6.1. Она также является стандартной локальной скользящей средней или скользящей средней длины $k$, центрированной относительно $x_0$. Она используется, например, для изображения на графике временного ряда $y$ в зависимости от времени $x$. Параметр $k$ играет роль ширины окна $h$ из раздела 9.4.1.: небольшие значения $k$ соответствуют небольшим значениям $h$.

В качестве примера рассмотрим данные, сгенерированные из модели
\begin{equation}
y_i = 150 + 6.5x_i - 0.15x_i^2 + 0.001x_i^3 + \e_i, i = 1, \dots, 100,
\end{equation}
\[
x_i = i,
\]
\[
\e_i \sim \mathcal{N}[0, 25^2].
\]
Математическое ожидание $y$ является кубическим по $x$, а $x$ принимает значения $1,2,\dots,100$, с точками перегиба при $x = 20$ и $x = 80$. К этому добавляются ошибки, имеющие нормальное распределение с стандартным отклонением 25.

График 9.5 отображает симметризованную $k - NN$ оценку с $k = 5$ и $25$. Обе скользящие средние говорят о кубической зависимости. Вторая является более гладкой, чем первая, но она всё-таки достаточно зубчатая, несмотря на то что для формирования среднего используется одна четверть выборки. Также на диаграмме изображена линия регрессии МНК.

\vspace{5cm}

График 9.5: Кривая регрессии $k$ ближайших соседей для двух различных вариантов $k$, а также линия регрессии МНК. Данные сгенерированы с помощью кубической полиномиальной модели.

Наклон $\hat{m}_k(x)$ более пологий в конечных точках при $k = 25$, чем при $k = 5$. Это иллюстрирует проблему краевых значений при оценивании $m(x)$ в краевых точках. Например, для наименьшего значения регрессора $x_1$ нет значений $x$, которые были бы меньше. Тогда средняя становится односторонней средней $\hat{m}_k(x_1) = (y_1 + \cdots + y_{1 + (k-1)/2})/[(k + 1)/2]$. Так как для этих данных $m_k(x)$ возрастает по $x$ на этом промежутке, это приводит к тому, что $\hat{m}_k(x_1)$ является переоценённой, и завышение увеличивается по $k$. Такие граничные проблемы можно уменьшить, используя методы, приведённые в разделе 9.6.2.

\subsection{Пример регрессии $LOWESS$}

Использование весов, альтернативных тем, которые используются для формирования симметризованной $k - NN$ оценки,  может привести к получению улучшенных оценок $m(x)$.

Примером может служить $LOWESS$ оценка, которая определена в разделе 9.6.2. Эта оценка является более гладкой оценкой $m(x)$, так как в данном случае используются ядерные веса, а не функция-индикатор. Это аналогично тому, что ядерная оценка плотности более гладкая, чем гистограмма. Она также имеет меньшее смещение (см. раздел 9.6.2), что особенно полезно при оценивании $m(x)$ в крайних точках.

График 9.6 отражает $LOWESS$ оценку при $k = 25$ для данных, cгенерированных с помощью (9.17). Эта оценка локальной регрессии довольно близка к истинной кубической функции условного математического ожидания, которая также изображена на графике. Сравнивая графики 9.6 и 9.5, на которых изображены симметризованные $k - NN$ оценки с $k = 25$, мы видим, что регрессия $LOWESS$ приводит к получению намного более гладкой оценки и более точных оценках на границах.

\subsection{Статистические выводы}

Когда ошибки имеют нормальное распределение, и анализ зависит от $x_1, \dots, x_N$, легко получить точное распределение $\hat{m}(x_0)$ для малых выборок из (9.16).

\vspace{5cm}

График 9.6: Непараметрическая линия регрессии с использованием $LOWESS$, а также кубическая линия регрессии. Используются те же данные, что и для графика 9.5.

Подстановка $y_i = m(x_i) + \e_i$ в определение $\hat{m}(x_0)$ приводит к следующему результату:
\[
\hat{m}(x_0) - \sum_{i=1}^N w_{i0,h} m(x_i) =  \sum_{i=1}^N w_{i0,h}\e_i.
\]
Следовательно, при фиксированных регрессорах и при независимых и одинаково нормально распределенных $\e_i$,  $\e_i\sim \mathcal{N}[0, \sigma_{\e}^2]$, мы получаем
\begin{equation}
\hat{m}(x_0) \sim \mathcal{N}\left[\sum_{i=1}^N w_{i0,h} m(x_i), \sigma_{\e}^2 \sum_{i=1}^N w_{i0,h}^2\right].
\end{equation}

Заметим, что в общем случае $\hat{m}(x_0)$ смещена, и распределение необязательно центрировано относительно $m(x_0)$.

При стохастических регрессорах и ошибках, не имеющих нормального распределения, мы берём зависимость от $x_1, \dots, x_N$ и применяем центральную предельную теорему для $U$-статистики, которая подходит для двойного суммирования (см., например, Паган и Улл, 1999, стр. 359). Тогда для $\e_i$, которые независимы и одинаково распределены по нормальному закону $\mathcal{N}[0, \sigma_{\e}^2]$,

\begin{equation}
c(N)\sum_{i=1}^N w_{i0,h}\e_i \stackrel{d}{\rightarrow} \mathcal{N}\left[ 0, \sigma_{\e}^2 \lim c(N)^2 \sum_{i=1}^N w_{i0,h}^2 \right],
\end{equation}
где $c(N)$ --- функция размера выборки с $O(c(N)) < N^{1/2}$, которая может меняться вместе с локальной оценкой. Например, $c(N) = \sqrt{Nh}$ для ядерной регрессии и $c(N) = N^{0.4}$ для ядерной регрессии с оптимальной шириной окна. Тогда
\begin{equation}
c(N)(\hat{m}(x_0) - m(x_0) - b(x_0)) \stackrel{d}{\rightarrow} \mathcal{N}\left[ 0, \sigma_{\e}^2 \lim c(N)^2 \sum_{i=1}^N w_{i0,h}^2 \right],
\end{equation}
где $b(x_0) = m(x_0) - \sum_i w_{i0,h} m(x_i)$. Обратите внимание, что (9.20) даёт (9.18) для асимптотического распределения $\hat{m}(x_0)$. 

Очевидно, что распределение $\hat{m}(x_0)$, простого средневзвешенного, может быть получено при альтернативных предположениях о распределении. Например, для гетероскедастичных ошибках дисперсия из (9.19) и (9.20) заменяется на $\lim c(N)^2 \sum_i \sigma_{\e,i}^2 w_{i0,h}^2$, где для получения состоятельных оценок можно заменить $\sigma_{\e,i}^2$ на квадрат остатков $(y_i - \hat{m}_(x_i))^2$. Также можно использовать метод бутстреп (см. раздел 11.6.5).

\subsection{Выбор ширины окна}

В этой главе мы следуем непараметрической терминологии, что оценка $\theta_0$ $\hat{\theta}$ имеет скорость сходимости $N^{-r}$, если $\hat{\theta} = \theta_0 + O_p(N^{-r})$. Таким образом, $N^r (\hat{\theta} - \theta_0) = O_p(1)$, и в идеальном случае $N^r (\hat{\theta} - \theta_0)$ имеет предельное нормальное распределение. Стоить обратить внимание, что $\sqrt{N}$-состоятельная оценка сходится со скоростью $N^{-1/2}$. Обычно непараметрические оценки имеют меньшую скорость сходимости, чем эта, при $r < 1/2$. Это так, потому что для устранения смещения необходима небольшая ширина окна $h$. Однако в таком случае для оценивания $\hat{m}(x_0)$ используются не все $N$ наблюдений.

В качестве примера рассмотрим $k - NN$ оценку из примера в разделе 9.4.2. Пусть $k = N^{4/5}$. Тогда, например, $k = 251$, если $N = 1 000$. В этом случае оценка является состоятельной, так как скользящая средняя использует $N^{4/5}/N = N^{-1/5}$ наблюдений из выборки, и при $N \rightarrow \infty$ она учитывает только наблюдения лежащие очень близко к $x_0$. Используя (9.18), дисперсия оценки скользящей средней равна $\sigma_{\e}^2 \sum_i w_{i0,k}^2 = \sigma_{\e}^2 \times k \times (1/k)^2 = \sigma_{\e}^2 \times 1/k = \sigma_{\e}^2 N^{-4/5}$. Поэтому в (9.19) $c(N) = \sqrt{k} = \sqrt{N^{4/5}} = N^{0.4}$, что меньше $N^{1/2}$. Другие значения $k$ также обеспечивают состоятельность при условии $k < O(N)$. 
 
В общем случае диапазон значений параметра ширины окна устраняет асимптотическое смещение, но небольшие значения ширины окна увеличивает разброс. В данной книге выбор между этими двумя показателями делается с помощью минимизации среднеквадратической ошибки, которая равна сумме дисперсии и квадрата смещения.

Стоун (1980) показал, что если $x$ является $k$-мерным и $m(x)$ дифференцируема $p$ раз, то наибольшая возможная скорость сходимости непараметрической оценки с производной $m(x)$ порядка $s$ равна $N^{-r}$, где $r = (p - s)/(2p + k)$. Эта скорость снижается по мере увеличения порядка производной, а также по мере увеличения размерности $x$. Она увеличивается, чем больше производных существует у $m(x)$, приближаясь к $N^{-1/2}$, если  $m(x)$ имеет производные порядка, близкого к бесконечности. Для получения скалярной оценки $m(x)$ в регрессии принято предполагать существование $m''(x)$. В этом случае $r = 2/5$ и наибольная скорость сходимости равна $N^{-0.4}$.

\section{Ядерная регрессия}

Ядерная регрессия является средневзвешенной оценкой с использованием ядерных весов. Такие проблемы, как смещённость и выбор ширины окна, представленные для ядерной оценки плотности, релевантны и в этом случае. Однако в случае ядерной оценки плотности существует меньше рекомендаций для выбора ширины окна, чем в случае регрессии. Кроме того, хотя мы описываем ядерную регрессию в педагогических целях, ядерные оценки локальной регрессии часто используются на практике (см. раздел 9.6).

\subsection{Оценка ядерной регрессии}

Задача ядерной регрессии состоит в том, чтобы оценить регрессионную функцию $m(x)$ в модели $y = m(x) + \e$ из (9.15).

Из раздела 9.4.1, очевидная оценка $m(x_0)$ --- среднее выборочных значений зависимой переменной $y_i$, соответствующих $x_i$, близким к $x_0$. Другой вариант --- найти среднее $y_i$ для всех наблюдений, для которых $x_i$ находится в пределах расстояния $h$ от $x_0$. Это можно выразить как
\[
\hat{m}(x_0) = \frac{\sum_{i=1}^N {\bf{1}} \left( \left. |\frac{x_i - x_0}{h}| \right. < 1 \right)y_i}{\sum_{i=1}^N {\bf{1}} \left( \left. | \frac{x_i - x_0}{h}| \right. < 1 \right)},
\]
где ${\bf{1}}(A)$ равно единице, если событие $A$ происходит, и равно нулю в противном случае. В числителе суммируются значения $y$, а знаменатель отражает количество $y$, которые суммируются. В этом выражении все наблюдения, близкие к $x_0$, имеют равный вес. Однако может быть лучше присваивать наибольший вес в $x_0$ и уменьшать вес при удалении от $x_0$. Таким образом, мы рассматриваем ядерную взвешивающую функцию $K(\cdot)$, которая была введена в разделе 9.3.2. Мы получаем ядерную оценку регрессии
\begin{equation}
\hat{m}(x_0) = \frac{\frac{1}{Nh}\sum_{i=1}^N {\bf{K}} \left( \frac{x_i - x_0}{h} \right)y_i}{\frac{1}{Nh}\sum_{i=1}^N {\bf{K}} \left( \frac{x_i - x_0}{h} \right)}.
\end{equation}
Несколько наиболее часто используемых ядерных функций --- равномерное, Гауссово, Епанечникова и квартичное --- уже были приведены в таблице 9.1.

Константа $h$ --- ширина окна, и $2h$ --- удвоенная ширина окна. Ширина окна играет ту же роль, что и $k$ в $k - NN$ примере из раздела 9.4.2.

Оценка (9.21) была предложена Надарайа (1964) и Уотсоном (1964), которые привели альтернативный вывод. Условное математическое ожидание равно $m(x) = \int yf(y|x)dy = \int y[f(y,x)/f(x)]dy$, его можно оценить с помощью $\hat{m}(y) = \int y[\hat{f}(y,x)/\hat{f}(x)]dy$, где $\hat{f}(y,x)$ и $\hat{f}(x)$ являются двумерной и одномерной ядерными оценками плотности. Можно показать, что это равняется оценке из (9.21). Литература по статистике также рассматривает ядерную регрессию в случае фиксированных регрессоров, где $f(x)$ известна и её не нужно оценивать. Однако мы рассматриваем только случай стохастических регрессоров, который возникает при наблюдаемых данных.

Ядерная оценка регрессии является частным случаем средневзвешенной (9.16) с весами
\begin{equation}
w_{i0,h} = \frac{\frac{1}{Nh}{\bf{K}} \left( \frac{x_i - x_0}{h} \right)}{\frac{1}{Nh}\sum_{i=1}^N {\bf{K}} \left( \frac{x_i - x_0}{h} \right)},
\end{equation}
сумма которых равна единице. Общие результаты раздела 9.4 релевантны, но мы приводим более подробный анализ. 

\subsection{Статистические выводы}

Мы приводим распределение ядерной оценки регрессии $\hat{m}(x)$ при заданных $K(\cdot)$ и $h$, предполагая, что данные $x$ независимы и одинаково распределены. Мы неявно предполагаем, что регрессоры непрерывны. При дискретных регрессорах $\hat{m}(x_0)$ будет по-прежнему стремиться к $m(x_0)$, функция $\hat{m}(x_0)$ в пределе, и функция $m(x_0)$ являются ступенчатыми.

\begin{center}
Состоятельность
\end{center}

Состоятельность $\hat{m}(x_0)$ для функции условного математического ожидания $m(x_0)$ требует, чтобы $h \rightarrow 0$. Таким образом, больший вес присваивается только $x_i$, очень близким к $x_0$. В то же время нам необходимо много $x_i$, близких к $x_0$, чтобы при расчёте средневзвешенной использовалось много наблюдений. Формально $\hat{m}(x_0) \stackrel{p}{\rightarrow} m(x_0)$, если $h \rightarrow 0$ и $Nh \rightarrow \infty$, так как $N \rightarrow \infty$.

\begin{center}
Смещение
\end{center}

Ядерная оценка регрессии смещена на $O(h^2)$ с параметром смещения
\begin{equation}
b(x_0) = h^2 \left( m'(x_0)\frac{f'(x_0)}{f(x_0)} + \frac{1}{2} m''(x_0) \right) \int z^2K(z)dz
\end{equation}
(см. раздел 9.8.2), предполагая, что $m(x)$ дважды дифференцируема. Что касается ядерной оценки плотности, то смещение варьируется в зависимости от используемой ядерной функции. Более того, смещение зависит от наклона и кривизны регрессионной функции $m(x_0)$ и наклона плотности $f(x_0)$ регрессоров, тогда как для оценки плотности смещение зависело только от второй производной $f(x_0)$. Смещение может быть особенно большим в краевых точках, как показано в разделе 9.4.2.

Можно уменьшить смещение, используя ядра более высокого порядка, которые определены в разделе 9.3.3, и модификации для оценки регрессии на границе, например, специальные граничные ядра. Локальная полиномиальная регрессия и такие модификации, как $LOWESS$ (см. раздел 9.6.2), привлекательны, так как показатель из (9.23), зависящий от $m'(x_0)$, сокращается. Также они дают хорошие результаты для краевых значений.

\begin{center}
Асимптотическая нормальность
\end{center}

В разделе 9.8.2 показано, что для $x_i$, которые независимы и одинаково распределены с плотностью $f(x_i)$, ядерная оценка регрессии имеет предельное распределение 
\begin{equation}
\sqrt{Nh}(\hat{m}(x_0) - m(x_0) - b(x_0)) \stackrel{d}{\rightarrow} \mathcal{N}\left[ 0, \frac{\sigma_{\e}^2}{f(x_0)} \int K(z)^2dz \right].
\end{equation}
Дисперсия из (9.24) больше для малых $f(x_0)$. Таким образом, как и ожидалось, дисперсия $\hat{m}(x_0)$ больше в областях, в которые попадает мало значений $x$.

\subsection{Выбор ширины окна}

Включение значений $y_i$, для которых $x_i \not= x_0$, в средневзвешенную приводит к появлению смещения, так как $\E[y_i|x_i] = m(x_i) \not= m(x_0)$ для $x_i \not= x_0$. Тем не менее, использование этих дополнительных точек уменьшает дисперсию оценки, так как мы усредняем по большему числу данных. Оптимальная ширина окна балансирует между увеличением смещения и снижением дисперсии, используя квадрат ошибок потерь. В отличие от ядерного оценивания плотности простые методы оценки ширины окна являются непрактичными, и поэтому более широко используется кросс-валидация.

Для простоты большинство исследований сосредоточено на выборе единой ширины окна для всех значений $x_0$. Существуют методы с меняющимися значениями ширины окна, в частности, $k - NN$ и $LOWESS$, приведены в разделе 9.6.

\begin{center}
Cредняя интегрированная квадратическая ошибка
\end{center} 

Локальное качество оценивания $\hat{m}(\cdot)$ в точке $x_0$ измеряется с помощью среднеквадратической ошибки, которая имеет вид:
\[
MSE[\hat{m}(x_0)] = \E[(\hat{m}(x_0) - m(x_0))^2],
\]
где математическое ожидание устраняет зависимость $\hat{m}(x_0)$ от $x$. Так как $MSE$ равняется сумме дисперсии и квадрата смещения, $MSE$ можно получить, используя (9.23) и (9.24).

Аналогично разделу 9.3.6 интегрированная квадратическая ошибка $(ISE)$ имеет вид:
\[
ISE(h) = \int (\hat{m}(x_0) - m(x_0))^2f(x_0) dx_0,
\]
где $f(x)$ обозначает плотность регрессоров $x$. Тогда средняя интегрированная квадратическая ошибка или интегрированная среднеквадратическая ошибка равна
\[
MISE(h) = \int MSE[\hat{m}(x_0)]f(x_0)dx_0.
\]

\begin{center}
Оптимальная ширина окна
\end{center}

Оптимальная ширина окна $h^*$ минимизирует $MISE(h)$. Мы получаем $h^* = O(N^{-0.2})$, поскольку смещение имеет порядок малости $O(h^2)$ из (9.23). Дисперсия имеет порядок малости $O((Nh)^{-1})$ из (9.24), так как можно получить дисперсию $O(1)$ после деления $\hat{m}(x_0)$ на $\sqrt{Nh}$. Для того чтобы квадрат смещения и дисперсия были одного и того же порядка, $(h^2)^2 = (Nh)^{-1}$ или $h = N^{-0.2}$. В этом случае ядерная оценка сходится к $m(x_0)$ со скоростью $(Nh^*)^{-1/2} = N^{-0.4}$, а не со стандартной скоростью $N^{-0.5}$ для параметрического анализа.

\begin{center}
Оценка ширины окна
\end{center}

Можно получить точное выражение для $h^*$, который минимизирует $MISE(h)$, используя методы расчёта, аналогичные методам для оценки плотности ядра из раздела 9.3.5. Тогда $h^*$ зависит от смещения и дисперсии из (9.23) и (9.24).

Метод подстановки предлагает вычислять $h^*$ с использованием оценок этих неизвестных. Тем не менее, оценивание $m''(x)$, например, требует непараметрических методов, которые, в свою очередь, требуют первоначального выбора ширины окна. Однако $h^*$ также зависит от таких неизвестных, как $m''(x)$. Эти усложнения следует принимать во внимании при оценивании методом подстановки. Более распространённым способом является применение кросс-валидации, которая представлена далее.

Также можно показать, что $MISE(h^*)$ минимизируется, если используется ядро Епанечникова (см. Хэрдл, 1990, стр. 186, или Хэрдл и Линтон, 1994, стр. 2321). Хотя, как и в случае ядерной регрессии, $MISE(h^*)$ не намного больше для других ядер. Ключевым вопросом является определение $h^*$, которое зависит от ядра и данных.

\begin{center}
Кросс-валидация
\end{center}

Эмпирическую оценку оптимальной $h$ можно получить с помощью  кросс-валидации с отбрасыванием отдельных наблюдений. Она выбирает такую $h^*$, которая минимизирует
\begin{equation}
CV(h) = \sum_{i=1}^N (y_i - \hat{m}_{-i}(x_i))^2\pi(x_i),
\end{equation}
где $\pi(x_i)$ --- взвешивающая функция (она будет рассмотрена далее) и 
\begin{equation}
\hat{m}_{-i}(x_i) = \sum_{j \not= i} w_{ji,h} y_j/\sum_{j \not= i} w_{ji,h}.
\end{equation}
$\hat{m}_{-i}(x_i)$ --- оценка $m(x_i)$ полученная при удалении одного наблюдения с помощью ядерной формулы (9.21) или в более общем случае с помощью взвешивающей процедуры (9.16) с модификацией, что $y_i$ не используется.

Кросс-валидация не требует такого большого числа вычислений, как кажется с первого взгляда. Можно показать, что
\begin{equation}
y_i - \hat{m}_{-i}(x_i) = \frac{y_i - \hat{m}(x_i)}{1 - [w_{ii,h}/\sum_j w_{ji,h}]}.
\end{equation}

Таким образом, для каждого значения $h$ кросс-валидация требует только расчёта средневзвешенной $\hat{m}(x_i)$, $i = 1, \dots, N$.

Веса $\pi(x_i)$ введены для того, чтобы уменьшить вес краевых точек, которые в противном случае могут получить слишком большое значение. Локальные взвешенные оценки могут быть сильно смещёнными в краевых точках, как было показано в разделе 9.4.2. Например, наблюдения $x_i$, которые находятся за пределами промежутка с 5-го по 95-й процентный квантиль, могут быть не использованы при расчёте $CV(h)$. В этом случае $\pi(x_i) = 0$ для этих наблюдений, и $\pi(x_i) = 0$ в противном случае. Термин кросс-валидация используется, так как он проверяет способность прогнозировать $i$-ое наблюдение, используя все остальные наблюдения в наборе данных. Само $i$-ое наблюдение не используется, потому что если бы оно было дополнительно использовано в прогнозе, то $CV(h)$ минимизировалось бы просто при $\hat{m}_h(x_i) = y_i$, $i = 1, \dots, N$. Величину $CV(h)$ также называют оценённой ошибкой прогнозирования.

Хэрдл и Маррон (1985) показали, что минимизация $CV(h)$ асимптотически эквивалентна минимизации модификации $ISE(h)$ и $MISE(h)$. Модификация включает взвешивающую функцию $\pi(x_0)$ под знаком интеграла, а также усреднённую квадратическую ошибку $(ASE)$ $N^{-1}\sum_i (\hat{m}(x_i) - m(x_i))^2 \pi(x_i)$, которая является дискретной выборочной аппроксимацией $ISE(h)$. Показатель $CV(h)$ сходится с медленной скоростью $O(N^{-0.1})$, поэтому $CV(h)$ может сильно варьироваться в малых выборках.

\begin{center}
Обобщённая кросс-валидация
\end{center}

Альтернативой поэлементной кросс-валидации может быть использование показателя, аналогичного $CV(h)$. Но он использует $\hat{m}(x_i)$ вместо $\hat{m}_{-i}(x_i)$, а также вводит штраф за усложнение модели, который увеличивается при уменьшении ширины окна $h$. Это приводит к 
\[
PV(h) = \sum_{i=1}^N (y_i - \hat{m}(x_i))^2\pi(x_i)p(w_{ii,h}),
\]
где $p(\cdot)$ --- функция штрафа, и $w_{ii,h}$ --- вес, который присваивается $i$-тому наблюдению в $\hat{m}(x_i) = \sum_j w_{ji,h} y_j$.

Распространённый пример --- обобщённая кросс-валидация, которая использует функцию штрафа $p(w_{ii,h}) = (1 - w_{ii,h})^2$. Другие варианты функции представлены у Хэрдла (1990, стр. 167), а также у Хэрдла и Линтона (1994, стр. 2323).

\begin{center}
Пример кросс-валидации
\end{center}

Для примера локальной скользящей средней из раздела 9.4.2 , $CV(k) =$ 54 811, 56 666, 63 456, 65 605 и 69 939 для $k = $ 3, 5, 7, 9 и 25 соответственно. В этом случае для расчёта $CV(k)$ были использованы все наблюдения с $\pi(x_i) = 1$, несмотря на возможные проблемы с краевыми точками. Там нет  реального выигрыша после $k = 5$, хотя на графике 9.5 это значение даёт слишком негладкую оценку, и на практике можно выбрать большую величину $k$, чтобы получить более гладкую кривую.

В более общем случае кросс-валидация не является идеальным способом. Зачастую, чтобы достичь желаемую степень гладкости для выбора $h$, выбор из оценённых непараметрических кривых осуществляется на глаз.

\begin{center}
Усечение
\end{center}

Знаменатель ядерной оценки из (9.21) --- это $\hat{f}(x_0)$, ядерная оценка плотности регрессора в $x_0$. В некоторых оцениваемых точках $\hat{f}(x_i)$ может принимать очень маленькие значения, что приводит к очень большой оценке $\hat{m}(x_i)$. Усечение устраняет или значительно уменьшает вес значений, для которых $\hat{f}(x_i) < b$, где, например, $b \rightarrow 0$ с подходящей скоростью при $N \rightarrow \infty$. Такие проблемы, скорее всего, могут возникнуть в хвостах распределения. Для непараметрического оценивания можно сосредоточиться на оценивании $m(x_i)$ для более близких к центру значений $x_i$, а вес значений в хвостах можно уменьшить при кросс-валидации. Тем не менее, полупараметрические методы из раздела 9.7 могут потребовать вычисление $\hat{m}(x_i)$ для всех значений $x_i$. В этом случае часто применяют усечение. В идеальном случае использование усечения не должно приводить к другим результатам асимптотически, хотя на малых выборках результаты могут отличаться.

\subsection{Доверительные интервалы}

Оценки ядерной регрессии в общем случае рекомендуется предъявлять вместе с поточечными доверительными интервалами. Простая процедура заключается в представлении поточечных доверительных интервалов для $f(x_0)$, например, во всех децилях $x_0$ с первого по девятый дециль $x$.

Если смещение $b(x_0)$ в $\hat{m}(x_0)$ игнорируется, то (9.24) приводит к следующему 95\% доверительному интервалу:
\[
m(x_0) \in \hat{m}(x_0) \pm 1.96 \sqrt{\frac{1}{Nh} \frac{\hat{\sigma}_{\e}^2}{\hat{f}(x_0)} \int K(z)^2dz},
\]
где $\hat{\sigma}_{\e}^2 = \sum_i w_{i0,h} \hat{\e}_i^2$ и $ w_{i0,h}$ из (9.22), и $\hat{f}(x_0)$ --- ядерная оценка плотности в точке $x_0$. Эта оценка предполагает гомоскедастичные ошибки, хотя, скорее всего, она устойчива к гетероскедастичности, так как наблюдениям, близким к $x_0$, придаётся наибольший вес. Альтернативно из рассуждений  после (9.20) можно получить 95\% доверительный интервал с поправкой на гетероскедастичность: $\hat{m}(x_0) \pm 1.96\hat{s}_0$, где $\hat{s}_0^2 = \sum_i w_{i0,h}^2 \hat{\e}_i^2$

Как и в случае ядерной плотности, смещение в $\hat{m}(x_0)$ нельзя игнорировать. Как уже отмечалось, оценивание смещения --- сложная задача. Здесь стандартная процедура --- недосглаживание с меньшей шириной окна $h$, где $h = o(N^{-0.2})$, а не оптимальной $h^* = O(N^{-0.2})$.

Хэрдл (1990) даёт подробное описание доверительных интервалов, в том числе полос постоянной ширины, а не точечных интервалов, и метода бутстреп, который рассмотрен в разделе 11.6.5.

\subsection{Оценивание производных}

В регрессии нас часто интересует, как меняется условное математическое ожидание $y$ при изменении $x$, предельный эффект, а не условное математическое ожидание само по себе.

Ядерные оценки можно легко использовать для формирования производной. Общим результатом является то, что $s$-тая
производная ядерной оценки регрессии, $\hat{m}^{(s)}(x_0)$, состоятельна для $m^{(s)}(x_0)$, $s$-той производной условного математического ожидания $m(x_0)$. Можно применять либо дифференциирование, либо конечно-разностные методы.

В качестве примера рассмотрим оценивание первой производной для примера сгенерированных данных из предыдущего раздела. Пусть $z_1, \dots, z_N$ обозначают упорядоченные точки, в которых оценивается ядерная функция регрессии, а $\hat{m}(z_1), \dots, \hat{m}(z_N)$ обозначают оценки в этих точках. Конечно-разностная оценка --- это $\hat{m}'(z_i) = [\hat{m}(z_i) - \hat{m}(z_{i-1})]/[z_i - z_{i-1}]$. Это представлено на графике 9.7 вместе с истинной производной, которая для процесса, порождающего данные,  из (9.17) является квадратичной $m'(z_i) = 6.5 - 0.30z_i + 0.003z_i^2$. Как и ожидалось, оценка производной имеет шумы, но она отражает основные особенности. Оценки производных должны основываться на сверхсглаженных оценках условного математического ожидания. Более подробное описание представлено у Пагана и Улла (1999, глава 4). Хэрдл (1990, стр. 160) представляет вариант кросс-валидации для оценивания производных.

В дополнение к производной в точке $m'(x_0)$ мы также можем быть заинтересованы в оценивании математического ожидания $\E[m'(x)]$. Средняя оценка производной, приведённая в разделе 9.7.4, даёт $\sqrt{N}$ состоятельную и асимптотически нормальную оценку $\E[m'(x)]$.
 
\subsection{Оценивание условного момента}

Методы ядерной регрессии для условного математического ожидания $\E[y|x] = m(x)$ можно расширить и на непараметрическое оценивание других условных моментов.

\vspace{5cm}

График 9.7: Непараметрические оценки производной, одна получена с помощью  оценённой ранее $LOWESS$ регрессионной кривой, другая --- с помощью кубической регрессии. Используются те же данные, что и для графика 9.5.

Для условных моментов таких, как $\E[y^k|x]$ мы используем взвешенную среднюю
\begin{equation}
\hat{\E}[y^k|x_0] = \sum_{i=1}^N w_{i0,h}y_i^k,
\end{equation}
где веса $w_{i0,h}$ могут быть теми же весами, что и веса, использованные для оценивания $m(x_0)$. 

Центральные условные моменты можно вычислить, выразив их как взвешенные суммы моментов. Например, так как $\V[y|x] = \E[y^2|x] - (\E[y|x])^2$, оценкой условной дисперсии может быть $\hat{\E}[y^2|x_0] - \hat{m}(x_0)^2$. Можно ожидать, что условные моменты более высокого порядка будут оценены с большим шумом, чем оценка условного математического ожидания.

\subsection{Многомерная ядерная регрессия}

Мы рассматривали ядерную регрессию с одним регрессором. Для регрессии скалярного $y$ на $k$-мерный вектор $x$, то есть $y_i = m(x_i) + \e_i = m(x_{1i}, \dots, x_{ki}) + \e_i$, ядерная оценка регрессии становится
\[
\hat{m}(x_0) = \frac{\frac{1}{Nh^k}\sum_{i=1}^N {\bf{K}} \left( \frac{x_i - x_0}{h} \right)y_i}{\frac{1}{Nh^k}\sum_{i=1}^N {\bf{K}} \left( \frac{x_i - x_0}{h} \right)},
\]
где в данном случае $K(\cdot)$ --- многомерное ядро. Часто $K(\cdot)$ обозначает произведение $k$ одномерных ядер, хотя можно использовать многомерные ядра такие, как многомерная плотность нормального распределения.

Если используется ядро-произведение, регрессоры должны быть приведены к общей шкале путём деления на стандартное отклонение. В таком случае для определения стандартной оптимальной ширины окна $h^*$ можно использовать кросс-валидацию (9.25). Хотя, когда $x$ многомерный, сложнее определить, каким $x$ следует придать меньший вес из-за близости к краевым точкам. Есть другой способ, в котором нет необходимости масштабировать регрессоры. Однако тогда для каждого регрессора надо использовать различную ширину окна.

Асимптотические результаты и выражения аналогичны тем, что были рассмотрены ранее, так как оценка снова является локальным средним $y_i$. Смещение $b(x_0)$ снова имеет порядок малости $O(h^2)$, как и раньше. Дисперсия $\hat{m}(x_0)$ убывает со скоростью $O(Nh^k)$, медленнее, чем в одномерном случае, так как существенно меньшая часть выборки используется для формирования $\hat{m}(x_0)$. Тогда
\[
\sqrt{Nh^k}(\hat{m}(x_0) - m(x_0) - b(x_0)) \stackrel{d}{\rightarrow} \mathcal{N}\left[ 0, \frac{\sigma_{\e}^2}{f(x_0)}\int K(z)^2dz \right].
\]

Оптимальная ширина окна  $h^* = O(N^{-1/(k+4)})$ больше, чем $O(N^{-0.2})$ в одномерном случае. Соответствующая оптимальная скорость сходимости $\hat{m}(x_0)$ равна $N^{-2/(k+4)}$.

Этот результат и приведённый ранее результат для скалярного случая предполагают, что $m(x)$ дважды дифференцируема --- это необходимая предпосылка для получения величины смещения из (9.23). Если вместо этого $m(x)$ дифференцируема $p$ раз, то ядерное оценивание с использованием ядра $p$-того порядка (см. раздел 9.3.3) уменьшает величину смещения. Это приводит к меньшей $h^*$ и более высокой скорости сходимости, которые достигают границу Стоуна, которая приведена в разделе 9.4.5, дополнительное описание можно посмотреть у Хэрдла (1990, стр. 93). Другие непараметрические оценки, приведённые в следующем разделе, также могут достичь границу Стоуна.

Скорость сходимости уменьшается по мере увеличения числа регрессоров, приближаясь к $N^0$, когда число регрессоров стремится к бесконечности. Это проклятие размерности сильно ограничивает использование непараметрических методов в регрессионных моделях с несколькими регрессорами. Полупараметрические модели (см. раздел 9.7) накладывают дополнительные ограничения так, чтобы непараметрические компоненты имели малую размерность.

\subsection{Тесты для параметрических моделей} 

Очевидный тест на верную спецификацию параметрической модели условного математического ожидания --- сравнить оценённое математическое ожидание с полученным математическим ожиданием из непараметрической модели.

Пусть $\hat{m}_{\theta}(x)$ --- параметрическая оценка $\E[y|x]$, $\hat{m}_h(x)$ --- непараметрическая оценка такая, как  ядерная оценка. Один подход состоит в том, чтобы сравнить  $\hat{m}_{\theta}(x)$ и $\hat{m}_h(x)$ для ряда значений $x$. Это сложно из-за того, что необходимо делать поправку на смещение в $\hat{m}_h(x)$ (см. Хэрдл и Маммен, 1993). Второй подход состоит в том, чтобы проверять тесты на условный момент вида $N^{-1}\sum_i w_i(y_i - \hat{m}_{\theta}(x))$, где различные веса, основанные на ядерной регрессии, проверяют условие $\E[y|x] = m_{\theta}(x)$. Например, Хоровиц и Хэрдл (1994) используют $w_i = \hat{m}_h(x_i) - \hat{m}_{\theta}(x_i)$. Паган и Улл (1999, стр. 141-150) и Ятчу (2003, стр. 119-124) рассматривают некоторые широко используемые методы.

\section{Некоторые альтернативные непараметрические оценки регрессии}

В разделе 9.4 описаны методы локальной регрессии, которые оценивают регрессионную функцию $m(x_0)$ с помощью локальной средневзешенной $\hat{m}(x_0) = \sum_i w_{i0,h}y_i$, где веса $w_{i0,h} = w_i(x_i, x_0, h)$ имеют различные оцениваемые точки $x_0$ и различные выборочные значения $x_i$. В разделе 9.5 представлены подробные результаты в случае ядерных весов.

Здесь мы рассматриваем часто используемые локальные оценки, для которых используются другие веса. Многие результаты раздела 9.5 остаются в силе с похожей оптимальной скоростью сходимости, также используется кросс-валидация для выбора ширины окна, хотя точные выражения для смещения и дисперсии отличаются от тех, что были даны в (9.23) и (9.24). Оценки, которые приведены в разделе 9.6.2, наиболее широко распространены.

\subsection{Оценка ближайших соседей}

Оценка $k$ ближайших соседей --- простое среднее арифметическое $y$ для $k$ наблюдений $x_i$, которые ближе всего к $x_0$. Пусть $N_k(x_0)$ --- множество, состоящее из $k$ наблюдений $x_i$, которые ближе всего к $x_0$. Тогда
\begin{equation}
\hat{m}_{k - NN}(x_0) = \frac{1}{k} \sum_{i=1}^N {\bf{1}}(x_i \in N_k(x_0))y_i.
\end{equation}
Эта оценка --- ядерная оценка с равномерными весами (см. таблицу 9.1). Однако в этом случае ширина окна изменяется. Здесь ширина окна $h_0$ в $x_0$ равняется расстоянию между $x_0$ и самым дальним из $k$ соседей, и более формально $h_0 \simeq k/(2Nf(x_0))$. Отношение $k/N$ называется размахом. Можно получить более гладкие кривые, используя ядерные веса из (9.29).

Достоинство данной оценки в том, что она даёт простое правило выбора ширины окна. Для ускорения вычислений рассчитывают симметризованную версию, которая использует $k/2$ ближних соседей слева и то же самое количество справа, как в методе локальной скользящей средней из раздела 9.4.2. Тогда можно использовать обновлённую версию формулы на отсортированных по увеличению $x_i$ наблюдениях, в которой одно наблюдение исключается и одно включается при увеличении $x_0$.  

\subsection{Локальная линейная регрессия и $LOWESS$}

Ядерная оценка регрессии --- локальная постоянная оценка, потому что предполагается, что $m(x)$ равняется константе в локальной окрестности $x_0$. Вместо этого можно задать $m(x)$ так, чтобы она была линейной в окрестности $x_0$, то есть $m(x_0) = a_0 + b_0(x - x_0)$.

Чтобы применить эту идею, заметим, что можно получить ядерную оценку регрессии $\hat{m}(x_0)$ с помощью минимизации $\sum_i K((x_i - x_0)/h)(y_i - m_0)^2$ по $m_0$. Оценка локальной линейной регрессии минимизирует 
\begin{equation}
\sum_{i=1}^N K \left( \frac{x_i - x_0}{h} \right) (y_i - a_0 - b_0(x_i - x_0))^2,
\end{equation}
по $a_0$ и $b_0$, где $K(\cdot)$ --- ядерная взвешивающая функция. Тогда $\hat{m}(x) = \hat{a}_0 + \hat{b}_0(x - x_0)$ в окрестности $x_0$. Оценка в точке $x_0$ тогда имеет вид $\hat{m}(x) = \hat{a}_0$, и $\hat{b}_0$ --- это оценка первой производной $\hat{m}'(x_0)$. В более общем случае локальная полиномиальная оценка степени $p$ минимизирует
\begin{equation}
\sum_{i=1}^N K \left( \frac{x_i - x_0}{h} \right) (y_i - a_{0,0} - a_{0,1}(x_i - x_0) - \dots - a_{0,p} \frac{(x_i - x_0)^p}{p!})^2,
\end{equation}
что даёт $\hat{m}^{(s)}(x_0) = \hat{a}_{0,s}$. 

Фан и Гайбельс (1996) указывают много свойств и достоинств этого метода. Оценивание подразумевает только взвешенную МНК регрессию в каждой оцениваемой точке $x_0$. Оценки можно выразить как средневзвешенное $y_i$, так как они являются оценками метода наименьших квадратов. Локальная линейная оценка имеет параметр смещения $b(x_0) = h^2(\frac{1}{2}m''(x_0))\int z^2 K(z)dz$, который, в отличие от смещения ядерной регрессии в (9.23), не зависит от $m'(x_0)$. Это особенно полезно при решении проблем с границами, которые описаны в разделе 9.4.2. Для оценивания производной $s$-того порядка хороший выбор $p$ -- это $p = s + 1$. Так, например, можно использовать локальную квадратичную оценку, чтобы оценить первую производную.

Распространённая локальная оценка регрессии --- локально взвешенное сглаживание диаграммы рассеяния  (locally weighted scatterplot smoothing, $LOWESS$) или оценка Кливленда (1979). Это вариант локального полиномиального оценивания из (9.31), который использует переменную ширины окна $h_{0,k}$. Она определяется расстоянием между $x_0$ и $k$-тым ближайшим соседом. В данном случае используется трикубическое ядро $K(z) = (70/81)(1 - |z|^3)^3{\bf{1}}(|z| < 1)$, при этом наблюдениям с большими остатками $y_i - \hat{m}(x_i)$ придаётся меньший вес. Для вычисления оценки приходится обсчитать данные $N$ раз. Обзор можно посмотреть у Фана и Гайбельса (1996, стр. 24). $LOWESS$ привлекательнее по сравнению с ядерной регрессией, так как этот метод использует локальную полиномиальную оценку, делает поправку на выбросы, а также использует локальную полиномиальную оценку, чтобы решить проблемы с границами. Однако это требует большого количества вычислений.

Другой популярный вариант --- супер-сглаживатель Фридмана (1984) (см. Хэрдл, 1990, стр. 181). Отправная точка --- симметризованная оценка $k - NN$. В данном случае для лучшего оценивания на границах используется локальное линейное, а не локальное постоянное оцениваниe. Вместо того, чтобы использовать фиксированный размах или фиксированные $k$, супер-сглаживатель --- сглаживатель с переменной шириной окна, которая определяется с помощью локальной кросс-валидации, для чего необходимо девять обсчётов данных. По сравнению с $LOWESS$ супер-сглаживатель не так устойчив к выбросам, но он использует переменную ширину окна. Более того, этот метод лёгок для расчётов.

\subsection{Оценка сглаженных сплайнов}

Кубическая оценка сглаженных сплайнов $\hat{m}_{\lambda}(x)$ минимизирует сумму квадратов остатков со штрафом
\begin{equation}
PRSS(\lambda) = \sum_{i=1}^N (y_i - m(x_i))^2 + \lambda \int (m''(x))^2dx,
\end{equation}
где $\lambda$ --- параметр сглаживания. В этой главе так же, как и в других, используется квадрат ошибок потерь. Первый член в отдельности приводит к недосглаженным результатам оценивания, так как тогда $\hat{m}(x_i) = y_i$. Второй член вводится, чтобы штрафовать за недосглаживание. Методы кросс-валидации из раздела 9.5.3 могут быть использованы, чтобы определить $\lambda$. При больших значениях $\lambda$ кривая будет более гладкая.

Хэрдл (1990, стр. 56-65) показывает, что $\hat{m}_{\lambda}(x)$ является кубическим полиномом между последовательными значениями $x$. Также он показывает, что можно представить оценку как локальное средневзвешенное $y$-ков и что она асимптотически эквивалентна ядерной оценке с определённым меняющимся ядром. В микроэконометрике сглаживающие сплайны используются реже, чем другие методы, которые представлены здесь. Этот подход может быть адаптирован для других штрафов за недосглаживание и для других функций потерь.

\subsection{Оценки с разложением в ряд}

Оценки с разложением в ряд аппроксимируют регрессионную функцию с помощью взвешенной суммы $K$ функций $z_1(x), \dots, z_K(x)$,
\begin{equation}
\hat{m}_k(x) = \sum_{j=1}^K \hat{\beta}_j z_j(x), 
\end{equation}
где коэффициенты $\hat{\beta}_1, \dots, \hat{\beta}_K$ получены с помощью МНК-регрессии $y$ на $z_1(x), \dots, z_K(x)$. Функции $z_1(x), \dots, z_K(x)$ образуют усечённые серии. Примеры включают в себя полиномиальную аппроксимацию $(K - 1)$-ого порядка или степенной ряд с $z_j(x) = x^{j-1}$, $j = 1, \dots, K$; ортогональные и ортонормированные варианты многочлена (см. раздел 12.3.1); усечённые ряды Фурье, где регрессор масштабируется так, чтобы $x \in [0,2\pi]$; гибкие функциональные формы Галланта для рядов Фурье (1981), которые представляют собой усечённые ряды Фурье c членами $x$ и $x^2$; сплайн регрессии, которые аппроксимируют регрессионную функцию $m(x)$ с помощью полиномиальных функций на заданном числе узлов.

Этот метод отличается от того, который был представлен в разделе 9.4. Он представляет собой подход, который заключается в глобальной аппроксимации оценки $m(x)$, а не локальной аппроксимации $m(x_0)$. Тем не менее, $\hat{m}_K(x) \stackrel{p}{\rightarrow} m(x_0)$ при $K \rightarrow \infty$ c соответствующей скоростью при $N \rightarrow \infty$. Ньюи (1997) показал, что если $x$ является $k$-мерным и $m(x)$ дифференцируема $p$ раз, средняя интегрированная квадратическая ошибка (см. раздел 9.5.3) $MISE(h) = O(K^{-2p/k} + K/N)$, где первый член отражает смещение, а второй --- дисперсию. Отсюда оптимальное $K^* = N^{k/(2p+k)}$. Таким образом, $K$ растёт с более медленным темпом, чем размер выборки. Скорость сходимости $\hat{m}_{K^*}(x)$ равна максимально возможной скорости Стоуна (1980), которая приведена в разделе 9.4.5. Логично, что оценки с разложением в ряд могут быть неустойчивы к выборосам, так как выбросы могут оказывать глобальное, а не только локальное влияние на $\hat{m}(x)$. Однако эта гипотеза не проверяется  в стандартных примерах, приводимых в учебниках.

Эндриус (1991) и Ньюи (1997) приводят хорошее общее описание, которое включает в себя многомерный случай, оценивание функционалов, отличных от условного математического ожидания, и расширение для полупараметрических моделей, где часто используется оценивание с разложением в ряд.

\section{Полупараметрическая регрессия}

Предшествующий анализ был сконцентрирован на регрессионных моделях, не имеющих структуры. В микроэконометрике, как правило, на регрессионную модель накладывается определённая структура.

Во-первых, экономическая теория накладывает некоторую структуру на функции спроса, например, ограничения на симметрию и однородность. Эту информацию можно включить в непараметрическую регрессию; см., например, Матцкин (1994).

Во-вторых, что случается довольно часто, эконометрические модели включают так много потенциальных регрессоров, что проклятие размерности делает полностью непараметрический анализ непрактичным. Вместо этого часто оценивают полупараметрические модели, которые содержат как параметрическую компоненту, так и непараметрическую компоненту; см. Пауэлл (1994), у которого приведено подробное описание термина полупараметрический.

Существует много различных полупараметрических моделей, и часто доступно бесчисленное число методов для получения состоятельных оценок этих моделей. В этом разделе мы приводим лишь несколько основных примеров. В этой книге также рассмотрены приложения, в том числе модели бинарного выбора и цензурированные регрессионные модели, представленные в главах 14 и 16.

\subsection{Примеры}

Таблица 9.2 отражает несколько основных примеры полупараметрических регрессий. Первые два примера, которые будут подробно описаны далее, являются обобщением $x'\beta$ из линейной модели с помощью добавления неспецифицированной компоненты $\lambda(z)$ или с помощью неспецифицированного преобразования $g(x'\beta)$. В то же время третий пример сочетает в себе первые два. Следующие три модели  используются в прикладной статистике чаще, чем в эконометрике. Они уменьшают размерность, предполагая аддитивность или сепарабельность регрессоров, а в остальном являются непараметрическими. Мы рассмотрим обобщённую аддитивную модель. С этими моделями связаны модели нейронных сетей, см. Куан и Уайт (1994). Последний пример, который также будет подробно описан далее, представляет собой гибкую модель условной дисперсии. Необходимо быть внимательным, чтобы обеспечить идентифицируемость полупараметрической модели. Например, посмотрите на обсуждение индентифицируемости одноиндексных моделей. Интересно также посмотреть не только на оценивание $\beta$, но и на предельные эффекты $\partial{\E}[y|x,z]/\partial{x}$.

\begin{table}[h]
\begin{center}
\begin{small}
\caption{\label{tab:pred} Полупараметрические модели: основные примеры}
\begin{tabular}[t]{llcc}
\hline
\hline
\bf{Название} & \bf{Модель} & \bf{Параметрическая} & \bf{Непараметрическая} \\
 & & \bf{компонента} & \bf{компонента} \\
\hline
Частично линейная & $\E[y|x,z] = x'\beta + \lambda(z)$ & $\beta$ & $\lambda(\cdot)$ \\
Одноиндексная & $\E[y|x] = g(x'\beta)$ & $\beta$ & $g(\cdot)$ \\
Обобщённая частично & $\E[y|x,z] = g(x'\beta + \lambda(z))$ & $\beta$ & $g(\cdot),\lambda(\cdot)$ \\
линейная & & & \\
Обобщённая аддитивная & $\E[y|x] = c + \sum_{j=1}^k g_j(x_j)$ & --- & $g_j(\cdot)$ \\
Частично аддитивная & $\E[y|x,z] = x'\beta + c + \sum_{j=1}^k g_j(x_j)$ & $\beta$ & $g_j(\cdot)$ \\
Поиск наилучшей & $\E[y|x] = \sum_{j=1}^M g_j(x_j'\beta_j)$ & $\beta_j$ & $g_j(\cdot)$ \\
проекции & & & \\
Линейная с гетероскедастичностью & $\E[y|x] = x'\beta$; $\V[y|x] = \sigma^2(x)$ & $\beta$ & $\sigma^2$ \\
\hline
\hline
\end{tabular}
\end{small}
\end{center}
\end{table}

\subsection{Эффективность полупараметрических оценок}

Мы рассмотрим потерю эффективности при оценке с помощью полупараметрических вместо параметрических методов. Далее мы представим результаты для нескольких основных полупараметрических моделей.

Наше описание аналогично описанию Робинсона (1988б), который рассматривает полупараметрическую модель с параметрической компонентой, которая обозначается как $\beta$, и  с непараметрической компонентой, которая обозначается как $G$. Компонента $G$ зависит от бесконечного числа мешающих параметров. Примеры $G$ включают форму распределения независимых, одинаково и симметрично распределённых ошибок и одноиндексную функцию $g(\cdot)$, которая приведена в (9.37) в разделе 9.7.4. Оценка $\hat{\beta} = \beta(\hat{G})$, где $\hat{G}$ --- непараметрическая оценка $G$.

В идеальном случае оценка $\hat{\beta}$ является адаптивной, то есть нет потери эффективности при оценивании $G$ с помощью непараметрических методов, т.е.
\[
\sqrt{N}(\hat{\beta} - \beta) \stackrel{d}{\rightarrow} \mathcal{N}[0,V_G],
\]
где $V_G$ --- ковариационная матрица для функции $G$ в конкретном рассматриваемом классе. В рамках метода правдоподобия $V_G$ --- нижняя граница Крамера-Рао. В контексте второго момента $V_G$ определяется из теоремы Гаусса-Маркова или из обобщений до, например, обобщённого метода моментов. Ярким примером адаптивной оценки является оценка с заданной функцией условного математического ожидания, но с неизвестной функциональной формой гетероскедастичности (см. раздел 9.7.6).

Если оценка $\hat{\beta}$ не является адаптивной, то следующая лучшая оценка в соответствии с критерием оптимальности будет оценка, достигающая полупараметрической границы эффективности $V_G^*$ так, чтобы
\[
\sqrt{N}(\hat{\beta} - \beta) \stackrel{d}{\rightarrow} \mathcal{N}[0,V_G^*],
\]
где $V_G^*$ является обобщением нижней границы Крамера-Рао или её аналогом второго момента, который обеспечивает наименьшую возможную ковариационную матрицу с учётом заданной полупараметрической модели. Для адаптивной оценки $V_G^* = V_G$, но обычно $V_G^*$ превышает $V_G$. Полупараметрические границы эффективности вводятся в разделе 9.7.8. Они могут быть получены только для некоторых полупараметрических моделей, и даже в этом случае может не существовать оценки, которая достигает границы. Пример, который достигает границу --- это оценка модели бинарного выбора Кляйна и Спэйди (1993) (см. раздел 14.7.4).

Если полупараметрическая граница эффективности недостижима или неизвестна, то следующее лучшее свойство заключается в том, выполняется ли $\sqrt{N}(\hat{\beta} - \beta) \stackrel{d}{\rightarrow} \mathcal{N}[0,V_G^{**}]$ для некоторой $V_G^{**}$, большей, чем $V_G^*$. Это позволяет делать обычные статистические выводы. В более общем случае $\sqrt{N}(\hat{\beta} - \beta) = O_p(1)$, но это выражение необязательно имеет нормальное распределение. В конце концов оценки, которые менее состоятельны, чем $\sqrt{N}$ состоятельные оценки имеют свойство $N^r(\hat{\beta} - \beta) = O_p(1)$, где $r < 0.5$. Часто невозможно достичь асимптотическую нормальность. Это часто возникает, если параметрическая и непараметрическая части рассматриваются одинаково, то есть максимизация происходит совместно по $\beta$ и $G$. Существует много примеров, особенно для дискретных и усечённых моделей выбора.

Несмотря на потенциальную неэффективность, полупараметрические оценки привлекательны тем, что
они могут сохранить состоятельность в условиях, когда полностью параметрическая оценка является несостоятельной. Пауэлл (1994, стр. 2513) приводит таблицу, которая кратко описывает существование состоятельных и $\sqrt{N}$ состоятельных асимптотических нормальных оценок для ряда полупараметрических моделей.

\subsection{Частично линейная модель}

Частично линейная модель задаёт условное математическое ожидание как обычную регрессионную функцию с добавлением неспецифицированной нелинейной компоненты:
\begin{equation}
\E[y|x,z] = x'\beta + \lambda(z),
\end{equation}
где скалярная функция $\lambda(\cdot)$ не задана.

Приведём пример оценивания функции спроса на электричество, где $z$ отражает показатель времени суток или индикаторы погоды такие, как температура. Второй пример --- модель самоотбора выборки, которая представлена в разделе 16.5. Игнорирование $\lambda(z)$ приводит к несостоятельным $\beta$ из-за смещения вызванного существованием пропущенных переменных кроме случая, когда $\Cov[x,\lambda(z)] = 0$. В прикладных исследованиях особое внимание может уделяться $\beta$, $\lambda$ или и тому, и другому. Полностью непараметрическое оценивание $\E[y|x,z]$ возможно, но оно приводит к оценкам, которые менее состоятельные, чем $\sqrt{N}$ состоятельные оценки $\beta$.

\begin{center}
Оценка разностей Робинсона
\end{center}

Робинсон (1988а) предложил следующий метод. Регрессионная модель имеет вид:
\[
y = x'\beta + \lambda(z) + u,
\]
где ошибка $u = y - \E[y|x,z]$. Из этого следует, что
\[
\E[y|z] = \E[x|z]'\beta + \lambda(z),
\]
так как $\E[u|x,z] = 0$ подразумевает, что $\E[u|z] = 0$. Вычитая одно из другого, получим
\begin{equation}
y - \E[y|z] = (x - \E[x|z])'\beta + u.
\end{equation}
Условные моменты из (9.35) неизвестны, но их можно заменить на непараметрические оценки.

Таким образом, Робинсон предложил с помощью МНК оценить регрессию
\begin{equation}
y_i - \hat{m}_{yi} = (x - \hat{m}_{xi})'\beta + v,
\end{equation}
где $\hat{m}_{yi}$ и $\hat{m}_{xi}$ --- прогнозы непараметрической регрессии $y_i$ и $x_i$ на $z_i$ соответственно. При условии независимости по $i$ МНК-оценка оценка $\beta$ из (9.36) $\sqrt{N}$ состоятельная и асимптотически нормальная с 
\[
\sqrt{N}(\hat{\beta}_{PL} - \beta) \stackrel{d}{\rightarrow} \mathcal{N} \left[ 0, \sigma^2 \left( \plim \frac{1}{N} \sum_{i=1}^N (x_i - \E[x_i|z_i])(x_i - \E[x_i|z_i])' \right)^{-1} \right],
\]
при предположении, что $u_i$ независимы и одинаково распределены с параметрами $[0,\sigma^2]$. Если $\lambda(z)$ не задана, то часто происходит потеря эффективности. Однако этой потери нет, если $\E[x|z]$ является линейным по $z$. Для того чтобы оценить $\V[\hat{\beta}_{PL}]$, можно просто заменить $(x_i - \E[x_i|z_i])$ на $(x_i - \hat{m}_{xi})$. Асимптотический результат можно обобщить и на гетероскедастичные ошибки. В этом случае используют обычные стандартные ошибки Эйкера-Уайта из регрессии МНК (9.36). Так как $\lambda(z) = \E[y|z] - \E[x|z]'\beta$, то можно получить её состоятельную оценку $\hat{\lambda}(z) = \hat{m}_{yi} - \hat{m}_{xi}'\hat{\beta}$.

Можно использовать разнообразные непараметрические оценки $\hat{m}_{yi}$ и $\hat{m}_{xi}$. Робинсон (1988a) использовал ядерные оценки, которые требуют сходимость со скоростью не меньшей, чем $N^{-1/4}$. По этой причине, если размерность $z$ велика, необходимо использовать сверхсглаживание или ядра более высокого порядка,; см. Паган и Улла (1999, стр. 205). Отметим также, что ядерные оценки тоже могут быть усечены (см. раздел 9.5.3).

\begin{center}
Другие оценки
\end{center}

Некоторые другие методы приводят к $\sqrt{N}$ состоятельным оценкам $\beta$ в частично линейной модели. Спекман (1988) также использовал ядра. Энгл и другие (1986) использовали обобщение оценки кубического сглаживания сплайнами. Эндриус (1991) представил регрессию $y$ на $x$ и аппроксимацию с помощью рядов для $\lambda(z)$, что приведено в разделе 9.6.4. Ятчу (1997) представил простую оценку разностей.

\subsection{Одноиндексные модели}

Одноиндексная модель задаёт условное математическое ожидание как неизвестную скалярную функцию линейной комбинации регрессоров с 
\begin{equation}
\E[y|x] = g(x'\beta),
\end{equation}
где скалярная функция $g(\cdot)$ не задана. Преимущества одноиндексных моделей были представлены в разделе 5.2.4. В данном случае функция $g(\cdot)$ оценивается по имеющимся данным, в то время как в предыдущих примерах она была задана, например, как $\E[y|x] = \exp(x'\beta)$.

\begin{center}
Идентификация
\end{center}

Ичимура (1993) привёл условия идентификации для одноиндексной модели. При неизвестной функции $g(\cdot)$ для одноиндексной модели $\beta$ заданы только с точностью до сдвига и масштабирования. Чтобы это увидеть, обратите внимание, что для скалярного $v$ функцию $g^*(a + bv)$ можно всегда выразить как $g(v)$. Таким образом, функция $g^*(a + bx'\beta)$ эквивалентна $g(x'\beta)$. Кроме того, $g(\cdot)$ должна быть дифференцируема. В простейшем случае все регрессоры являются непрерывными. Но если некоторые регрессоры являются дискретными, то по крайней мере один регрессор должен быть непрерывным. Также если $g(\cdot)$ монотонна, то можно получить границы для $\beta$.

\begin{center}
Оценка средней производной
\end{center}

Для непрерывных регрессоров Стокер (1986) заметил, что если условное математическое ожидание является одноиндексным, то вектор средних производных условного математического ожидания определяет $\beta$ с точностью до масштаба, так как для $m(x_i) = g(x_i'\beta)$
\begin{equation}
\delta \equiv \E\left[ \frac{\partial{m(x)}}{\partial{x}} \right] = \E[g'(x'\beta)]\beta,
\end{equation}
и $\E[g'(x_i'\beta)]$ --- скаляр. Более того, из обобщённого равенства информационных матриц, которое приведено в разделе 5.6.3, следует, что для любой функции $h(x)$ $\E[\partial{h(x)}/\partial{x}] = - \E[h(x)s(x)]$, где $s(x) = \partial{\ln f(x)}/\partial{x} = f'(x)/f(x)$ и $f(x)$ --- плотность $x$. Таким образом, 
\begin{equation}
\delta = -\E[m(x)s(x)] = - \E[\E[y|x]s(x)].
\end{equation}
Отсюда следует, что $\delta$ и соответственно $\beta$ с точностью до масштаба могут быть оценены с помощью оценки средней производной $(AD)$
\begin{equation}
\hat{\delta}_{AD} = - \frac{1}{N} \sum_{i=1}^N y_i \hat{s}(x_i),
\end{equation}
где $\hat{s}(x_i) = \hat{f}'(x_i)/\hat{f}(x_i)$ можно получить с помощью ядерного оценивания плотности $x_i$ и её первой производной. Оценка $\hat{\delta}$ является $\sqrt{N}$ состоятельной, и её асимптотическое нормальное распределение были получено Хэрдлом и Стокером (1989). Функцию $g(\cdot)$ можно оценить с помощью непераметрической регрессии $y_i$ на $x_i'\hat{\delta}$. Обратите внимание, что $\hat{\delta}_{AD}$ даёт оценку $\E[m'(x)]$ вне зависимости от того, является ли одноиндексная модель релевантной или нет.

Недостаток $\hat{\delta}_{AD}$ заключается в том, что $\hat{s}(x_i)$ может быть очень большой, если $\hat{f}(x_i)$ маленькая. Один из вариантов --- усечь те значения, в которых $\hat{f}(x_i)$ маленькая. Пауэлл, Сток и Стокер (1989) обратили внимание, что результат из (9.38) можно обобщить до взвешенных производных с $\delta \equiv \E[w(x)m'(x)]$. Особенно удобно выбирать $w(x) = f(x)$, что даёт средневзвешенную оценку производной плотности $DWAD$
\begin{equation}
\hat{\delta}_{DWAD} = - \frac{1}{N} \sum_{i=1}^N y_i \hat{f}'(x_i),
\end{equation}
которую больше нет необходимости делить на $\hat{f}(x_i)$. Это позволяет получить $\sqrt{N}$ состоятельную и асимптотически нормальную оценку $\beta$ с точностью до масштаба. Например, первая компонента $\beta$ нормируется к единице, когда $\hat{\beta}_1 = 1$ и $\hat{\beta}_j = \hat{\delta}_j/\hat{\delta}_1$ для $j > 1$.

Эти методы требуют непрерывных регрессоров для того, чтобы существовали производные. Хорвитц и Хэрдл (1996) приводят обобщение и на дискретные регрессоры.

\begin{center}
Полупараметрический метод наименьших квадратов
\end{center}

Альтернативная оценка одноиндексной модели была предложена Ичимура (1993). Начнём с предположения, что $g(\cdot)$ известно. В этом случае $WLS$-оценка $\beta$ минимизирует
\[
S_N(\beta) = \frac{1}{N} \sum_{i=1}^N w_i(x)(y_i - g(x_i'\beta))^2.
\]
Для неизвестной $g(\cdot)$ Ичимура предложил заменить $g(x_i'\beta)$ непараметрической оценкой $\hat{g}(x_i'\beta)$, что приводит к взвешенной полупараметрической оценке наименьших квадратов $(WSLS)$ --- $\hat{\beta}_{WSLS}$, которая минимизирует
\[
Q_N(\beta) = \frac{1}{N} \sum_{i=1}^N \pi(x_i)w_i(x)(y_i - \hat{g}(x_i'\beta))^2,
\]
где $\pi(x)$ является функцией усечения, которая удаляет наблюдения, если ядерная оценка регрессии скаляра $x_i'\beta$ очень маленькая, и $\hat{g}(x_i'\beta)$ ---  ядерная оценка с выбрасывание отдельного наблюдения из регрессии $y_i$ на $x_i'\beta$. Эта $\sqrt{N}$ состоятельная и асимптотически нормальная оценка $\beta$ с точностью до масштаба. В общем случае она более эффективная, чем оценка $DWAD$. Для гетероскедастичных данных наиболее эффективная оценка --- аналог ДОМНК, который использует оценку взвешивающей функции $\hat{w}_i(x) = 1/\hat{\sigma}_i^2$, где $\hat{\sigma}_i^2$ --- ядерная оценка из (9.43) раздела 9.7.6 и где $\hat{u}_i = y_i - \hat{g}(x_i'\hat{\beta})$, а $\hat{\beta}$ получена из первоначальной минимизации $Q_N(\beta)$ с $w_i(x) = 1$.

$WSLS$ оценка вычисляется с помощью итерационных методов. Начнём с первоначальной оценки $\hat{\beta}^{(1)}$, например, $DWAD$ оценки с нормированной к единице первой компонентой. Получим ядерную оценку $\hat{g}(x_i'\hat{\beta}^{(1)})$ и соответственно $Q_N(\hat{\beta}^{(1)})$. Несильно поменяем $\hat{\beta}^{(1)}$, чтобы получить градиент $g_N(\hat{\beta}^{(1)}) = \partial{Q_N(\beta)}/\partial{\beta}|_{\hat{\beta}^{(1)}}$ и $\hat{\beta}^{(2)} = \hat{\beta}^{(1)} + A_Ng_N(\hat{\beta}^{(1)})$ и так далее. Эту оценку значительно сложнее вычислить, чем $DWAD$ оценку, особенно если $Q_N(\beta)$ может быть невыпуклой и мультимодальной.
 
\subsection{Обобщённые аддитивные модели}

Обобщённые аддитивные модели задают $\E[y|x] = g_1(x_1) + \cdots + g_k(x_k)$, частный случай полностью непараметрической модели $\E[y|x] = g(x_1, \dots, x_k)$. Этот частный случай даёт оценки подфункций $\hat{g}_j(x_j)$, которые сходятся с той же скоростью, что и одномерная непараметрическая регрессия, а не с более медленной скоростью $k$-мерной непараметрической регрессии. 

Существует хорошо развитая методология для оценивания таких моделей (см. Хасти и Тибшарани, 1990). Это автоматизировано в некоторых статистических пакетах таких, как S-Plus. Графики оценённых подфункций $\hat{g}_j(x_j)$ в зависимости от $x_j$ отражают предельные эффекты $x_j$ на $\E[y|x]$. По этой причине аддитивная модель может быть полезным инструментом для анализа данных. Эта модель редко используется в микроэконометрике отчасти потому, что многие приложения такие, как цензурированные, усечённые и дискретные переменные приводят к одноиндексным и частично линейным моделям.

\subsection{Гетероскедастичная линейная модель}

Гетероскедастичная линейная модель задаёт 
\[
\E[y|x] = x'\beta,
\]
\[
\V[y|x] = \sigma^2(x),
\]
где функция дисперсии $\sigma^2(\cdot)$ не задана.

Предположение, что ошибки гетероскедастичны, является стандартным предположением для пространственных данных в современной микроэконометрике. Можно получить состоятельные, но неэффективные оценки $\beta$ с помощью МНК и используя устойчивую к гетероскедастичности состоятельную оценку Эйкера-Уайта ковариационной матрицы МНК-оценки. Крэгг (1983) и Амемия (1983) предложили оценку инструментальных переменных, которая является более эффективной, чем МНК, но всё ещё не полностью эффективной. ДОМНК позволяет получить полностью эффективную оценку второго момента. Однако этот способ не является привлекательным, так как он требует наличия заданной функциональной формы для $\sigma^2(x)$ такой, как $\sigma^2(x) = \exp(x'\gamma)$.

Робинсон (1987) предложил вариант ДОМНК с использованием непараметрической оценки $\sigma_i^2 = \sigma^2(x_i)$. Тогда
\begin{equation}
\hat{\beta}_{HLM} = \left( \sum_{i=1}^N \hat{\sigma}_i^{-2} x_ix_i' \right)^{-1} \left( \sum_{i=1}^N \hat{\sigma}_i^{-2} x_iy_i \right),
\end{equation}
где Робинсон (1987) использовал $k - NN$ оценку $\sigma_i^2$ с равными весами. Тогда
\begin{equation}
\hat{\sigma}_i^2 = \frac{1}{k} \sum_{j=1}^N {\bf{1}} (x_j \in N_k(x_i))\hat{u}_j^2,
\end{equation}
где $\hat{u}_i = y_i - x_i'\hat{\beta}_{OLS}$ --- это остатки из первого шага МНК регрессии $y_i$ на $x_i$, и $N_k(x_i)$ --- множество из $k$ наблюдений $x_j$, которые наиболее близки к $x_i$ во взвешенной Евклидовой норме. В этом случае
\[
\sqrt{N}(\hat{\beta}_{HLM}  - \beta) \stackrel{d}{\rightarrow} \mathcal{N}\left[ 0, \left( \plim \frac{1}{N} \sum_{i=1}^N \sigma^{-2}(x_i)x_ix_i' \right)^{-1} \right],
\] 
предполагая, что $u_i$ независимы и одинаково распределены с параметрами $[0, \sigma^2(x_i)]$. Эта оценка адаптивная, так как она достигает границы Гаусса-Маркова так же, как и ОМНК-оценка при известной $\sigma_i^2$. Состоятельная оценка ковариационной матрицы $(N^{-1}\sum_i \hat{\sigma}_i^{-2} x_ix_i')^{-1}$.

Можно использовать другие непараметрические оценки $\sigma^2(x_i)$, но Кэрролл (1982) и другие изначально предложили использовать ядерную оценку $\sigma_i^2$ и обнаружили, что можно доказать эффективность только при  наложении ограничений на $x_i$. Метод Робинсон можно обобщить и на модели с нелинейной функцией математического ожидания.

\subsection{Полунепараметрический метод максимального правдоподобия}

Пусть $y_i$ независимы и одинаково распределены с заданной плотностью $f(y_i|x_i,\beta)$. В общем случае неверная спецификация плотности приводит к несостоятельным оценкам параметров. Галлант и Ничка (1987) предложили аппроксимировать неизвестную истинную плотность с помощью разложения в степенной ряд в окрестности плотности $f(y|x,\beta)$. Чтобы обеспечить положительную плотность, они фактически используют разложение в степенной ряд в квадрате в окрестности $f(y|x,\beta)$. Это даёт
\begin{equation}
h_p(y|x,\beta, \alpha) = \frac{(p(y|\alpha))^2f(y|x,\beta)}{\int (p(z|\alpha))^2f(y|z,\beta)dz},
\end{equation}
где $p(y|\alpha)$ --- многочлен порядка $p$, $\alpha$ --- вектор коэффициентов многочлена. Деление на знаменатель гарантирует, что интеграл плотности или  сумма вероятностей равна единице. Оценка $\beta$ и $\alpha$ максимизирует логарифм функции правдоподобия $\sum_{i=1}^N \ln h_p(y_i|x,\beta, \alpha)$. Подход можно сразу обобщить и на многомерный $y_i$. Эта оценка --- полунепараметрическая оценка максимального правдоподобия, потому что это непараметрическая оценка, которую можно оценить таким же образом, что и оценку метода максимального правдоподобия. Галлант и Ничка (1987) показали, что при достаточно общих условиях получаются состоятельны оценки плотности, если порядок $p$ многочлена увеличивается с размером выборки $N$ с соответствующей скоростью.

Этот результат даёт обоснование для использования (9.44), чтобы получить класс гибких распределений для каких-либо конкретных данных. Этот способ особенно прост, если полиномиальный ряд $p(y|\alpha)$ является ортогональным или ортонормированный полиномиальным рядом (см. раздел 12.3.1) для базовой плотности $f(y|x,\beta)$. Тогда можно выбрать более простой нормирующий множитель в знаменателе. Порядок многочлена можно выбрать с помощью информационных критериев с показателями, которые штрафуют за сложность модели сильнее, чем $AIC$, который используется на практике. Стандартные статистические выводы метода максимального правдоподобия можно делать, если проигнорировать, что выбор порядка многочлена зависит от данных, и предполагать, что плотность $h_p(y|x,\beta,\alpha)$ верно специфицирована. Пример такого подхода для регрессии счётных данных приведён у Камерона и Йоханссона (1997).

\subsection{Полупараметрические границы эффективности}

Полупараметрические оценки эффективности расширяют такие границы эффективности, как Крамер-Рао или теорему Гаусса-Маркова, для тех случаев, когда процесс, порождающий данные, имеет непараметрическую компоненту. Лучшие полупараметрические методы достигают эту границу эффективности.

Мы используем $\beta$ для обозначения параметров, которые мы хотим оценить. Они могут включать дисперсию $\sigma^2$. Мешающие параметры обозначены буквой $\eta$. Для простоты мы рассмотрим оценку метода максимального правдоподобия с непараметрической компонентой.

Начнём с полностью параметрического случая. Оценка метода максимального правдоподобия $(\hat{\beta},\hat{\eta})$ максимизирует $\mathcal{L}(\beta, \eta) = \ln L(\beta, \eta)$. Пусть $\theta = (\beta, \eta)$, и пусть $\mathcal{I}_{\theta \theta}$ --- информационная матрица, которая определена в (5.43). Тогда $\sqrt{N}(\hat{\theta} - \theta) \stackrel{d}{\rightarrow} \mathcal{N}[0, \mathcal{I}_{\theta \theta}^{-1}]$. Для $\sqrt{N}(\hat{\beta} - \beta)$ блочное обращение $\mathcal{I}_{\theta \theta}$ приводит к 
\begin{equation}
V^* = (\mathcal{I}_{\beta \beta} - \mathcal{I}_{\beta \eta} \mathcal{I}_{\eta \eta}^{-1} \mathcal{I}_{\eta \beta})^{-1}
\end{equation}
в качестве границы эффективности для оценки $\beta$ при неизвестном $\eta$. Существует потеря эффективности
при неизвестном $\eta$ кроме случая, когда информационная матрица является блочно-диагональной, и $\mathcal{I}_{\beta \eta} = 0$, а дисперсия сводится к $\mathcal{I}_{\beta \beta}^{-1}$.

Теперь рассмотрим обобщение на непараметрический случай. Предположим, что у нас есть параметрическая подмодель, например, $\mathcal{L}_0(\beta)$, которая включает в себя только $\beta$. Рассмотрим семейство всех возможных параметрических моделей $\mathcal{L}(\beta, \eta)$ таких, что $\mathcal{L}_0(\beta)$ являются их вложенными моделями при некотором значении $\eta$. Полупараметрическая граница эффективности --- наибольшее значение $V^*$ из (9.45) для всех возможных параметрических моделей $\mathcal{L}(\beta, \eta)$. Однако её сложно получить.

Возможно упростить эту ситуацию, рассматривая
\[
\tilde{s}_{\beta} = s_{\beta} - \E[s_{\beta}|s_{\eta}],
\]
где $s_{\theta}$ означает скор-функцию $\partial{\mathcal{L}}/\partial{\theta}$ и $\tilde{s}_{\beta}$ --- скор-функция для $\beta$ после удаления $\eta$. Для $\eta$ с конечной размерностью можно показать, что $\E[N^{-1}\tilde{s}_{\beta}\tilde{s}_{\beta}'] = V^*$. В данном случае наоборот $\eta$ имеет бесконечную размерность. Предположим, что данные независимы и одинаково распределены. Пусть $s_{\theta_i}$ обозначает $i$-тую компоненту суммы, которая приводит к скор-функции $s_{\theta}$. Беган и другие (1983) определяют касательное множество как все линейные комбинации $s_{\eta_i}$. Когда оно является линейным и замкнутым наибольшее значение $V^*$ из (9.45) равно
\[
\Omega = (\plim N^{-1}\tilde{s}_{\beta}\tilde{s}_{\beta}')^{-1} = (\E[\tilde{s}_{\beta_i} \tilde{s}_{\beta_i}'])^{-1}.
\]
В этом случае матрица $\Omega$ будет являться полупараметрической границей эффективности.

В прикладных исследованиях сначала получают $s_{\eta} = \sum_i s_{\eta_i}$. Потом получают $\E[s_{\beta_i}| s_{\eta_i}]$, для чего могут потребоваться разные  предположения, например, симметрия ошибок. Они накладывают ограничения на класс рассматриваемых полупараметрических моделей. Это даёт $\tilde{s}_{\beta_i}$ и соответственно $\Omega$. Более подробное описание представлено у Ньюи (1990б), Пагана и Улла (1999), Северини и Трипати (2001).

\section{Вывод математического ожидания и дисперсии ядерных оценок}

Непараметрическое оценивание подразумевает баланс между гладкостью (дисперсией) и смещением (математическим ожиданием). Здесь мы выводим математическое ожидание и дисперсию ядерной плотности и ядерных оценок регрессии. Вывод аналогичен тому, как это делал М. Дж. Ли (1996).

\subsection{Математическое ожидание и дисперсия ядерной оценки плотности}

Так как $x_i$ независимы и одинаково распределены, каждый член суммы имеет одинаковое ожидаемое значение и
\[
\E[\hat{f}(x_0)] = \E \left[ \frac{1}{h} K \left( \frac{x-x_0}{h} \right) \right] = \int \frac{1}{h} K \left( \frac{x-x_0}{h} \right) f(x)dx.
\]
Заменив переменную на $z = (x - x_0)/h$ так, чтобы $x = x_0 + hz$ и $dx/dz = h$, мы получим
\[
\E[\hat{f}(x_0)] = \int K(z)f(x_0 + hz)dz.
\]
Разложение $f(x_0 + hz)$ в ряд Тейлора второго порядка в окрестности $f(x_0)$ даёт  
\[
\E[\hat{f}(x_0)] = \int K(z)\{ f(x_0) + f'(x_0)hz + \frac{1}{2}f''(x_0)(hz)^2\}dz 
\]
\[
= f(x_0) \int K(z)dz + hf'(x_0)\int zK(z)dz + \frac{1}{2} h^2f''(x_0)\int z^2K(z)dz.
\]
Так как интеграл $K(z)$ равен единице, то 
\[
\E[\hat{f}(x_0)] - f(x_0) = hf'(x_0) \int zK(z)dz + \frac{1}{2}h^2f''(x_0) \int z^2K(z)dz.
\]
Если также ядро удовлетворяет условию $\int zK(z)dz = 0$, что предполагалось в условии (2) в разделе 9.3.3, и вторая производная $f$ ограничена, то первый член с правой стороны исчезает. Тогда получается $\E[\hat{f}(x_0)] - f(x_0) = b(x_0)$, где граница $b(x_0)$ задана в 9.4.

Чтобы получить дисперсию $\hat{f}(x_0)$, заметим, что если $y_i$ независимы и одинаково распределены, то $\V[\bar{y}] = N^{-1}\V[y] = N^{-1}\E[y^2] - N^{-1}(\E[y])^2$. Таким образом,
\[
\V[\hat{f}(x_0)] = \frac{1}{N} \E\left[ \left( \frac{1}{h} K \left( \frac{x - x_0}{h} \right) \right)^2 \right] - \frac{1}{N} \left( \E \left[ \frac{1}{h} K \left( \frac{x - x_0}{h} \right) \right] \right)^2.
\]
Теперь с помощью замены переменных и разложения в ряд Тейлора первого порядка получим
\[
\E\left[ \left( \frac{1}{h} K \left( \frac{x - x_0}{h} \right) \right)^2 \right] = \int \frac{1}{h} K(z)^2 \{f(x_0) + f'(x_0)hz\}dz
\]
\[
= \frac{1}{h}f(x_0)\int K(z)^2dz + f'(x_0)\int zK(z)^2dz.
\]
Отсюда следует, что 
\[
\V[\hat{f}(x_0)] = \frac{1}{Nh} f(x_0) \int K(z)^2 dz + \frac{1}{N} f'(x) \int zK(z)^2dz
\]
\[
- \frac{1}{N} \left[f(x_0) + \frac{h^2}{2} f''(x_0)\left[ \int z^2 K(z)dz\right]\right]^2.
\]
Для $h \rightarrow 0$ и $N \rightarrow \infty$ это доминируется первым членом, что приводит к уравнению (9.5).

\subsection{Распределение ядерной оценки регрессии}

Мы получаем распределение для регрессоров $x_i$, которые являются независимыми и одинаково распределёнными с плотностью $f(x)$. Из раздела 9.5.1 ядерная оценка --- средневзвешенное $\hat{m}(x_0) = \sum_i w_{i0,h}y_i$, где ядерные веса $w_{i0,h}$ приведены в (9.22). Так как сумма весов равна единице, мы получаем $\hat{m}(x_0) - m(x_0) = \sum_i w_{i0,h}(y_i - m(x_0))$. Заменяя (9.15) на $y_i$ и нормируя на $\sqrt{Nh}$, так как в случае ядерной оценки плотности мы получаем
\begin{equation}
\sqrt{Nh}(\hat{m}(x_0) - m(x_0)) = \sqrt{Nh} \sum_{i=1}^N w_{i0,h}(m(x_i) - m(x_0) + \e_i).
\end{equation}

Один подход, позволяющий получить предельное распределение (9.46), состоит в том, чтобы разложить $m(x_i)$ в ряд Тейлора второго порядка в окрестности $x_0$. Такой подход используют не всегда, так как веса $w_{i0,h}$ имеют сложный вид из-за нормирования, чтобы их сумма равнялась единице (см. (9.22)).

Вместо этого мы применяем подход Ли (1996, стр. 148-151), который аналогичен подходу Биренса (1987, стр. 106-108). Обратите внимание, что знаменатель взвешивающей функции --- ядерная оценка плотности $x_0$, так как $\hat{f}(x_0) = (Nh)^{-1} \sum_i K((x_i - x_0)/h)$. Тогда (9.46) даёт
\begin{equation}
\sqrt{Nh}(\hat{m}(x_0) - m(x_0)) = \frac{1}{\sqrt{Nh}} \left( \sum_{i=1}^N K \left( \frac{x_i - x_0}{h} \right)(m(x_i) - m(x_0) + \e_i) \right) / \hat{f}(x_0)
\end{equation}
Мы применяем теорему о преобразовании (Теорема А.12) к (9.47), используя $\hat{f}(x_0) \stackrel{p}{\rightarrow} f(x_0)$ в знаменателе. Получение предельного распределения для числителя происходит в несколько этапов:
\begin{multline}
\frac{1}{\sqrt{Nh}} \sum_{i=1}^N K \left( \frac{x_i - x_0}{h} \right)(m(x_i) - m(x_0) + \e_i) = \\
 \frac{1}{\sqrt{Nh}} \sum_{i=1}^N K \left( \frac{x_i - x_0}{h} \right)(m(x_i) - m(x_0)) +  \frac{1}{\sqrt{Nh}} \sum_{i=1}^N K \left( \frac{x_i - x_0}{h} \right)\e_i
\end{multline}

Рассмотрим первую сумму из (9.48). Если можно применить закон больших чисел, то эта сумма сходится по вероятности к своему математическому ожиданию:

\begin{multline}
\E \left[ \frac{1}{\sqrt{Nh}} \sum_{i=1}^N K \left( \frac{x_i - x_0}{h} \right)(m(x_i) - m(x_0)) \right] = \\
\frac{\sqrt{N}}{\sqrt{h}} \int K \left( \frac{x - x_0}{h} \right)(m(x) - m(x_0))f(x)dx = \\
\sqrt{Nh} \int K(z)(m(x_0 + hz) - m(x_0))f(x_0 + hz)dz = \\
\sqrt{Nh} \int K(z) \left( hzm'(x_0)+ \frac{1}{2}h^2z^2m''(x_0) \right) (f(x_0) + hzf'(x_0))dz =  \\
\sqrt{Nh} \left\{ \int K(z)h^2z^2 m'(x_0)f'(x_0)dz + \int K(z) h^2z^2m''(x_0)f(x_0)dz \right\} =  \\
\sqrt{Nh}h^2 \left( m'(x_0)f'(x_0) + \frac{1}{2} m''(x_0) f(x_0) \right) \int z^2 K(z)dz  = \\
\sqrt{Nh}f(x_0)b(x_0),
\end{multline}
где $b(x_0)$ определено в (9.23). Первое равенство использует независимость и одинаковость распределения $x_i$. Второе равенство отражает замену переменных на $z = (x - x_0)/h$. Третье равенство применяет разложение в ряд Тейлора до второго порядка к $m(x_0 + hz)$ и разложение в ряд Тейлора до первого порядка к $f(x_0 + hz)$. Четвёртое равенство следует, потому что почленное перемножение даёт четыре члена, два из которых доминируют над остальными (см., например, Ли, 1996, стр. 150).

Сейчас рассмотрим вторую сумму из (9.48). Математическое ожидание каждого из членов суммы равно нулю, а дисперсия каждого из них, опуская индекс $i$, имеет вид:
\begin{equation}
\begin{split}
\V \left[ K \left( \frac{x - x_0}{h} \right)\e \right] &= \E \left[ K^2 \left( \frac{x - x_0}{h} \right)\e^2 \right] \\
 &= \int K^2 \left( \frac{x - x_0}{h} \right) \V[\e|x]f(x)dx \\
 &= h \int K^2(z)\V[\e|x_0 + hz]f(x_0 + hz)dz \\
 &= h\V[\e|x_0]f(x_0) \int K^2(z)dz,
\end{split}
\end{equation}
заменяя переменные на $z = (x - x_0)/h$ с $dx = hdz$ в члене на третьей строке и устремляя $h \rightarrow 0$, получим выражение из последней строки. Применив центральную предельную теорему получаем, что 
\begin{equation}
\frac{1}{\sqrt{Nh}} \sum_{i=1}^N K \left( \frac{x_i - x_0}{h} \right)\e_i \stackrel{d}{\rightarrow} \mathcal{N} \left[ 0, \V[\e|x_0]f(x_0) \int K^2(z)dz \right].
\end{equation}

Соединяя (9.49) и (9.51), мы получаем, что $\sqrt{Nh}(\hat{m}(x_0) - m(x_0))$ из (9.47) сходится к $1/f(x_0)$, умноженному на $\mathcal{N}[\sqrt{Nh}f(x_0)b(x_0),\V[\e|x_0]f(x_0) \int K^2(z)dz]$. Деление математического ожидания на $f(x_0)$ и дисперсии на $f(x_0)^2$ приводит к получению предельного распределения, которое определено в (9.24).

\section{Практические сообращения}

Статистические пакеты, которые содержат различные регрессии, предлагают адекватные методы для одномерного непараметрического оценивания плотности. Язык программирования Xplore используется для непараметрических и графических методов. Подробную информацию о многих методов можно найти на веб-сайте этого языка программирования.

Непараметрическое оценивание одномерной плотности можно осуществить, используя ядерную оценку плотности на основе таких ядер, как Гауссово или Епанечникова. Легко вычислить оценки ширины окна с помощью правила Сильвермана. Оно представляет хорошую отправную точку, потому что потом можно взять половину этого значения или умножить его на два, чтобы проверить наличие улучшения.

Непараметрическую одномерную регрессию также легко осуществить, но в данном случае возникает проблема выбора ширины окна. Если мы хотим получить относительно несмещённые оценки регрессионной функции в граничных точках, то оценки локальной линейной регрессии или $LOWESS$ лучше, чем ядерная регрессия. Сложнее получить оценки ширины окна с помощью метода аналогичного оценке Сильвермана, поэтому вместо него применяется кросс-валидация (см. раздел 9.5.3 ), а также визуальный анализ с помощью диаграммы рассеяния, на которой изображена оценённая линия регрессии. Степень требуемой гладкости может изменяться в зависимости от желания исследователя. Для непараметрической многомерной регрессии подобного рода визуальный анализ может оказаться невозможным.

Полупараметрическую регрессию сложнее осуществить. Её применение может повлечь за собой такие неоднозначные моменты, как усечение и недосглаживание непараметрической компоненты, так как обычно оценивание параметрической компоненты включает в себя усреднение непараметрической компоненты. Для таких целей обычно используют специализированный код, написанный на таких языках, как Gauss, Matlab, Splus или Xplore. Для непараметрического оценивания компоненты можно значительно сократить вычисления за счёт использования быстрых вычислительных алгоритмов таких, как разбиение значений случайной величины на интервалы и пошагового обновления, см., например, Фан и Гайбельс (1996), Хэрдл и Линтон (1994).

На определённом этапе все методы требуют спецификации ширины окна. Различные варианты приводят к различным оценкам на конечных выборках. Эти различия могут быть довольно большими, как было показано на многих графиках в этой главе. С другой стороны, в полностью параметрических рамках разные исследователи, которые оценивают одну и ту же модель с помощью метода максимального правдоподобия, будут получать одни и те же оценки параметров. Эта неопределённость является недостатком непараметрических методов, хотя есть надежда, что по крайней мере в полупараметрических методах побочные эффекты для параметрической компоненты модели могут быть небольшими.

\section{Библиографические заметки}

Непараметрическое оценивание хорошо описано во многих статистических текстах, в том числе в работах Фана и Гайбельса (1996). Рупперт, Ванд, и Кэрролл (2003) представляют способы применения многих полупараметрических методов. Книги по эконометрики Хэрдла (1990), М. Дж. Ли (1996), Хоровица (1998б), Пагана и Улла (1999), и Ятчу (2003) охватывают как непараметрическое, так и полупараметрическое оценивание. Паган и Улл (1999), в частности, приводят наиболее подробное описание. Ятчу (2003) больше ориентировался на эконометриста-практика. Он уделяет особое внимание частично линейным и одноиндексным моделям, а также практическим аспектам их реализации, например, построению доверительных интервалов.

\begin{itemize}
\item [$9.3$] Наиболее ранее описание ядерного оценивания плотности приведено у Росенблатта (1956) и Парзена (1962). Книга Сильвермана (1986) является классической книгой о непараметрическом оценивании плотности.
\item [$9.4$] Довольно общее описание оптимальной скорости сходимости для непараметрических оценок приведено у Стоуна (1980).
\item [$9.5$] Ядерное оценивание было предложено Надарайа (1964) и Уотсоном (1964). Очень полезное и довольно ясное описание ядерной регрессии и метода ближайших соседей было приведено Альтманом (1992). В статистической литературе есть ещё много других полезных работ. Хэрдл (1990, глава 5) привёл подробное рассмотрение вопроса выбора ширины окна и построения доверительных интервалов.
\item [$9.6$] У Стоуна приведено много подходов к непараметрической локальной регрессии (1977). Об оценках серий можно посмотреть у Эндриуса (1991) и Ньюи (1997). 
\item [$9.6$] Границы полупараметрической эффективности описаны у Ньюи (1990б), а также в более недавних работах Северини и Трипати (2001). Эконометрическое применение было описано Чемберленом (1987).
\item [$9.7$] Книги по эконометрике подробно рассматривают полупараметрическую регрессию. Об этом писали Пауэлл (1994), Робинсон (1988б) и для более начального уровня Ятчу (1998). Также в этой книге приведены дополнительные ссылки, особенно в разделах 14.7, 15.11, 16.9, 20.5 и 23.8. Практическое исследование Беллемара, Меленберга и Ван Соеста (2002) иллюстрирует некоторые полупараметрические методы. 
\end{itemize}


\section{Упражнения}

\begin{enumerate}
\item [$9 - 1$] Предположим, что мы получаем ядерную оценку плотности, используя равномерное ядро (см. таблицу 9.1), с $h = 1$ и размером выборки $N = 100$. Предположим, что данные имеют нормальное распределение $x \sim \mathcal{N}[0,1]$.
\begin{enumerate}
\item Рассчитайте смещение ядерной оценки плотности в точке $x_0 = 1$, используя (9.4).
\item Велика ли величина смещения относительно истинного значения $\phi(1)$, где $\phi(\cdot)$ --- функция плотности стандартного нормального распределения?
\item Рассчитайте дисперсию ядерной оценки плотности в точке $x_0 = 1$, используя (9.5).
\item У кого больший вклад в $MSE$ в точке $x_0 = 1$, у дисперсии или квадрата смещения?
\item Используя результаты из раздела 9.3.7, постройте 95\% доверительный интервал для плотности в точке $x_0 = 1$ с помощью ядерной оценки $\hat{f}(1)$.
\item Определите оптимальную ширину окна $h^*$ из (9.10) для этого примера.
\end{enumerate}
\item [$9 - 2$] Предположим, что мы получаем ядерную оценку плотности, используя равномерное ядро (см. таблицу 9.1), с $h = 1$ и размером выборки $N = 100$. Предположим, что данные имеют нормальное распределение $x \sim \mathcal{N}[0,1]$ и функция условного математического ожидания --- это $m(x) = x^2$.
\begin{enumerate}
\item Рассчитайте смещение ядерной оценки плотности в точке $x_0 = 1$, используя (9.23).
\item Велика ли величина смещения относительно истинного значения $m(1) = 1$?
\item Рассчитайте дисперсию ядерной оценки плотности в точке $x_0 = 1$, используя (9.24).
\item У кого больший вклад в $MSE$ в точке $x_0 = 1$, у дисперсии или квадрата смещения?
\item Используя результаты из раздела 9.5.4, постройте 95\% доверительный интервал $\E[y|x_0 = 1]$ с помощью ядерной оценки регрессии $\hat{m}(1)$.
\item Определите оптимальную ширину окна $h^*$ из (9.10) для этого примера.
\end{enumerate}
\item [$9 - 3$] Этот вопрос предполагает доступ к программе непараметрического оценивания плотности. Используйте данные о расходах на здравоохранение из раздела 4.6.4. Используйте ядерную оценку плотности с Гауссовым ядром (если оно доступно).
\begin{enumerate}
\item Получите ядерную оценку плотности для расходов на здравоохранение, выбирая подходящую ширину окна с помощью визуального анализа и метода проб и ошибок. Укажите выбранную ширину окна.
\item Получите ядерную оценку плотности для натурального логарифма расходов на здравоохранение, выбирая подходящую ширину окна с помощью визуального анализа и метода проб и ошибок. Укажите выбранную ширину окна.
\item Сравните ответ из пункта (б) с соответствующей гистограммой.
\item Если возможно, наложите плотность нормального распределения на этот же график, что и
ядерную оценку плотности из пункта (б). Имеют ли расходы на здравоохранение лог-нормальное распределение?
\end{enumerate}
\item [$9 - 4$] Этот вопрос предполагает доступ к программе, где есть непараметрическая регрессия или другой	 непараметрический сглаживатель. Используйте полную выборку данных из раздела 4.6.4, где есть натуральный логарифм расходов на здравоохранение $(y)$ и натуральный логарифм общих расходов $(x)$.
\begin{enumerate}
\item Получите ядерную оценку плотности из регрессии для расходов на здравоохранение, выбирая подходящую ширину окна с помощью визуального анализа и метода проб и ошибок. Укажите выбранную ширину окна.
\item Смотря на результат пункта (а), является ли здоровье нормальным благом?
\item Смотря на результат пункта (а), является ли здоровье благом высшей категории?
\item Сравните Ваши непараметрические оценки с прогнозами из линейной и квадратичной регрессии.
\end{enumerate}
\end{enumerate}

