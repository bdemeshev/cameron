\part{Основные методы}

В части два представлены основные методы --- метод наименьших квадратов, метод максимального правдоподобия и метод моментов, а также соответствующие методы статистических выводов для нелинейных моделей, которые наиболее важны в микроэконометрике. Материал также покрывает такие современные сюжеты, как квантильная регрессия, последовательное оценивание, эмпирическая функция правдоподобия, полупараметрическая и непараметрическая регрессия, статистические выводы основанные на бутстрепе. В целом уровень изложения материала рассчитан на то, чтобы дать практику уровень подготовки, достаточный для чтения и понимания статей в ведущих эконометрических журналах, и, где это необходимо, материала следующих глав. Мы предполагаем предварительное знакомство читателя с линейной регрессией.

Основа теории оценивания представлена в трех следующих главах. Глава 4 начинает изложение с линейной модели регрессии. Она также на вводном уровне покрывает квантильную регрессию, которая моделирует не условное среднее, а другие особенности распределения зависимой переменной. Подробно излагаются инструментальные переменные --- основной метод исследования причинно-следственных связей. В главе 5 представлены основные методы оценивания нелинейных моделей. Она начинается с М-оценок, затем излагается метод максимального правдоподобия и нелинейным метод наименьших квадратов. Глава 6 содержит подробное изложение обобщенного метода моментов, который является общим методом, применимым и к линейным, и к нелинейным моделям, состоящим как из одного уравнения, так и из нескольких. В главе также приведен частный случай использования инструментальных переменных.

Затем мы обращаемся к тестированию моделей. В главе 7 представлены классический подход к тестированию гипотез и бутстрэп. А в главе 8 представлены более современные методы выбора моделей и анализа спецификации. Из-за своей важности бутстрэп методы, требующие большого объема вычислений, занимают отдельную 11 главу в части 3. Особенностью данной книги является то, что по возможности тесты излагаются в единообразной манере в этих трех главах. Далее алгоритмы иллюстрируются примерами на протяжении всей книги. 

Глава 9 стоит отдельно от остальных, в ней излагаются непараметрические и полупараметрические методы, налагающие слабые предпосылки на структуру эконометрической модели.

В главе 10 изложены численные методы, используемые при вычислении нелинейных оценок из глав 5 и 6. Этот материал становится важным для практика, если необходимые оценки не реализованы в статистическом пакете, или если появляются существенные численные трудности.




\chapter{Линейные модели}

\section{Введение}

Внушительное число эпирических микроэконометрических исследований использует линейную регрессию и её различные расширения. Прежде чем перейти к линейным моделям, на которых фокусируется эта книга, мы предоставляем обзор некоторых важных результатов для модели регрессии с одной зависимой переменной с пространственными (cross-section) данными. Представлены несколько различных оценок модели линейной регрессии.

Метод наименьших квадратов (МНК, или ordinary least squares --- OLS) особенно популярен. Для типичных микроэконометрических пространственных данных ошибки в модели часто могут быть гетероскедастичными. Статистический анализ должен быть устойчивым к гетероскедастичности, и можно получить выигрыш в эффективности, используя взвешенный метод наименьших квадратов.

Оценка МНК минимизирует сумму квадратов остатков. Одна из альтернатив, минимизация суммы модулей остатков, порождает оценку метода наименьших модулей. Эта оценка также представлена вместе с её расширением до квантильной регрессии.

Различные ошибки спецификации модели могут приводить к несостоятельности оценки МНК. В таких случаях вынесение суждений о параметрах, интересующих экономиста, может потребовать более сложных процедур, которые описаны с достаточным объёмом и глубиной в других частях этой книги. Метод инструментальных переменных --- одна из наиболее часто используемых процедур. Текущая глава содержит вводное описание этого важного метода и дополнительно указывает сложности, связанные со слабыми инструментами.

Раздел 4.2 предоставляет определение регрессии и различные функции потерь, которые приводят к разным оценкам функции регрессии. Пример представлен в разделе 4.3. Некоторые популярные процедуры оценки, а именно метод наименьших квадратов, взвешенный метод наименьших квадратов и квантильная регрессия представлены в разделах 4.4, 4.5 и 4.6 соответственно. Ошибки спецификации модели рассмотрены в разделе 4.7. Метод	инструментальных переменных представлен в разделах 4.8 и 4.9. Разделы 4.3-4.5, 4.7 и 4.8 охватывают стандартный материал вводных курсов, тогда как разделы 4.2, 4.6 и 4.9 вводят более продвинутый материал.

\section{Регрессии и функции потерь}
В современной микроэконометрике термин \textbf{регрессия} относится к огромному множеству процедур, изучающих отношение между зависимой переменной $y$ и набором независимых переменных $x$. Поэтому будет полезным начать с мотивации и обоснования некоторых распространённых типов регрессий.

Для начала будет правильно думать о цели регрессии как об \textbf{условном прогнозе} $y$ при заданном $x$. На практике модели регрессии также используют для других целей, в особенности для изучения причинно-следственной связи. Но даже тогда функция прогноза содержит полезное описание данных и представляет интерес. См. Раздел 4.2.3, где даётся различие между линейным прогнозом и изучением причинно-следственной связи, основанном на линейном причинном среднем.

\subsection{Функции потерь}
Обозначим символом $\hat{y}$  прогноз, т.е. оценку $y$, определённую как функция от $x$. Пусть $e \equiv y - \hat{y} $ обозначает \textbf{ошибку прогноза}, и пусть
\begin{equation}
\Loss(e)=\Loss(y - \hat{y}).
\end{equation}
обозначает \textbf{потери}, связанные с ошибкой $e$. Мы предполагаем, что прогноз формирует основу некоторого решения, и ошибка прогноза приводит к уменьшению полезности для лица, принимающего решения. Функция потерь обозначается как $\Loss(e)$, а её функциональная форма определяется лицом, принимающим решения. Одним из свойств функции потерь является возрастание по $|e|$.

Рассматривая $(y, \hat{y})$ как случайный вектор, лицо, принимающее решения, минимизирует ожидаемое значение функции потерь $\E{[\Loss(e)]}$. Если прогноз зависит от $x$, $K$-мерного вектора, то \textbf{ожидаемые потери} выражаются как
\begin{equation}
\E{[\Loss((y - \hat{y})|x)]}
\end{equation}
Выбор функции потерь должен зависеть от действительных потерь, связанных с ошибкой прогноза. В некоторых ситуациях, таких, как прогнозирование погоды, могут существовать серьёзные основания предпочесть одну функцию потерь другой.

В эконометрике часто не существует ясных указаний, но общепринятым считается специфицировать функцию потерь как квадратичную. Тогда (4.1) уточняется до $\Loss(e)=e^2$, и согласно (4.2) оптимальный прогноз минимизирует $\E{[\Loss(e|x)]}=\E{[e^2|x]}$. Следовательно, в этом случае критерий наименьшей среднеквадратической ошибки предсказания используется для сравнения качества прогнозов.

\subsection{Оптимальный прогноз}
Подход теории принятия решений к выбору \textbf{оптимального прогноза} описывается в терминах \textbf{минимизации ожидаемых потерь},

\[
\min_{\hat{y}} \E{[\Loss((y - \hat{y})|x)]}.
\]

Следовательно, свойство оптимальности определено относительно функции потерь лица, принимающего решения.

\begin{table}[h]
\caption{\label{tab:loss}Функции потерь и соответствующие оптимальные прогнозы}
\begin{tabular}[t]{lll}
\hline
\hline
\bf{Тип функции потерь} & \bf{Определение} & \bf{Оптимальный прогноз}  \\
\hline
Квадрат ошибки &  $\Loss(e)=e^2$  & $\E{[y|x]}$ \\
Модуль ошибки &  $\Loss(e)=|e|$  & $\mathrm{med}[y|x]$ \\
Асимметричный модуль ошибки &  $\Loss(e)= \left\{
\begin{array}{c l}      
    (1-\alpha) |e| &\text{  если  } e < 0 \\
    \alpha |e| & \text{ если } e \geqq 0
\end{array}
\right. $
 & $q_{\alpha}[y|x]$ \\
Дискретные потери &  $\Loss(e)=  \left\{
\begin{array}{c l}      
    0 & \text{ если } e = 0\\
    1  & \text{ если } e \neq 0
\end{array}
\right. $ & $\mathrm{mod}[y|x]$ \\
\hline
\hline
\end{tabular}
\end{table}

Четыре распространённых примера функций потерь и связанные с ними функции оптимальных прогнозов приведены в таблице 4.1.  Мы предоставляем краткий обзор их всех по очереди. Подробный анализ можно найти в работе Мански (1988a). 

Наиболее известной функцией потерь является \textbf{квадратичная функция потерь}  (или среднеквадратичная функция потерь). Тогда оптимальным прогнозом является функция \textbf{условного математического ожидания} $\E{[y|x]}$. В самом общем случае на функцию $\E{[y|x]}$ не накладывается никаких ограничений, и оценкой является непараметрическая регрессия (см. Главу 9). Но чаще модель для $\E{[y|x]}$ специфицирована как $\E{[y|x]} = g(x, \beta)$, где $g(\cdot)$ --- специфицированная функция, а $\beta$ --- конечномерный вектор параметров, которые должны быть оценены. Тогда оптимален прогноз $\hat{y} = g(x, \hat{\beta})$, где $\hat{\beta}$ выбрана так, чтобы минимизировать потери в выборке 
\[
\sum_{i=1}^{N}\Loss(e_i) = \sum_{i=1}^{N}e_i^2 = \sum_{i=1}^{N}(y_i - g(x_i, \beta))^2
\].
Функция потерь задана как сумма квадратов остатков, и потому для оценки используется нелинейный МНК (см. Раздел 5.8). Если функция условного математического ожидания ограничена до линейной по $\beta$, так что $\E{[y|x]} = x' \beta $, то оптимальным прогнозом является $\hat{y}=x'\hat{\beta}$, где $\hat{\beta}$ --- оценка МНК, подробно описанная в Разделе 4.4. 

Если критерием потерь является \textbf{модуль ошибки}, то оптимальным прогнозом является \textbf{условная медиана}, обозначенная как $\mathrm{med}[y|x]$. Если функция условной медианы линейная, так, что $\mathrm{med}[y|x] = x'\beta$, то оптимальный прогноз --- $\hat{y}=x'\hat{\beta}$, где  $\hat{\beta}$ --- оценка методом наименьших модулей, минимизирующая $\sum_i |y_i - x_i ' \beta|$. Эта оценка представлена в Разделе 4.6.

Функции потерь, равные квадрату ошибки и модулю ошибки, симметричные, т.е. одинаковый штраф  налагается на ошибки одинаковые по модулю, независимо от их знака.  При использовании \textbf{асимметричного модуля ошибки}  функция потерь  налагает штраф  $(1-\alpha) |e|$ за слишком высокие прогнозы и штраф $\alpha |e|$ за слишком низкие. Параметр асимметрии $\alpha$ специфцирован. Он лежит в интервале $(0, 1)$, с симметрией при $\alpha = 0.5$ и увеличением асимметрии при приближении $\alpha$ к 0 или 1. Можно показать, что оптимальным прогнозом является \textbf{условная квантиль}, обозначаемая как $q_{\alpha}[y|x]$; условная медиана является её частным случаем при $\alpha = 0.5$. Условные квантили определены в Разделе 4.6, который представляет квантильную регрессию (Коэнкер и Бассетт, 1978).

Последняя штрафная функция, приведённая в таблице 4.1, это \textbf{дискретные потери}, которые основываются только на наличии ошибки предсказания независимо от её величины. Оптимальным прогнозом является \textbf{условная мода}, обозначенная как $\mathrm{mod}[y|x]$. Эта штрафная функция является отправной точкой для  модальной регрессии (Ли, 1989).

Метод максимального правдоподобия не так легко умещается в рамки текущего раздела, связанные с предсказанием. Однако ему можно дать интерпретацию, связанную с ожидаемыми потерями, при предсказании плотности распределения и минимизации расстояния Кульбака-Лейблера (см. Раздел 5.7).

Только что обозначенные результаты означают, что эконометрист, заинтересованный в оценке предсказывающей функции по данным ($y$, $x$) должен выбрать метод оценки, соответствующий функции потерь. Использование популярной линейной регрессии означает, по крайней мере неявно, что лицо, принимающее решения, имеет квадратичную функцию потерь и верит, что функция условного математического ожидания линейна. Однако если специфицирована одна из трёх других функций потерь, то оптимальный прогноз будет основан на одном из трёх других типов регрессий. На практике однозначной причины предпочесть определённую функцию потерь может не существовать.

Регрессии часто используют для описания данных, а не для прогнозирования как такового. В этом случае может быть полезным рассмотреть целый ряд оценок, так как альтернативные оценки могут предоставлять полезную информацию о чувствительности оцениваемых параметров. Мански (1988a, 1991) заметил, что обе функции, квадрата ошибки и модуля ошибки, являются выпуклыми. Если условное распределение $y|x$ симметрично, то обе оценки, условного математического ожидания и условной медианы, состоятельны, и можно ожидать, что они будут близки друг к другу. Более того, если избегать предположений о распределении $y|x$, то различия между альтернативными оценками могут предоставить хороший способ получения информации о распределении данных.

\subsection{Линейный прогноз}

Оптимальным прогнозом при квадратичной функции потерь является условное математическое ожидание $E{[y|x]}$. Если оно линейно по $x$, так что $\E{[y|x]} = x' \beta $, то параметр $\beta$ имеет структурную или причинно-следственную интерпретацию, и состоятельная оценка $\beta$ означает состоятельную оценку $\E{[y|x]} = x' \beta $. Это позволяет 	содержательно анализировать эффект, оказываемый изменениями в регрессорах на условное среднее.

Если, однако, условное математическое ожидание нелинейно по $x$, так что $\E{[y|x]} \neq x, \beta)$, структурная интерпретация МНК исчезает. Однако всё ещё можно интерпретировать $x' \beta $ как наилучший линейный прогноз при квадратичной функции потерь. Дифференцирование ожидаемых потерь $\E{[(y - x' \beta)^2]}$ по $\beta$ даёт условия первого порядка $-2 \E{[y - x' \beta]} =0 $, так что оптимальным линейным прогнозом является $\beta = (\E[xx'])^{-1}\E[xy]$. Выборочным аналогом данного выражения является  МНК оценка.

Обычно мы специфицируем модели со свободным членом. Изменив обозначения, мы определим $x$ как  множество регрессоров кроме свободного члена, и заменим $x' \beta$ на $\alpha + x' \gamma$. Условия первого порядка относительно $\alpha$ и $\gamma$ выражаются как $-2\E{[u]}=0$ и $-2\E{[x u]}=0$, где $u = y - (\alpha + x' \gamma)$. Отсюда следует, что $\E{[u]}=0$ и $\Cov[x,u]=0$. Решением системы является
\begin{equation}
\begin{matrix}
\gamma = (\V[x])^{-1}\Cov[x,y], \\
\alpha = \E[y] - \E[x'] \gamma;
\end{matrix}
\end{equation} 
см, например, Голдбергер (1991, стр. 52).

Из выведения (4.3) должно быть ясно, что для данных $(y, x)$ мы всегда можем написать модель линейной регрессии 
\begin{equation}
 y = \alpha + x' \gamma + u ,
\end{equation}
где параметры  $\alpha$ и $\gamma$ определены в (4.3) и ошибка $u$ удовлетворяет  $\E[u]=0$ и $\Cov[x,u]=0$.

Таким образом, модели линейной регрессии всегда можно придать неструктурную интерпретацию, или интерпретацию приведённой формы, как \textbf{наилучшему линейному прогнозу} (или линейной проекции) при квадратичной функции потерь. Однако, чтобы условное математическое ожидание было линейным, так, что $\E{[y|x]} = \alpha + x' \gamma$, требуется дополнительное предположение, что $\E{[u|x]} = 0$, вдобавок к уже имеющимся условиям $\E{[u]}=0$ и $\Cov[x,u]=0$. 

Это различие важно с практической точки зрения. Например, если $\E{[u|x]} = 0$, так, что  $\E{[y|x]} = \alpha + x' \gamma$, то предел по вероятности оценки наименьших квадратов $\hat{\gamma}$  равен $\gamma$ независимо от того, взвешенный или обыкновенный МНК используется, и от того, получена ли выборка простым случайным отбором или экзогенным стратифицированным. Если, однако, $\E{[y|x]} \neq \alpha + x' \gamma$, то эти различные оценки наименьших квадратов могут иметь разные пределы по вероятности. Этот пример обсуждается дальше в Разделе 24.3.

Структурная интерпретация МНК требует, чтобы условное ожидание случайного члена при любых заданных регрессорах равнялось нулю.


\section{Пример: отдача от образования}
Ярким примером применения линейной регрессии из эконометрики труда является задача измерения влияния образования на заработную плату или доходы.

Типичная модель отдачи от образования специфицирует
\begin{equation}
\mathrm{ln} w_i = \alpha s_i + x_{2i}' \beta + u_i, i=1, \ldots , N,
\end{equation}
где $w$ обозначает почасовую заработную плату или годовой доход, $s$ обозначает число лет законченного образования, а $x_{2}$ обозначает контрольные переменные, такие, как опыт работы, пол и семейное происхождение. Индекс $i$ обозначает $i$-того индивида в выборке. Поскольку зависимая переменная --- логарифм заработной платы, модель является лог-линейной, и коэффициент $\alpha$  измеряет пропорциональные изменения заработной платы, связанные с увеличением образования на один год.

Для оценки этой модели наиболее часто используется метод наименьших квадратов. Логарифмирование  $\ln w$ на практике делает ошибки приблизительно гомоскедастичными, но, тем не менее, лучше использовать оценки стандартных ошибок, устойчивые к гетероскедастичности, как это описано в Разделе 4.4. Оценить модель также можно с помощью квантильной регрессии (см. Раздел 4.6), если интерес представляют особенности распределения, такие, как поведение нижней квартили. 

Регрессия (4.5) может быть немедленно использована в описательных целях. Например, если $\hat{\alpha}=0.10$, то увеличение образования на один год связано с на 10\% более высоким заработком при фиксированных остальных переменных, включённых в $x_{2}$. Последнее уточнение важно добавить, поскольку в этом примере оценка  $\hat{\alpha}$ обычно уменьшается по мере того, как $x_{2}$ расширяется за счёт новых контрольных переменных, которые могут повлиять на заработок.

Интерес с точки зрения политики заключается в определении влияния \textit{экзогенного изменения} в образовании на заработную плату. Однако образование не является присвоенным индивиду случайно, напротив, это исход, который зависит от выбора, сделанного индивидом. Теория человеческого капитала рассматривает образование как инвестицию самих индивидов, и $\alpha$ интерпретируется как мера отдачи на человеческий капитал. Регрессия (4.5) в этом случае является регрессией одной эндогенной переменной, $y$, на другую эндогенную переменную, $s$, и потому не отражает причинно-следственное воздействие экзогенного изменения $s$. Функция условного математического ожидания в этом случае бессмысленна с точки зрения причинности, потому что она <<условная>> относительно фактора --- образования --- являющегося \textit{эндогенным}. Действительно, пока мы не докажем, что $s$ сама является функцией от переменных, хотя бы одна из которых варьируется независимо от $u$, неясно, что может значить интерпретация $\alpha$ как причинно-следственного параметра.

Такая обеспокоенность по поводу эндогенных регрессоров с данными, являющимися наблюдениями относительно индивидов, пронизывает микроэконометрический анализ. Стандартные предположения модели линейной регрессии, приведённые в Разделе 4.4, требуют, чтобы регрессоры были экзогенными. Последствия эндогенности регрессоров рассмотрены в Разделе 4.7. Один из методов учёта эндогенных регрессоров, инструментальные переменные, описан в Разделе 4.8. Недавний широкий обзор способов учёта эндогенности в этом примере с образованием и заработной платой приведён в работе Ангриста и Крюгера (1999). Эти методы описаны в Разделе 2.8 и представлены на протяжении всей этой книги.


\section{Метод наименьших квадратов}

Простейшим примером регрессии является МНК в линейной регрессионной модели.

После определения модели и оценки мы приводим подробное представление асимптотического распределения оценки МНК. Такое рассмотрение предполагает, что читатель уже знаком с более вводным представлением материала. Предположения, на которых базируется модель здесь, допускают стохастические регрессоры и гетероскедастичные ошибки и предназначены для данных, полученных методом экзогенной стратифицированной выборки. 

Ключевой результат по получению устойчивых к гетероскедостичности оценок стандартных ошибок оценки МНК дан в Разделе 4.4.5.

\subsection{Линейная регрессионная модель}

В стандартной пространственной модели регрессии с $N$ наблюдениями скалярной зависимой переменной и нескольких регрессоров данные специфицированы как $(y,X)$, где $y$ обозначает вектор наблюдений независимой переменной, а $X$ обозначает матрицу объясняющих переменных.

В общем виде регрессионная модель с аддитивными ошибками в векторном виде записывается как 
\begin{equation}
y = \E{[y|X]}+u,
\end{equation}
где $\E{[y|X]}$ обозначает условное математическое ожидание случайной величины $y$ при данном $X$, а $u$ обозначает вектор ненаблюдаемых случайных ошибок или возмущений. Правая сторона уравнения раскладывает $y$ на две компоненты, одна из которых является фиксированной при данных регрессорах, а другая приписывается случайной вариации или шуму. Мы рассматриваем $\E{[y|X]}$ как функцию условного математического ожидания, которая определяет среднее или, говоря более точно, ожидаемое значение $y$ при данном $X$. 

\textbf{Линейная регрессионная модель} имеет место, если $\E{[y|X]}$ специфицирована как линейная функция от $X$. Обозначения для этой модели были подробно представлены в разделе 1.6. В векторной форме $i$-ое наблюдение равняется
\begin{equation}
y_i = x'_i \beta + u_i ,
\end{equation}
где $x_i$ --- \textbf{вектор регрессоров} размером $K\times 1$, а $\beta$ --- \textbf{вектор параметров} размером $K\times 1$. Иногда проще опустить индекс $i$ и описывать модель для типичного наблюдения как $y = x' \beta + u$. В матричном виде $N$ наблюдений соединены по строкам до  
\begin{equation}
y = X' \beta + u,
\end{equation}
где $y$ - \textbf{вектор зависимых переменных} размером  $N\times 1$, $X$ - \textbf{регрессионная матрица} размером $N\times K$, а $u$ - \textbf{вектор ошибок} размером  $N\times 1$. 

Уравнения (4.7) и (4.8) --- эквивалентные выражения для линейной регрессии и будут использоваться взаимозаменяемо. Второе из них более лаконичное и обычно более удобно для представления модели.

В такой постановке об $y$ говорят, как о \textbf{зависимой переменной}, \textbf{эндогенной переменной}, изменчивость которой мы хотим объяснить с помощью изменчивости $X$ и $u$; об $u$ говорят, как об \textbf{случайной ошибке} или \textbf{возмущении}; об $X$ говорят, как о \textbf{регрессорах} или \textbf{объясняющих переменных}. Если к тому же выполняется Предположение 4 из Раздела 4.4.6, то все компоненты $x$ являются \textbf{экзогенными переменными} или \textbf{независимыми переменными}.

\subsection{Оценка МНК}

Оценка МНК определена как оценка, минимизирующая сумму квадратов ошибок
\begin{equation}
\sum_{i=1}^N u_i^2 = u'u = (y-X\beta)'(y-X\beta)
\end{equation}
Приравнивая производную по $\beta$ к нулю и выражая $\beta$, можно получить оценку МНК,
\begin{equation}
\hat{\beta}_{\text{OLS}} = (X'X)^{-1}X'y.
\end{equation}
Более общий результат представлен в Упражнении 4.5, где предполагается, что существует матрица, обратная к $(X'X)^{-1}$. Если ранг $(X'X)^{-1}$ не полный, вместо обратной матрицы можно использовать псевдообратную. В этом случае оценка МНК всё так же является наилучшим линейным прогнозом $y$ при данном $x$, если используется квадратичная функция потерь, но множество других линейных комбинаций $x$ также выдадут эту оптимальную оценку. 

\subsection{Идентификация}

Оценку МНК всегда можно вычислить при условии, что $X'X$ невырождена. Более интересным является вопрос, что $\hat{\beta}_{\text{OLS}}$ может нам сказать о данных.

Мы обращаем внимание на возможность с помощью оценки МНК идентифицировать (см. Раздел 2.5) условное математическое ожидание $\E[y|X]$. Для линейной модели параметр $\beta$ идентифицирован, если:
\begin{enumerate}
\item $\E{[y|X]}=X\beta$ и
\item $X\beta^{(1)}=X\beta^{(2)}$, если и только если $\beta^{(1)}=\beta^{(2)}$.
\end{enumerate}
Первое условие, что условное математическое ожидание правильно специфицировано, гарантирует, что $\beta$ сама по себе представляет интерес; второе условие эквивалентно тому, что матрица $X'X$ невырождена, то есть тому самому условию, которое требовалось для существования единственной оценки МНК (4.10).

\subsection{Распределение оценки МНК}
Мы обращаем внимание на асимптотические свойства оценки МНК. Установив состоятельность оценки, мы масштабируем оценку МНК, чтобы получить предельное распределение оценки. В этом случае статистические выводы можно делать, если состоятельно оценена ковариационная матрица оценки. Данный анализ широко использует асимптотичекую теорию, обзор которой дан в Приложении A.

\begin{center}
Состоятельность
\end{center}

Свойства оценки зависят от процесса, результатом которого стали имеющиеся данные --- \textbf{процесса, порождающего данные}, (data generating process, dgp). Мы предполагаем, что процесс порождающий данные задан уравнением  $Y = X'\beta + u$, так что модель (4.8) специфицирована корректно. В некоторых местах, особенно в Главах 5 и 6 и Приложении A к $\beta$ часто добавляется индекс $0$, так что процесс порождающий данные записывается как  $Y = X'\beta_0 + u$. См. обсуждение в Разделе 5.2.3.

Тогда
\[
\begin{array}{rcl}
\hat{\beta}_{\text{ МНК }}&=&(X'X)^{-1}X'y \\ 
&=& (X'X)^{-1}X'(X'\beta+u) \\
&=&(X'X)^{-1}X'X'\beta+(X'X)^{-1}X'u, \\
\end{array}
\]
и оценка МНК может быть выражена как
\begin{equation}
\hat{\beta}_{\text{OLS}} = \beta + (X'X)^{-1}X'u.
\end{equation}
Чтобы доказать состоятельность, мы перепишем (4.11) как
\begin{equation}
\hat{\beta}_{\text{OLS}} = \beta + (N^{-1}X'X)^{-1}N^{-1}X'u.
\end{equation}
Причина перенормировки в правой части состоит в том, что $N^{-1}X'X = N^{-1}\sum_i x_i x_i'$ является средним значением, которое сходится по вероятности к конечной ненулевой матрице, если $x_i$ удовлетворяет предпосылкам, которые позволяют применить закон больших чисел к $x_i x_i'$ (см. подробности в Разделе 4.4.8). Тогда

\begin{center}
$\plim \hat{\beta}_{\text{OLS}} = \beta + (\plim N^{-1} X'X)^{-1}(\plim N^{-1}X'u)$, 
\end{center} 

согласно теореме Слуцкого (Теорема A.3). Оценка МНК \textbf{состоятельна} для $\beta$ (т.е. $\plim \hat{\beta}_{\text{OLS}}$), если 
\begin{equation}
\plim N^{-1}X'u = 0.
\end{equation}

Если к среднему $N^{-1}X'u = N^{-1} \sum_i x_i u_i$ можно применить закон больших чисел, то необходимым условием для (4.13) является $\E{x_i u_i}=0$.

\begin{center}
 Предельное распределение
 \end{center} 

При условии состоятельности предельное распределение $\hat{\beta}_{\text{OLS}}$ вырождено со всей вероятностью, сосредоточенной в точке $\beta$. Чтобы получить предельное распределение, $\hat{\beta}_{\text{OLS}}$ нужно умножить на $\sqrt{N}$, поскольку это масштабирование приводит к случайной величине, которая при стандартных предположениях для пространственной регрессии асимтотически имеет ненулевую, но конечную дисперсию. Тогда вместо (4.11) получаем
\begin{equation}
\sqrt{N} (\hat{\beta}_{\text{OLS}} - \beta) = (N^{-1}X'X)^{-1}N^{-1/2}X'u.
\end{equation}
Доказательство состоятельности основывалось на предположении, что $\plim N^{-1}X'X$ существует, конечный и ненулевой. Мы предполагаем, что можно применить центральную предельную теорему к $N^{-1/2}X'u$, тогда в пределе получается многомерное нормальное  распределение с конечной невырожденной ковариационной матрицей. Применение правила произведения для  нормальных распределений  (Теорема A.17) означает, что произведение в правой части (4.14) имеет в пределе нормальное распределение. Подробности предъявлены в разделе 4.4.8. 

Это приводит к следующему утверждению, которое допускает стохастические регрессоры и не ограничивает ошибки модели до гомоскедастичных и некоррелированных.

\textbf{Утверждение 4.1 (Распределение оценки МНК)}. Сделаем следующие предположения:
\begin{enumerate}[(i)]
 \item Процесс порождающий данные описывается моделью (4.8), т.е. $y = X \beta +u$.
 \item Данные независимы по $i$ с $\E{[u|X]}=0$ и $\E{[uu'|X]}=\Omega = Diag[\sigma_i^2]$.
 \item Матрица $X$ имеет полный ранг, то есть $X\beta^{(1)}=X\beta^{(2)}$ если, и только если $\beta^{(1)} = \beta^{(2)}$.
 \item Матрица размером $K \times K $
 \begin{equation}
 	M_{XX}= \plim N^{-1}X'X = \plim \frac{1}{N}\sum_{i=1}^N x_ix_i' = \lim \frac{1}{N} \sum_{i=1}^N \E{[x_i x_i']}
 \end{equation}
 существует, конечная и невырожденная.
 \item Вектор размером $K \times 1 $, $N^{-1/2}X'u=N^{-1/2}\sum_{i=1}^N x_ix_i' \xrightarrow{d} \mathcal{N}[0,M_{x \Omega x}] $, где
 \begin{equation}
 M_{x \Omega x} = \plim N^{-1} X'uu'X = \plim \frac{1}{N} \sum_{i=1}^N u_i^2 x_i x_i' = \lim \frac{1}{N} \sum_{i=1}^N \E [u_i^2 x_i x_i'].
 \end{equation}
\end{enumerate} 

Тогда оценка МНК $\hat{\beta}_{\text{OLS}}$, определённая в (4.10), состоятельна для $\beta$, и 
\begin{equation}
\sqrt{N}(\hat{\beta}_{\text{OLS}}-\beta) \xrightarrow{d} \mathcal{N} [0, M_{xx}^{-1}M_{x \Omega x} M_{xx}^{-1}].
\end{equation}


Предположение (i) используется, чтобы получить (4.11). Предположение (ii) обеспечивает $\E [y|X] = X\beta$  и допускает гетероскедастичные ошибки с дисперсией $\sigma_i^2$, более общие, чем гомоскедастичные некоррелированные ошибки, которые накладывают ограничение  $\Omega = \sigma^2 I$. Предположение (iii) исключает совершенную мультиколлинеарность регрессоров. Предположение (iv) приводит к масштабированию $X'X$ на $N^{-1}$  в (4.12) и (4.14). Заметим, что, согласно закону больших чисел, $\plim = \lim \E$ (см. Приложение, Раздел A.3).

Ключевым условием для состоятельности является (4.13). Вместо того, чтобы предположить это напрямую, мы использовали более сильное предположение (v), которое нужно, чтобы получить результат (4.17). При условии, что $N^{-1}X'u$ имеет предельное распределение с нулевым математическим ожиданием и конечной дисперсией, умножение на $N^{-1/2}$ порождает случайную величину, которая сходится по вероятности к нулю, и (4.13) выполняется, чего и требовалось. Предположение (v) требуется вместе с предположением (iv), чтобы получить предельное нормальное распределение (4.17), которое по теореме A.17 незамедлительно следует из (4.14). Более простые предположения на $u_i$ и $x_i$, нужные, чтобы обеспечить (iv) и (v), даны в Разделе 4.4.6, с формальным доказательством в разделе 4.4.8.

\begin{center}
 Асимптотическое распределение
 \end{center} 
Утверждение (4.1) даёт \textbf{предельное распределение} $\sqrt{N}(\hat{\beta}_{\text{OLS}}-\beta)$, масштабированной $\hat{\beta}_{\text{OLS}}$. Многие практики предпочитают видеть асимптотические результаты, записанные напрямую в терминах распределения $\hat{\beta}_{\text{OLS}}$, в случае чего распределение называется \textit{асимптотическим распределением}. Асимптотическое распределение следует понимать как применимое \textbf{в больших выборках}, то есть выборках, настолько больших, чтобы предельное распределение было достаточно хорошим приближением, но не настолько больших, что $\hat{\beta}_{\text{OLS}} \xrightarrow{d}\beta$, так как в последнем случае асимптотическое распределение было бы вырожденным. Изложение похоже на изложение в Приложении A.4.6.

Асимптотическое распределение можно получить из (4.17) делением на $\sqrt{N}$ и прибавлением $\beta$. Таким образом \textbf{асимптотическое распределение} 
\begin{equation}
\hat{\beta}_{\text{OLS}} \stackrel{a}{\sim} \mathcal{N} [\beta,N^{-1} M_{xx}^{-1}M_{x \Omega x} M_{xx}^{-1}], 
\end{equation}
где $\stackrel{a}{\sim}$ означает \textit{<<асимптотически распределено как>>}. Ковариационная матрица в (4.18) называется \textbf{асимптотической ковариационной матрицей} для $\hat{\beta}_{\text{OLS}}$ и обозначается как $\mathrm{V}[\hat{\beta}_{\text{OLS}}]$. Ещё более простой вариант обозначения опускает пределы и математические ожидания в определениях $M_{xx}$ и $M_{x \Omega x}$, и асимптотическое распределение обозначается как
\begin{equation}
\hat{\beta}_{\text{OLS}} \stackrel{a}{\sim} \mathcal{N} [\beta, (X'X)^{-1} X\Omega X (X'X)^{-1}],
\end{equation}
и $\mathrm{V}[\hat{\beta}_{\text{OLS}}]$ определяется как ковариационная матрица в (4.19).

Мы используем и (4.18), и (4.19) для обозначения асимптотического распределения в дальнейших главах. Их использование обусловлено удобством представления. Формальные асимптотические результаты, использующиеся для статиcтических заключений, основаны на предельном, а не на асимптотическом распределении.

При практическом использовании матрицы $M_{xx}$ и $M_{x \Omega x}$ в (4.17) и (4.18) заменяются на состоятельные оценки $\hat{M}_{xx}$ и $\hat{M}_{x \Omega x}$. Тогда \textbf{оценкой асимптотической ковариационной матрицы}  $\hat{\beta}_{\text{OLS}}$ является
\begin{equation}
\mathrm{\hat{V}}[\hat{\beta}_{\text{OLS}}] = N^{-1} \hat{M}_{xx}^{-1} \hat{M}_{x \Omega x} \hat{M}_{xx}^{-1}.
\end{equation}
Эта оценка называется \textbf{сэндвич-оценкой}, так как матрица $ \hat{M}_{x \Omega x}$ вложена между $\hat{M}_{xx}^{-1}$ и ещё одной $\hat{M}_{xx}^{-1}$.


\subsection{Стандартные ошибки для МНК, устойчивые к гетероскедастичности}

Очевидным выбором для $\hat{M}_{xx}$ в (4.20) является $N^{-1}X'X$. Оценка $ \hat{M}_{x \Omega x}$, определённая в (4.16), зависит от допущений о распределении случайного члена. 

В микроэкономических приложениях ошибки модели часто задаются условно гетероскедастичными, где $\mathrm{V}[u_i|x_i] = \E[u_i^2|x_i]= \sigma_i^2$ различается для различных $i$. Уайт (1984a) предложил использовать  $ \hat{M}_{x \Omega x} = N^{-1}\sum_i \hat{u}_i^2 x_i x_i'$. Эта оценка требует дополнительных допущений, данных в Разделе 4.4.8.

Объединяя эти оценки $\hat{M}_{xx}$ и $\hat{M}_{x \Omega x}$ и упрощая выражение, можно получить оценку асимптотической ковариационной матрицы оценки МНК:
\begin{equation}
\mathrm{\hat{V}}[\hat{\beta}_{\text{OLS}}] = (X'X)^{-1} X\Omega X (X'X)^{-1} = (\sum_{i=1}^N x_i x_i')^{-1} \sum_i \hat{u}_i^2 x_i x_i (\sum_{i=1}^N x_i x_i')^{-1},
\end{equation}
где $\hat{\Omega} = \mathrm{Diag}[\hat{u}_i^2]$, а $\hat{u}_i = y_i - x_i'\hat{\beta}$ --- остаток МНК. Эта оценка, известная благодаря Уайту (1980a), называется \textbf{устойчивой к гетероскедастичности} оценкой асимптотической ковариационной матрицы оценки МНК, и приводит к оценкам стандартных ошибок, которые называются \textbf{устойчивыми к гетероскедастичности стандартными ошибками}, или \textbf{робастными стандартными ошибками}. Они дают состоятельную оценку для $\mathrm{\hat{V}}[\hat{\beta}_{\text{OLS}}]$, несмотря на то, что $\hat{u}_i$ не состоятельны по отношению к $\sigma_i^2$.

Во вводных курсах на ошибки накладывается допущение о \textbf{гомоскедастичности}. Тогда $\Omega = \sigma^2 \mathrm{I}$, так что $X'\Omega X = \sigma^2 X'X$, и потому $M_{x \Omega x} = \sigma^2 M_{xx}$. Ковариационная матрица предельного распределения в этом случае упрощается до $\sigma^2 M_{xx}^{-1}$, и многие компьютерные пакеты на её месте используют оценку, которая называется \textbf{оценкой по умолчанию} для ковариации оценок МНК:
\begin{equation}
 \mathrm{\tilde{V}}[\hat{\beta}_{\text{OLS}}] = s^2 (X'X)^{-1},
 \end{equation} 
где $s^2 = (N-K)^{-1} \sum_i \hat{u}_i^2$.

Выводы, сделанные на основе (4.22), а не на (4.21) недействительны, если только ошибки не являются гомоскедастичными и некоррелированными. В общем случае ошибочное использование (4.22), когда ошибки гетероскедастичные, как это часто бывает в пространственных данных, может приводить как к недооценке, так и к переоценке стандартных ошибок.

На практике $\hat{M}_{x \Omega x}$ вычисляется с делением на $(N-K)$, а не на $N$, для соответствия со сходным делением при расчёте $s^2$ в гомоскедастичном случае. Тогда $\mathrm{\hat{V}}[\hat{\beta}_{\text{OLS}}]$ в (4.21) умножается на $N/(N-K)$. В случае гетероскедастичных ошибок теоретического обоснования для этой коррекции степеней свободы не существует, но некоторые работы, использовавшие симуляцию, поддерживают её (см. МакКиннон и Уайт, 1985, и Лонг и Эрвин, 2000).

В микроэконометрическом анализе устойчивые стандартные ошибки используются всегда, когда это возможно. В данном случае ошибки устойчивы к гетеорскедастичности. Защита от других ошибок спецификации также может быть обеспечена. Например, если данные кластеризованы, стандартные ошибки также должны быть устойчивы к кластерам; см. Разделы 21.2.3 и 24.5.

\subsection{Предположения модели пространственной регрессии}

Утверждение 4.1 является весьма общей теоремой, которая основывается на допущениях о $N^{-1}X'X$ и $N^{-1/2}X'u$. На практике выполнение этих допущений проверяют, применяя закон больших чисел и центральные предельные теоремы к средним значениям $x_i'x_i$ и $x_i u_i$. Это, в свою очередь, требует предположений о том, как наблюдения $x_i$ и ошибки $u_i$ были порождены, а, следовательно, как был порождён $y_i$. Об этих предположениях, взятых вместе, говорят, как о предположениях о \textbf{процессе, порождающем данные}. Простой обучающий пример приведён в Задаче 4.4.

На данном этапе нашей целью является сделать допущения, которые подходят для большого числа прикладных задач, использующих пространственные данные. Это допущения, описанные в работе Уайта (1980a), и они включают в себя три важных расхождения с допущениями во вводных курсах. Во-первых, регрессоры могут быть стохастическими (Предположения 1 и 3 ниже), так что предположения об ошибках являются условными по отношению к регрессорам. Во-вторых, условная дисперсия ошибок может быть различна для различных наблюдений (Предположение 5). В-третьих, от ошибок не требуется быть нормально распределёнными.

Вот эти предположения:
\begin{small}
\begin{enumerate}
\item Данные $(y_i, x_i)$ независимые, но неодинаково распределены по $i$.
\item Модель правильно специфицирована, так что
$$y_i = x_i' \beta + u_i .$$
\item Вектор регрессоров может быть стохастическим с конечной дисперсией, и $\E[|x_{ij}x_{ik}|^{1+\delta}]\leq \infty$ для всех $j, k = 1, \ldots , K$ для некоторой $\delta >0$, а матрица $M_{xx}$, определённая в (4.15) существует и является конечной положительно определённой матрицей ранга $K$. К тому же, в анализируемой выборке $X$ тоже имеет ранг $K$. 
\item Ошибки имеют нулевое математематическое ожидание при фиксированных регрессорах:
$$\E [u_i | x_i] = 0$$
\item Ошибки условно гетероскедастичны при фиксированных регрессорах с
\begin{equation}
\sigma_i^2 = \E[u_i^2 | x_i], \\
\Omega = \E [uu'|X] = \mathrm{Diag} [\sigma_i^2],
\end{equation}
где $\Omega$ является положительно определённой матрицей размера $N \times N$. К тому же, для некоторой $\delta>0$ верно $\E [|u_i^2|^{1+\delta}]< \infty$
\item Матрица $M_{x \Omega x}$, определённая в (4.16), существует и является конечной положительно определённой матрицей ранга $K$, где $M_{x \Omega x} = \plim N^{-1} \sum_{i} u_i^2 x_i x_i'$ при независимости по $i$. К тому же, для некоторой $\delta>0$ верно $\E [|u_i^2 x_{ij} x_{ik}|^{1+\delta}]< \infty$ для всех $j, k = 1, \ldots , K$. 
\end{enumerate}
\end{small}

\subsection{Примечания к предположениям}
Для полноты мы приводим подробное обсуждение каждого предположения, прежде чем перейти к ключевым результатам в следующем разделе.

\begin{center}
Стратифицированная случайная выборка
\end{center}

Предположение 1 --- одно из тех, которые часто делаются неявно при работе с пространственными данными. Здесь мы делаем его явным. Оно требует, чтобы $(y_i, x_i)$ были независимыми при различных $i$, но позволяет распределению различаться в зависимости от $i$. Многие микроэконометрические наборы данных основываются на \textbf{стратифицированной случайной выборке} (см. Раздел 3.2). В этом случае генеральная совокупность разбивается на страты, и из страт выбираются случайные наблюдения. Однако из одних страт наблюдения берутся с большей вероятностью, чем из других, вследствие чего выбранные $(y_i, x_i)$ независимые, но необязательно одинаково распределённые. Если же вместо этого данные происходили бы из \textbf{простой случайной выборки}, то $(y_i, x_i)$  были бы независимыми и одинаково распределёнными, что является более строгим предположением, чем наше и, по сути, его частным случаем. Во многих вводных курсах предполагается, что регрессоры являются \textbf{фиксированными в повторяющихся выборках}. В этом случае  $(y_i, x_i)$ независимы и неодинаково распределены, поскольку $y_i$ --- случайная величина, значение которой зависит от $x_i$. Предположение о фиксированных регрессорах редко выполняется для микроэконометрических данных, которые обычно основаны на наблюдении. С другой стороны, оно используется для экспериментальных данных, где $x$ --- уровень воздействия.

Эти различные предположения о распределении $(y_i, x_i)$ учитываются конкретными законами больших чисел и центральными предельными теоремами, которые используются для получения асимптотических свойств оценки МНК. Заметим, что, даже если $(y_i, x_i)$ независимы и одинаково распределены, $y_i$ при фиксированном $x_i$ не одинаково распределены, поскольку, например, $\E[y_i|x_i] = x_i'\beta$ различается в зависимости от $x_i$.

Предположение 1 отбрасывает большую часть временных рядов, поскольку в них наблюдения обычно являются зависимыми. Оно также нарушается, если устройство выборки предполагает кластеризацию наблюдений. Оценка МНК в этих случаях по-прежнему может быть состоятельной, если выполняются Предположения 2---4, но обычно её ковариационная матрица отличается от матрицы, представленной в этом разделе.

\begin{center}
Правильно специфицированная модель
\end{center}

Предположение 2 выглядит весьма очевидным, поскольку является важным ингредиентом при выводе оценки МНК. Однако его всё же стоит сделать явно, поскольку $\hat{\beta} = (X'X)^{-1} X'y$ --- функция от $y$, и её свойства зависят от $y$.

Если Предположение 2 выполняется, то предполагается, что модель регрессии линейна по $x$, что в регрессии нет \textit{пропущенных переменных}, и что в регрессорах нет \textbf{ошибки измерения}, так как регрессоры $x$, использующиеся для вычисления $\hat{\beta}$, являются теми же самыми, что были включены в процесс, порождающий данные. Вдобавок параметры $\beta$ одинаковы для всех индивидов, что исключает модели с переменными параметрами. 

Если Предположение 2 не выполняется, оценку МНК можно интерпретировать лишь как наилучшую из линейных оценок (см. Раздел 4.2.3).

\begin{center}
 Стохастические регрессоры
 \end{center} 
 
Предположение 3 допускает использование \textbf{стохастических регрессоров}, какими они обычно бывают, когда используются данные опросов, а не экспериментов. Предполагается, что в пределе выборочная ковариационная матрица постоянна и невырождена.

Если регрессоры распределены одинаково и независимо, как это предполагается при простой случайной выборке, то $M_{xx}=\E[xx']$, и Предположение 3 может быть упрощено до допущения, что вторые моменты существуют. Если регрессоры стохастические и независимые, но не обязательно одинаково распределённые, как это предполагается в стратифицированной выборке, необходимо более строгое Предположение 3, которое позволяет применить закон больших чисел Маркова, чтобы получить $\plim N^{-1}X'X$. Если регрессоры являются фиксированными в повторяющихся выборках --- обычное менее удачное предположение, которое часто делают во вводных курсах, --- то $M_{xx} = \lim N^{-1}X'X$, и Предположение 3 сводится к предположению, что этот предел существует.
 
 
\begin{center}
 Слабо экзогенные регрессоры
 \end{center} 
 
Предположение 4 о \textbf{нулевом условном математическом ожидании ошибок} необходимо, поскольку при использовании Предположения 2 из него следует, что $\E[y|X] = X\beta$, то есть условное математическое ожидание действительно такое, каким моделируется.

Из предположения, что $\E[u|X]=0$, следует, что $\Cov[x,u]=0$, то есть ошибка не коррелирует с регрессорами. Это следует из того, что $\Cov[x,u] = \E[xu]-\E[x]\E[u]$, а $\E[u|x]$ означает, что $\E[u]=0$ и $\E[xu]=0$ по закону о повторных математических ожиданиях. Более слабого предположения $\Cov[x,u]=0$ достаточно, чтобы МНК был состоятельным, тогда как более сильное предположение  $\E[u|X]=0$ требуется для несмещённости МНК.

Экономический смысл Предположения 4 состоит в том, что случайный член включает все исключённые из модели факторы, которые, как предполагается, не коррелируют с $X$, и они, в среднем, не оказывают никакого влияния на $y$. Это ключевое предположение, которое в Разделе 2.3 было названо \textit{предположением о слабой экзогенности}. По существу это означает, что знание процесса, порождающего $X$, не содержит полезной информации для оценки $\beta$. Если данная предпосылка не выполняется, один или более из $K$ регрессоров называются \textit{совместно зависимыми} c $y$ или просто \textit{эндогенными}. Общий термин для корреляции регрессоров с ненаблюдаемыми ошибками --- \textbf{эндогенность}, или \textbf{эндогенные регрессоры}.  Термин <<эндогенный>> здесь означает вызванный факторами внутри системы. Как будет показано в Разделе 4.7, нарушение слабой экзогенности может привести к несостоятельности оценки. Существует множество способов нарушения экзогенности, но в одном из наиболее часто встречающихся одна из переменных $x$ является выбором или решением индивида, связанным с $y$ в более широкой модели. Если игнорировать эти взаимосвязи, относясь к $x_i$, как будто его значение было случайно присвоено наблюдению $i$ и, таким образом, было некоррелировано с $u_i$, последствия будут нетривиальными. \textbf{Эндогенный отбор} исключается Предположением 4. Вместо этого, если данные собраны из стратифицированной выборки, это должна быть \textbf{экзогенная стратифицированная выборка}.

\begin{center}
  Условно гетероскедастичные ошибки
\end{center}  

Предполагаются \textbf{независимые ошибки регрессии}, не коррелирующие с регрессорами, как следствие Предположений 1, 2 и 4. Вводные курсы обычно сосредоточивают внимание на гомоскедастичных ошибках с однородной или постоянной дисперсией, где $\sigma_i^2 = \sigma^2$ для всех $i$. Тогда эти ошибки являются  независимыми одинаково распределенными $(0,\sigma^2)$ и называются \textbf{сферическими ошибками}, поскольку $\Omega = \sigma^2 \times \mathrm{I}$.

Предположение 5, напротив, вводит \textbf{условно гетероскедастичные ошибки регрессии} где \textit{гетероскедастичные} означает неоднородные или различные дисперсии. Это предположение сформулировано через $\E[u^2|x]$, но это эквивалентно дисперсии $\mathrm{V}[u|x]$, поскольку $\E[u|x]=0$ по Предположению 4. Это более общее предположение о гетероскедастичных ошибках сделано, потому что для пространственной регрессии оно часто оказывается верным. Более того, ослабление предположения о гомоскедастичности несущественно усложняет модель, так как корректные стандартные ошибки для МНК можно вычислить, даже если функциональная форма гетероскедастичности неизвестна.

Термин \textit{условная} гетероскедастичность используется по следующей причине. Даже если $(y_i, x_i)$ независимо и одинаково распределены, как в случае простой случайной выборки, условное математическое ожидание и условная диспресия $y_i$ могут зависеть от $x_i$. Точно так же, ошибки $u_i = y_i - x_i'\beta$ независимы и одинаково распределены в простой случайной выборке, и потому безусловно гомоскедастичны. Но как только мы поставим условие на $x_i$ и рассмотрим распределение $u_i$ \textit{при условии} $x_i$, дисперсия этого условного распределения может зависеть от $x_i$.

\begin{center}
Предел ковариационной матрицы $N^{-1}X'u$ 
\end{center}
Предположение 6 нужно, чтобы получить предельную ковариационную матрицу $N^{-1}X'u$. Если регрессоры независимы от ошибок (более сильное допущение, чем Предположение 4), то из Предположения 5 ($\E [|u_i^2|^{1+\delta}]< \infty$) и Предположения 3 ($\E[|x_{ij}x_{ik}|^{1+\delta}]\leq \infty$) следует, что $\E[|u_i^2 x_{ij}x_{ik}|^{1+\delta}]\leq \infty$

Мы намеренно не сделали седьмого предположения о том, что ошибка $u$ нормально распределена при условии $X$. Предположение о нормальности нужно, чтобы получить точное распределение оценки МНК для конечных выборок. Однако на протяжении этой книги мы фокусируемся на асимптотических методах, поскольку точные распределения для малых выборок редко доступны для оценок, использующихся в микроэконометрике, и предположение о нормальности больше не нужно.

\subsection{Вывод оценки МНК}
Ниже мы представляем распределение оценки МНК как для малых выборок, так и в пределе, и обосновываем применение оценки Уайта для ковариационной матрицы оценок МНК при выполнении Предположений 1---6.

\begin{center}
 Распределение в малых выборках
 \end{center} 

Параметр $\beta$ можно идентифицировать при выполнении предположений 1-4, поскольку в этом случае $\E	[y|X] = X\beta$ и $X$ имеет ранг $K$.

В маленьких выборках оценка МНК является несмещённой при выполнении предположений 1---4,  а её ковариационную матрицу легко получить, если истинно Предположение 5. Эти результаты можно получить, используя закон повторных математических ожиданий, то есть сначала взять ожидание $u$ при условии $X$,  а затем взять безусловное математическое ожидание. Тогда из (4.11) следует
\begin{equation}
\begin{array}{rcl}
\E[\hat{\beta}_{\text{OLS}}]  & = & \beta+\E_{X,u}[(X'X)^{-1}X'u] \\
& = &  \beta+\E_{X}[\E_{u|X}[(X'X)^{-1}X'u|X]] \\
& = & \beta+\E_{X}[(X'X)^{-1}X'\E_{u|X}[u|X]] \\
& = & \beta, \\
\end{array}
\end{equation}
здесь мы используем закон повторных ожиданий (Теорема A.23), а из выполнения предположений 1 и 4 следует, что $\E[u|X]=0$. Аналогично, из (4.11) следует
\begin{equation}
\mathrm{V}[\hat{\beta}_{\text{OLS}}] = \E_{X}[(X'X)^{-1}X'\Omega X (X'X)^{-1}],
\end{equation}
при Предположении 5, что $\E[uu'|X]=\Omega$. Мы использовали теорему A.23, говорящую, что в общем случае 
$$\mathrm{V}_{X,u}[g(X,u)] = \E_X[\mathrm{V}_{u|X}[g(X,u)]] + \mathrm{V}_{X}[\E_{u|X}[g(X,u)]]$$.
Это выражение можно упростить, обнулив второе слагаемое, поскольку $\E_{u|X}[(X'X)^{-1}X'u] = 0$.

Таким образом, оценка МНК является \textbf{несмещённой}, если $\E[u|X]=0$. Это ценное свойство, как правило, не обобщается на нелинейные оценки. Многие нелинейные оценки, такие, как нелинейный МНК, смещены, и даже линейные оценки, такие, как оценка метода инструментальных переменных, могут быть смещёнными. Оценка МНК \textbf{неэффективна}, так как её ковариационная матрица не является наименьшей ковариационной матрицей среди линейных несмещённых оценок, кроме случая, когда $\Omega = \sigma^2 \times \mathrm{I}$. Неэффективность МНК обосновывает использование других оценок, таких, как обобщённый МНК, хотя потеря эффективности МНК не всегда высока. При дополнительном допущении о нормальности ошибок при условии $X$, которое обычно не делается в микроэконометрических приложениях, оценка МНК нормально распределена при условии $X$.

\begin{center}
Состоятельность
\end{center}
Из Предположения 3 следует, что $\plim (N^{-1} X'X)^{-1}=M_{xx}^{-1}$. В этом случае состоятельность требует, чтобы выполнялось (4.13). Это доказывается, применяя закон больших чисел к среднему $N^{-1}X'u=N^{-1}\sum_i x_i u_i$, которое сходится по вероятности к нулю, если $\E[x_i u_i]=0$. При выполнении предположений 1 и 2, $x_i u_i$ распределены независимо и неодинаково, и предположения 1---5 допускают использование закона больших чисел Маркова (Теорема A.9). Если Предположение 1 упрощено до независимости и одинакового распределения $(y_i,x_i)$, то $x_i u_i$ тоже распределены независимо и одинаково, и предположения 1---4 позволяют использовать более простой закон больших чисел Колмогорова (Теорема A.8).

\begin{center}
Предельное распределение
\end{center}

Согласно Предположению 3,  $\plim (N^{-1} X'X)^{-1}=M_{xx}^{-1}$. Требуется получить предельное распределение $N^{-1}X'u=N^{-1}\sum_i x_i u_i$, применив центральную предельную теорему. При выполнении предположений 1 и 2, $x_i u_i$ распределены независимо, и предположения 1---6 допускают использование центральной предельной теоремы Ляпунова (Теорема A.15). Если Предположение 1 упрощено до независимости и одинакового распределения $(y_i,x_i)$, то $x_i u_i$ тоже распределены независимо и одинаково, и предположения 1---5 позволяют использовать более простую центральную предельную теорему Линдеберга-Леви (Теорема A.14).

Отсюда следует, что 
\begin{equation}
\frac{1}{\sqrt{N}}X'u \xrightarrow{d} \mathcal{N} [0,M_{x \Omega x}],
\end{equation}
где $M_{x \Omega x} =\plim N^{-1} X'uu'X = \plim \sum_i u_i^2 x_i x_i'$, если наблюдения независимы по $i$. Применяя закон больших чисел, получаем $M_{x \Omega x} = \lim N^{-1} \sum_i \E_{x_i}[\sigma_i^2 x_i x_i']$, из $\E_{u_i,x_i}[u_i^2 x_i x_i'] = \E_{x_i}[\E[u_i^2|x_i]x_i x_i']$ и $\sigma_i^2 = \E[u_i^2|x_i]$. Следовательно, $,M_{x \Omega x} = \lim N^{-1} \E[X'\Omega X]$, где $\Omega = \mathrm{Diag}[\sigma_i^2]$, и математическое ожидание берётся только по $X$, а не по $X$ и $u$.

Данное представление материала предполагает независимость по $i$. В более общем случае мы можем допустить корреляцию между наблюдениями. Тогда $M_{x\Omega x} = \plim N^{-1} \sum_i \sum_j u_i u_j x_i x_j'$, а элемент $ij$ матрицы $\Omega$ равен $\sigma_{ij} = \Cov[u_i, u_j]$. Это обобщение отложено до изучения оценивания методом нелинейного МНК в Разделе 5.8.

\begin{center}
Стандартные ошибки, устойчивые к гетероскедастичности
\end{center}

Рассмотрим ключевой шаг состоятельного оценивания $M_{x \Omega x}$. Начав с первоначального определения $M_{x \Omega x} = \plim \sum_i u_i^2 x_i x_i'$, заменим $u_i$ на $\hat{u}_i = y_i-x_i'\hat{\beta}$, где асимптотически  $\hat{u}_i \xrightarrow{p} u_i$, поскольку $\hat{\beta} \xrightarrow{p} \beta$. Отсюда выводится состоятельная оценка
\begin{equation}
\hat{M}_{x \Omega x} = \frac{1}{N} \sum_{i=1}^{N}\hat{u}_i^2 x_i x_i' = N^{-1} X' \hat{\Omega} X,
\end{equation}
где $\hat{\Omega} = \mathrm{Diag}[\hat{u}_i^2]$. Необходимо дополнительное предположение о том, что $\E [|x_{ij}^2 x_{ik} x_{il}|^{1+\delta}]< \Delta$ для положительных констант $\delta$  и $\Delta$ и $j,k,l = 1, \ldots , K$, так как $\hat{u}_i^2 x_i x_i' = (u_i - x_i'(\hat{\beta} - \beta))^2 x_i x_i'$ содержит степени вплоть до четвертой $x_i$ (см. работу Уайта (1980a)).

Заметим, что $\hat{\Omega}$ не сходится к матрице $\Omega$ размера $N \times N$. Без внесения дополнительных предположений это очевидно невозможная задача, поскольку требуется оценить $N$ дисперсий $\sigma_i^2$. Однако, всё, что нам нужно, это сходимость $N^{-1} X' \hat{\Omega} X$ к матрице $\plim N^{-1} X' \Omega X =  N^{-1} \plim \sum_i \sigma_i^2 x_i x_i'$  размером $K \times K$. Этого достичь проще, поскольку число регрессоров $K$ фиксировано. Чтобы понять оценку Уайта, рассмотрим оценку МНК для модели с одним только свободным членом $y_i = \beta + u_i$ и с гетероскедастичной ошибкой. Тогда в нашей системе обозначений мы можем показать, что $\hat{\beta} = \bar{y}$, $M_{xx} = \lim N^{-1} \sum_{i} 1 = 1$, а $M_{x \Omega x} = \lim N^{-1} \sum_{i} \E[u_i^2]$. Очевидно, $M_{x \Omega x}$ можно оценить с помощью $\hat{M}_{x \Omega x} = N^{-1} \sum_{i} \hat{u}_i^2$, где $ \hat{u}_i = y_i-\hat{\beta}$. Чтобы получить предел по вероятности этой оценки, достаточно рассмотреть $N^{-1}\sum_i u_i^2$, поскольку $\hat{u}_i - u_i \xrightarrow{p} 0$, если $\hat{\beta}\xrightarrow{p}\beta$. Если можно применить закон больших чисел, это среднее значение сходится по вероятности к своему математическому ожиданию, так что $\plim N^{-1} \sum_i u_i^2 = \lim N^{-1} \E[u_i^2] = M_{x \Omega x}$, как и требовалось. Айкер (1967) приводит формальные условия для этого примера.

\section{Взвешенный метод наименьших квадратов}

В ситуациях, когда требуется использовать робастные стандартные ошибки, обычно возможно увеличить эффективность оценки. Например, в присутствии гетероскедастичности оценка доступного обобщённого МНК (ОМНК) эффективнее, чем оценка МНК.

В этом разделе мы представляем оценку доступного ОМНК, которая использует более сильные предположения о дисперсии случайного члена. Тем не менее, можно получить стандартные ошибки доступного ОМНК, которые были бы устойчивы к неправильной спецификации дисперсии ошибки, как и в случае МНК.

Многие микроэконометрические работы не используют возможное преимущество в эффективности, которое даёт ОМНК, по причинам удобства и потому, что ожидают относительно небольшой выигрыш в эффективности. Вместо этого общепринятым является использование менее эффективных взвешенных оценок, особенно МНК, с робастными оценками стандартных ошибок.

\subsection{ОМНК и доступный ОМНК}

Согласно теореме Гаусса-Маркова, приводимой во вводных текстах, оценка МНК является эффективной в классе линейных несмещенных оценок, если ошибки в линейной регрессионной модели независимы и гомоскедастичны. 

Вместо этого, мы предполагаем, что ковариационная матрица ошибок равна $\Omega \neq \sigma^2\mathrm{I}$.  Если $\Omega$ известна и невырождена, мы можем домножить линейную модель регрессии (4.8) на $\Omega^{-1/2}$, где $\Omega^{-1/2}\Omega^{-1/2}=\Omega$, чтобы получить 
$$
\Omega^{-1/2} y  = \Omega^{-1/2} X\beta + \Omega^{-1/2} u
$$
Применив алгебру, получим $\V [\Omega^{-1/2} u] = \E [(\Omega^{-1/2} u)(\Omega^{-1/2} u)'|X] = \mathrm{I}$. Таким образом, ошибки в этой преобразованной модели обладают нулевым математическим ожиданием, одинаковой --- единичной --- дисперсией и некоррелированы друг с другом. Поэтому $\beta$ можно эффективно оценить с помощью МНК-регрессии $\Omega^{-1/2} y$ на $\Omega^{-1/2} X$. 

Этот аргумент приводит к \textbf{оценке обобщённого метода наименьших квадратов} (generalized least-squares, GLS):
\begin{equation}
\hat{\beta}_{GLS} = (X'\Omega^{-1}X)^{-1}X'\Omega^{-1}y
\end{equation}
Оценку ОМНК нельзя использовать непосредственно, поскольку на практике $\Omega$ неизвестна. Вместо этого мы специфицируем $\Omega = \Omega(\gamma)$, где $\gamma$ - конечномерный вектор параметров, получаем его состоятельную оценку $\hat{\gamma}$, и формируем $\hat{\Omega} =  \Omega(\hat{\gamma})$. Например, если ошибки гетероскедастичны, можно положить $\V[u|x] = \exp (z'\gamma)$, где $z$ --- подмножество $x$, а экспоненциальная функция используется, чтобы обеспечить положительную дисперсию. Тогда $\hat{\gamma}$ можно оценить состоятельно с помощью регрессии методом нелинейного МНК квадрата ошибки $\hat{u}^2_i = (y-x\hat{\beta}_{\text{OLS}})^2$ на $ \exp (z'\gamma)$. Эту оценку $\hat{\Omega}$ можно затем использовать вместо $\Omega$ в (4.28). Заметим, что нельзя  заменить $\Omega$ в (4.28) на $\hat{\Omega} =  \mathrm{Diag}[\hat{u}^2_i]$, поскольку такая оценка будет несостоятельной (см. Раздел 5.6.8).

Оценкой \textbf{доступного обобщённого МНК} (feasible GLS) называют
\begin{equation}
\hat{\beta}_{\text{FGLS}} = (X'\hat{\Omega}^{-1}X)^{-1}X'\hat{\Omega}^{-1}y.
\end{equation}
Если Предположения 1---6 выполняются и $\Omega(\gamma)$ правильно специфицирована --- сильное предположение, которое в дальнейшем будет ослаблено, --- и $\hat{\gamma}$ состоятельно оценивает $\gamma$, то можно показать, что
\begin{equation}
\sqrt{N}(\hat{\beta}_{\text{FGLS}} - \beta) \xrightarrow{d} \mathcal{N} [0, (\plim N^{-1}X'\Omega X)^{-1}].
\end{equation}

Оценка доступного ОМНК имеет ту же предельную ковариационную матрицу, что и оценка ОМНК, и поэтому является эффективной. На практике в (4.30)  $\Omega$ заменяют на $\hat{\Omega}$. 

Можно показать, что оценка ОМНК минимизирует $u' \Omega^{-1} u$ (см. упражнение 4.5), что упрощается до $\sum_i u_i^2/\sigma_i^2$, если ошибки гетероскедастичные, но некоррелированы. Мотивацией для ОМНК было эффективное оценивание $\beta$. С точки зрения обсуждаемых в Разделе 4.2 функции потерь и оптимального предсказания, для гетероскедастичных ошибок можно указать функцию потерь $\Loss(e) = e^2/\sigma^2$. В сравнении с функцией потерь для МНК $\Loss(e) = e^2$, функция потерь ОМНК налагает относительно меньший штраф на ошибки прогноза для наблюдений с большей условной дисперсией случайного члена. 

\subsection{Взвешенный МНК}

Результат (4.30) предполагает верную спецификацию ковариационной матрицы ошибок $\Omega(\gamma)$. Если это не так, оценка доступного ОМНК по-прежнему состоятельна, но (4.30) даёт неверную дисперсию. К счастью, можно найти робастную оценку дисперсии оценки ОМНК, даже если $\Omega(\gamma)$ специфицирована неверно.

Определим $\Sigma = \Sigma(\gamma)$ как \textbf{рабочую ковариационную матрицу}, которая не обязательно совпадает с истинной ковариационной матрицей $\Omega  = \E [uu'|X]$. Сформируем оценку $\hat{\Sigma} = \Sigma(\hat{\gamma})$, где $\hat{\gamma}$ --- оценка $\gamma$. Тогда можно использовать аналог ОМНК с матрицей весов $\hat{\Sigma}^{-1}$.

Так получается оценка \textbf{взвешенного МНК} (weighted least-squares, WLS):
\begin{equation}
\hat{\beta}_{\text{WLS}} = (X'\hat{\Sigma}^{-1}X)^{-1}X'\hat{\Sigma}^{-1}y.
\end{equation}
Статистические выводы делаются без использования предпосылки $\Sigma = \Omega$.  В литературе этот подход называют подходом рабочей матрицы весов. Мы называем его взвешенным МНК, но сознаём, что другие под этим названием подразумевают ОМНК или доступный ОМНК в особом случае, когда матрица $\Omega ^{-1}$ диагональная. Здесь мы не делаем таких допущений о матрице весов $\Sigma^{-1} = \Omega^{-1}$.

Используя алгебраические преобразования, как при получении оценки МНК в Разделе 4.4.5, можно получить оценку асимптотической ковариационной матрицы. 
\begin{equation}
\hat{\V}[\hat{\beta}_{\text{WLS}}] = (X'\hat{\Sigma}^{-1}X)^{-1}X'\hat{\Sigma}^{-1}\hat{\Omega}\hat{\Sigma}^{-1}X(X'\hat{\Sigma}^{-1}X)^{-1},
\end{equation}
где $\Omega$ --- такая матрица, что
$$
\plim N^{-1} X'\hat{\Sigma}^{-1}\hat{\Omega}\hat{\Sigma}^{-1}X = \plim N^{-1} X'\Sigma^{-1}\Omega\Sigma^{-1}X.
$$
В случае гетероскедастичности $\hat{\Omega} = \mathrm{Diag}[\hat{u}_i^{*2}]$, где $\hat{u}_i^{*2} = y_i - x_i'\hat{\beta}_{\text{WLS}}$.

В случае гетероскедастичных ошибок базовым подходом является выбор простой модели гетероскедастичности, такой, где дисперсия ошибок зависит только от одного или двух ключевых регрессоров. Например, в линейной регрессионной модели уровня зарплат как функции от образования и других переменных гетероскедастичность можно моделировать как функцию одного только уровня образования. Пусть из этой модели можно получить $\hat{\Sigma} = \mathrm{Diag}[\hat{\sigma}_i^2]$. Тогда МНК-регрессия $y_i/\sigma_i$ на $x_i/\sigma_i$ (без константы) даёт оценку $\hat{\beta}_{\text{WLS}}$, а стандартные ошибки Уайта для этой регрессии, устойчивые к гетероскедастичности, как можно показать, описываются (4.32).

Подход взвешенного МНК или рабочей матрицы особенно удобен, когда есть более чем одна проблема. Например, в модели случайных эффектов, основанной на панельных данных (Глава 21), ошибки можно рассматривать одновременно как коррелированные во времени для данного индивида и гетероскедастичные. Можно использовать оценку случайных эффектов, которая устраняет только первую проблему, но потом рассчитать стандартные ошибки для этой оценки, устойчивые к гетероскедастичности.

Различные оценки метода наименьших квадратов приведены в таблице 4.2.
\begin{table}[h]
\begin{center}
\caption{\label{tab:gls}Оценки МНК и их асимптотические ковариационные матрицы}
\begin{tabular}[t]{lll}
\hline
\hline
\bf{Оценка\footnote{Оценки для линейной модели с условной ковариационной матрицей ошибок $\Omega$. Для ОМНК предполагается, что $\hat{\Omega}$ состоятельна. Для МНК и ВМНК устойчивая к гетероскедостичности ковариационная матрица $\beta$  использует $\hat{\Omega}$, равную диагональной матрице с квадратами остатков на диагонали.}} & \bf{Определение} & \bf{Оценка предельной дисперсии}  \\
\hline
МНК &  $\hat{\beta} = (X'X)^{-1}X'y$  & $(X'X)^{-1}X'\hat{\Omega}X(X'X)^{-1}$ \\
Доступный ОМНК &  $\hat{\beta} = (X'\hat{\Omega}^{-1}X)^{-1}X'\hat{\Omega}^{-1}y$   & $(X'\hat{\Omega}^{-2}X)^{-1}$ \\
Взвешенный МНК & $\hat{\beta} = (X'\hat{\Sigma}^{-1}X)^{-1}X'\hat{\Sigma}^{-1}y$   &
$(X'\hat{\Sigma}X)^{-1}X'\hat{\Sigma}^{-1}\hat{\Omega}\hat{\Sigma}^{-1}X(X'\hat{\Sigma}X)^{-1}$ \\
\hline
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Пример робастных стандартных ошибок}

В качестве примера оценивания с использованием робастных стандартных ошибок рассмотрим оценивание стандартных ошибок коэффициента наклона в модели с мультипликативной гетероскедастичностью
$$
y = 1 + 1\times x +u,
$$
$$
u = x\epsilon,
$$
где скалярный регрессор $x \sim \mathcal{N} [0, 25]$, а $\epsilon\sim \mathcal{N} [0, 4]$.

Эти ошибки обладают условной гетероскедастичностью, так как $\V [u|x] = \V [x\epsilon | x] = x^2\V[\epsilon|x] = 4x^2$ --- функция от регрессора $x$. Это не так для безусловной дисперсии, где $\V[u] = \V[x\epsilon] = \E[(x\epsilon)^2]-(\E[x\epsilon])^2 = \E[x^2] \E[\epsilon^2] = \V[x] \V[\epsilon] = 100$, при условии, что $x$ независимы $\epsilon$.

Стандартные ошибки оценки МНК следует рассчитывать с помощью оценки, устойчивой к гетероскедастичности (4.21). Поскольку МНК не вполне эффективен, ВМНК может помочь увеличить эффективность. ОМНК даст выигрыш в эффективности в любом случае, и в примере с искусственно сгенерированными данными мы пользуемся преимуществом знания того, что $\V [u|x] = 4x^2$. Все способы оценивания дают несмещённые оценки свободного члена и коэффициента наклона.

Различные оценки МНК и связанные с ними стандартные ошибки из сгенерированного набора данных размером 100 приведены в Таблице 4.3. Нас интересует в первую очередь коэффициент наклона.

\begin{table}[h]
\caption{\label{tab:wls} ВМНК: пример с условно гетероскедастичными ошибками}
\begin{minipage}{\textwidth}
\begin{tabular}[t]{llll}
\hline
\hline
 & \bf{МНК}  \footnote{Сгенерированные данные для выборки размером 100. МНК, ОМНК и ВМНК все состоятельны, но МНК и ВМНК неэффективны. Даны два различных типа стандартных ошибок: в круглых скобках стандартные ошибки по умолчанию, предполагающие гомоскедастичные ошибки, а в квадратных скобках стандартные ошибки, устойчивые к гетероскедастичности. Процесс, порождающий данные, описан в тексте.} & \bf{ВМНК} & \bf{ОМНК}  \\
\hline
    Константа & 2.213 & 1.060  & 0.996 \\
          & (0.823) & (0.150) & (0.007) \\
          & [0.820] & [0.051] & [0.006] \\
    $x$ & 0.979 & 0.957 & 0.952 \\
          & (0.178) & (0.190) & (0.209) \\
          & [0.275] & [0.232] & [0.208] \\
    $R^2$ & 0.236 & 0.205 & 0.174 \\

\hline
\hline
\end{tabular}
\end{minipage}
\end{table}

Оценка МНК коэффициента наклона равна 0.979. Показаны две стандартные ошибки, и правильная, устойчивая к гетероскедастичности (рассчитанная согласно (4.21)), равна 0.275, что намного больше, чем ошибка 0.177, использующая $s^2(X'X)^(-1)$. Такая существенная разница в оценках стандартных ошибок может привести к принципиальным различиям в статистических выводах. В этом примере можно показать что теоретически, в пределе, робастные стандартные ошибки в $\sqrt{3}$ раз больше неправильных. Для конкретно этого примера с размером выборки $N$ правильные и неправильные оценки стандартных ошибок коэффициента наклона сходятся, соответственно, к $\sqrt{12 / N}$ и $\sqrt{4 / N}$. 

В качестве примера оценки ВМНК, предположим, что $u = \sqrt{|x|}\epsilon$, а не $u = x\epsilon$, то есть что $\V [u|x] = \sigma^2 |x|$. Оценку ВМНК тогда можно получить с помощью регрессии МНК, предварительно поделив $y$, свободный член и $x$ на $\sqrt{|x|}$. Так как это неверная модель гетероскедастичности, правильная (робастная) оценка стандартной ошибки для коэффициента наклона рассчитывается по формуле (4.32) и равна 0.232.

Оценку ОМНК для этой модели можно получить с помощью регрессии МНК, предварительно поделив $y$, свободный член и $x$ на $|x|$, так как в этом случае ошибки станут гомоскедастичными. Обычная и робастная оценки стандартной ошибки коэффициента наклона теперь очень похожи (0.209 и 0.208). Это ожидаемо, так как обе асимптотически несмещённые, поскольку оценка ОМНК в данном случае использует верную модель гетероскедастичности. Можно показать теоретически, что в этом случае стандартная ошибка оценки ОМНК коэффициента наклона сходится к $\sqrt{4 / N}$. 

И МНК, и ВМНК менее эффективны, чем ОМНК, как и ожидалось, со стандартными ошибками коэффициента наклона $0.275 >  0.232 > 0.208$.

Модель в данном примере похожа на стандартные модели, использующиеся при оценивании пространственных данных. И $y$, и $x$ --- случайные величины.  Пары $(y_i, x_i)$ независимы по $i$ и одинаково распределены, как при случайной выборке. Однако условное распределение $y_i|x_i$ различается для разных $i$, поскольку условные математическое ожидание и дисперсия $y_i$ зависят от $x_i$.

\section{Медианная и квантильная регрессия}

В модели с одним только свободным членом описательные статистики для выборочного распределения кроме выборочного среднего включают квантили, такие, как медиана, верхний и нижний квартили и процентили.

В контексте регрессии нас могут подобным образом интересовать условные квантили. Например, интерес может представлять то, насколько процентили распределения доходов низкообразованных работников более сжаты, чем у работников с высоким уровнем образования. В этом простом примере можно просто разделить вычисления для низко- и высокообразованных работников. Однако этот подход становится неприменимым, когда есть несколько регрессоров, каждый из которых может принимать несколько различных значений. Вместо этого требуется использовать методы квантильной регрессии, чтобы оценить квантили условного распределения $y$ при условии $x$.

Согласно Таблице 4.1, квантильной регрессии соответствует функция потерь с асимметричным модулем ошибки, а в специальном случае медианной регрессии --- с модулем ошибки. Эти методы являются альтернативой МНК, пользующимся квадратичной функцией потерь.

Методы квантильной регрессии имеют другие преимущества помимо предоставления более богатой характеристики данных. Медианная регрессия более устойчива к выбросам, чем регрессия МНК. Более того, оценки квантильной регрессии могут быть состоятельными при более слабых предположениях, чем это возможно для оценок МНК. Ведущими примерами являются оценка максимального счета Мански (1975) (maximum score estimator) для моделей с бинарной зависимой переменной (см. Раздел 14.6) и цензурированная оценка наименьших модулей Пауэлла (1984) для цензурированных моделей (см. Раздел 16.6).

Мы начнём с краткого объяснения квантилей в генеральной совокупности, а после перейдём к оценке выборочных квантилей.

\subsection{Квантили в генеральной совокупности}

Для непрерывной случайной величины $y$ $q$-ой квантилью в генеральной совокупности называется число $\mu_q$, такое, что $y$, меньшие или равные $\mu_q$ встречаются с вероятностью $q$. То есть
$$
q = \mathrm{Pr}[y \leq \mu_q] = F_y(\mu_q),
$$
где $F_y$ ---  функция распределения  $y$. Например, если $\mu_{0.75}=3$, то вероятность, что $y\leq 3$ равна $0.75$. Следовательно,
$$
\mu_q = F_y^{-1}(q).
$$
Популярными примерами являются медиана, $q=0.5$, верхняя квартиль, $q=0.75$, и нижняя квартиль, $q=0.25$. Для стандартного нормального распределения $\mu_{0.5}=0$, $\mu_{0.95}=1.645$, а $\mu_{0.975}=1.960$.  $100q$-той \textbf{процентилью} называется $q$-тая квантиль.

В регрессионной модели \textbf{$q$-той квантилью в генеральной совокупности} для $y$ при условии $x$ называют функцию $\mu_q(x)$, такую, что при условии $x$ вероятность, что $y$ не превышает  $\mu_q$, равна $q$, где эта вероятность вычислена с помощью условного распределения $y$ при данном $x$. Тогда
\begin{equation}
\mu_q(x) = F^{-1}_{y|x}(q),
\end{equation}
где $F_{y|x}$ --- условная функция распределения $y$ при $x$, и мы опустили  параметры  распределения.

Познавательным будет вывести квантильную функцию $\mu_q(x)$ при предположении, что $y$ порождён линейной моделью с мультипликативной гетероскедастичностью
\[
\begin{array}{c} 
y = x'\beta + u \\ 
u = x'\alpha \times \epsilon \\
\epsilon \sim\mathrm{iid} [0, \sigma^2],
\end{array}
\]
где предполагается, что $x'\alpha > 0$. Тогда $q$-тая квантиль генеральной совокупности $y$  при условии $x$ равна  функции $\mu_q(x,\beta,\alpha)$, такой что
\[
\begin{array}{rcl} 
q & = & \mathrm{Pr} [y\leq \mu_q(x,\beta,\alpha)] \\
& = & \mathrm{Pr} [u \leq \mu_q(x,\beta,\alpha) - x\beta] \\
& = & \mathrm{Pr} [\epsilon \leq [\mu_q(x,\beta,\alpha) - x\beta]/x'\alpha] \\
& = & F_{\epsilon} ([\mu_q(x,\beta,\alpha) - x\beta]/x'\alpha),
\end{array}
\]
где мы используем $u = y-x'\beta$ и $\epsilon = u/x'\alpha$, а $F_{\epsilon}$ --- функция распределения $\epsilon$. Следовательно, $[\mu_q(x,\beta,\alpha) - x\beta]/x'\alpha =F^{-1}_q$, и 

\[
\begin{array}{rcl}
\mu_q(x,\beta,\alpha) & = & x'\beta +x'\alpha \times F^{-1}_q \\
& = & x' (\beta + \alpha \times F^{-1}_q).
\end{array}
\]

Таким образом, в линейной модели с мультипликативной гетероскедастичностью в форме $u = x'\alpha \times \epsilon$ условные квантили также линейны по $x$. В особом случае гомоскедастичности $x'\alpha$ константа и все условные квантили имеют один и тот же наклон и различаются только свободным членом, который увеличивается при увеличении $q$.

В более общем случае квантильная функция может быть нелинейной по $x$ благодаря другим формам гетероскедастичности, таким, как $u=h(x,\alpha) \times \epsilon$, где $h(\cdot)$ нелинейна по $x$, или потому что сама регрессионная функция имеет нелинейную форму $g(x, \beta)$. Однако принято оценивать линейные квантильные функции и интерпретировать их как наилучшие линейные прогнозы при функции ошибок квантильной регрессии, приведённой в (4.34) в следующем разделе.  

\subsection{Выборочные квантили }

Для одномерной случайной величины $y$ обычном способом вычисления квантилей является сортировка выборки по возрастанию. Тогда $\hat{\mu_q}$ равняется $[Nq]$-тому наименьшему значению, где $N$ --- размер выборки, а  $[Nq]$ обозначает $Nq$, округлённое вверх до ближайшего целого. Например, если $N=97$, нижним квартилем считается 25-тое наблюдение, поскольку $[97 \times 0.25] =[24.25] = 25$.

Коэнкер и Бассетт (1978) заметили, что $q$-тую \textbf{выборочную квантиль} $\hat{\mu}_q$ можно выразить по-другому как решение проблемы минимизации по $\beta$ функции
$$\sum_{i:y_i \geq x_i'\beta}^{N} q|y_i-\beta| + \sum_{i:y_i < x_i'\beta}^{N} (1-q)|y_i-\beta|. $$
Результат не очевиден. Чтобы осознать его, рассмотрим медиану, для которой $q=0.5$. Тогда медиану можно найти как минимум $\sum_i |y_i -\beta|$. Представим, что в выборке из 99 наблюдений 50-е по возрастанию наблюдение, медиана, равно 10, а 51-е наблюдение равно 12. Если мы установим $\beta$ 12, а не 10, то $\sum_i |y_i -\beta|$ увеличится на 2 для меньших 50 наблюдений и уменьшится на 2 для оставшихся 49 наблюдений, то есть в целом увеличится на $50 \times 2 - 49 \times 2 = 2$. Таким образом, 51-е наблюдение --- худший выбор, чем 50-е. Аналагично можно показать, что 49-е наблюдение также является худшим выбором, чем 50-е.

Эту целевую функцию легко обобщить на случай линейной регрессии, так, что $q$-тая \textbf{оценка квантильной регрессии} $\hat{\beta}_q$ минимизирует
\begin{equation}
Q_n(\beta_q) = \sum_{i:y_i \geq x_i'\beta_q}^{N} q|y_i-x_i'\beta_q| + \sum_{i:y_i < x_i'\beta_q}^{N} (1-q)|y_i-x_i'\beta_q|,
\end{equation}
где мы используем $\beta_q$ вместо $\beta$, чтобы показать явно, что различный выбор $q$ приводит к различным оценкам $\beta$. Заметим, что это функция потерь с асимметричным модулем, приведнная в Таблице 4.1, где $\hat{y}$ ограничен до линейного по $x$, так, что $e = y -x'\beta$.  Особый случай $q=0.5$ назвается \textbf{оценкой медианной регрессии} или \textbf{оценкой наименьших абсолютных отклонений}. 

\subsection{Свойства оценок квантильной регрессии}

Целевая функция (4.34) не является дифференцируемой, и потому градиентные методы оптимизации, описанные в Главе 10, не применимы к ней. К счастью, можно использовать методы линейного программирования, и они делают возможным относительно быстрое вычисление $\hat{\beta}_q$.

Так как явного выражения для $\hat{\beta}_q$ не существует, определить асимптотическое распределение $\hat{\beta}_q$, пользуясь подходом, применённым в Разделе 4.4 для МНК, невозможно. Методы Главы 5 также требуют адаптации, поскольку целевая функция недифференцируема. Можно показать, что 
\begin{equation}
\sqrt{N}(\hat{\beta}_q-\beta_q)  \xrightarrow{d} \mathcal{N} [0, A^{-1}BA^{-1}],
\end{equation}
(см. например Бучински, 1998, стр. 85), где 
\begin{equation}
A = \plim \frac{1}{N} \sum_{i=1}^{N} f_{u_{q}}(0|x_i)x_i'x_i, \\
B = \plim \frac{1}{N} \sum_{i=1}^{N}q(1-q)x_i'x_i, \\
\end{equation}
а $f_{u_{q}}(0|x_i)$ --- условная плотность распределения случайного члена $u_q = y -x'\beta_q$, подсчитанная в точке $u_q = 0$. Оценка дисперсии $\hat{\beta}_q$ затруднена необходимостью оценки  $f_{u_{q}}(0|x_i)$. Проще оценивать стандартные ошибки $\hat{\beta}_q$, используя методы парного бутстрэпа, описанные в Главе 11.

\subsection{Пример квантильной регрессии}

В этом разделе мы производим оценку условных квантилей и сравниваем её с обычной оценкой условного математического ожидания, использующей регрессию МНК. В этом приложении оценивается кривая Энгеля для годовых расходов домохозяйств на медицину. Более конкретно, мы рассматриваем регрессионное соотношение между логарифмом медицинских расходов и логарифмом совокупных расходов домохозяйств. Таким образом, эта регрессия оценивает (постоянную) эластичность медицинских расходов по совокупным расходам.

Данные взяты из Вьетнамского исследования стандартов жизни (Vietnam Living Standards Survey), проведённого Мировым банком в 1997 году. Выборка состоит из 5006 домохозяйств, понесших положительные расходы на медицину, после исключения 16.6\% выборки, которые имели нулевые расходы, чтобы было возможно взятие логарифма. Нулевые значения можно обрабатывать с помощью методов цензурированной квантильной регрессии, разработанной Пауэллом (1986a), представленной в Разделе 16.9.2. Для простоты мы не использовали эти наблюдения в расчётах. Наибольшая доля медицинских расходов, особенно при низких уровнях дохода, состоит из лекарств, купленных в аптеках. Хотя доступны различные характеристики домохозяйств, для простоты мы рассмотрим только один регрессор, логарифм совокупных расходов домохозяйства, в качестве прокси для  совокупного дохода домохозяйства. 

Линейная регрессия МНК даёт оценку эластичности, равную 0.57. Эту оценку обычно интерпретируют в том смысле, что лекарства являются <<товаром первой необходимости>>, а потому спрос на них не эластичен по доходу. Эта оценка не является неожиданной, но прежде чем принять её на веру, мы должны допустить, что между различными группами по доходу эластичность может быть неоднородна.

Квантильная регрессия является полезным инструментом для изучения такой неоднородности, как подчёркивают Коэнкер и Халлок (2001). Мы минимизируем функцию (4.34), где $y$ --- логарифм медицинских расходов, а $x'\beta = \beta_1 +\beta_2 x$, где $x$ --- логарифм совокупных расходов домохозяйства. Это было сделано для 19 квантилей $q = {0.05, 0.10,\ldots , 0.95}$, где $q = 0.5$ --- медиана. Результаты этого упражнения помещены на Рис. 4.1 и Рис. 4.2. 


Slope and confidence bands --- Коэффициент наклона с доверительным интервалом

Quantiles --- Квантиль

Slope estimates as constant varies --- Зависимость оценки углового коэффициента от порядка квантили.

OLS slope coefficinet --- МНК оценка наклона

Quantile slope coefficinet --- Квантильная оценка наклона

Lower 95\% confidence band --- Нижняя граница 95\% доверительного интервала

Upper 95\% confidence band --- Верхняя граница 95\% доверительного интервала


\begin{figure}[t]
 \caption{Оценки квантильной регрессии для коэффициента наклона и $q = 0.05,0.10,\ldots 0.95$ и соответствующие 95\% доверительные интервалы, нарисованные в зависимости от $q$, из регрессии логарифма медицинских расходов на логарифм совокупных расходов.}
\end{figure}


Рисунок 4.1 изображает коэффициент наклона $\hat{\beta}_2$ для различных значений $q$ и соответствующий ему доверительный интервал. График показывает, как квантильная оценка эластичности изменяется в зависимости от $q$. Оценка эластичности систематически увеличивается с ростом дохода домохозяйста, поднимаясь от 0.15 при $q=0.05$ до максимума в $0.80$ при $q=0.85$. МНК-оценка наклона, равная 0.57,  представлена в виде горизонтальной линии, не зависящей от квантили. Оценки эластичности на верхних и нижних квантилях определённо статистически отличаются друг от друга и от оценки МНК, которая имеет стандартную ошибку 0.032. Кажется вероятным, что агрегированная оценка эластичности будет меняться соответственно изменениям в распределении дохода. Этот график поддерживает наблюдения Мостеллера и Тьюки (1977, стр.236), цитируемые Коэнкером и Халлокок (2001), о том, что, сосредотачиваясь только на функции условного среднего, регрессия МНК предоставляет неполное описание совместного распределения зависимой и объясняющей переменных.

\begin{figure}[t]
 \caption{Графики оценки квантильной регрессии для $q=0.1, q=0.5$ и $q=0.9$ из регрессии логарифма медицинских расходов на логарифм совокупных расходов. Данные по 5006 вьетнамским домохозяйствам с положительными медицинскими расходами в 1997 году.}
\end{figure}

Regression Lines as Quantile Varies --- Линии регрессии для разных квантилей

Log Household Medical Expenditure --- Логарифм медицинских расходов домохозяйства

Log Household Total Expenditure --- Логарифм всех расходов домохозяйства


Actual Data -- Фактические данные
 
90th percentile --- 90-ый перцентиль

Median --- медиана
 
10th percentile --- 10-ый перцентиль


Рисунок 4.2 совмещает три оцененные линии квантильной регрессии $\hat{y}_q = \hat{\beta}_{1,q}+\hat{\beta}_{2,q}x$ для $q=0.1, 0.5, 0.9$ и линию регрессии МНК. Линия регрессии МНК полностью совпадает с медианной ($q=0.5$) линией регрессии. На рисунке 4.2 заметно расхождение линий квантильных регрессий при росте $q$. Это не удивительно, если оценки наклона увеличиваются, как это видно по Рис. 4.1. Коэнкер и Бассетт (1982) разработали квантильную регрессию как средство тестирования ошибок на гетероскедастичность, когда процесс, порождающий данные, является линейной моделью. В этом случае расхождение линий квантильных регрессий рассматривается как свидетельство гетероскедастичности. Другая интерпретация состоит в том, что условное математическое ожидание нелинейно по $x$, а, напротив, наклон возрастает, и это приводит к возрастанию квантильных коэффициентов наклона по $q$.

Более подробная информация о квантильных регрессиях дана в работе Бучински (1994) и Коэнкера и Халлока (2001).

\section{Ошибки спецификации модели}

Термин <<ошибки спецификации>> в самом широком смысле обозначает, что как минимум одно предположение о процессе, порождающем данные, некорректно. Ошибки спецификации могут происходить по отдельности или в комбинации, но анализ упрощается, если рассматривать последствия только одной ошибки.

В следующем обсуждении мы подчёркиваем ошибки спецификации, которые приводят к несостоятельности оценок МНК и неидентифицируемости параметров, представляющих интерес. Оценка МНК, тем не менее, может по-прежнему обладать содержательной интерпретацией, однако отличной от той, которая предполагалась для корректно специфицированной модели. В частности, оценка может асимптотически сходиться к значению, отличному от истинного значения в генеральной совокупности, --- понятие, определённое в Разделе 4.7.5 как псевдоистинное значение.

Проблемы, поднятые здесь относительно состоятельности МНК, имеют отношение и к оценкам других моделей. Для последних состоятельность может требовать более сильных допущений, чем требуются для состоятельности МНК, и, следовательно, несостоятельность этих моделей более вероятна. 

\subsection{Несостоятельность МНК}

Наиболее серьёзным последствием ошибок спецификации является несостоятельная оценка параметров регрессии $\beta$. Из Раздела 4.4 мы знаем, что двумя ключевыми условиями, требующимися для состоятельности оценки МНК, являются следующие (1) данные порождены процессом $y = X\beta+u$ и (2) процесс, порождающий данные, таков, что $\plim N^{-1}X'u=0$. Тогда
\begin{equation}
\hat{\beta}_{\text{OLS}}=\beta + (N^{-1}X'X)^{-1}N^{-1}X'u \xrightarrow{p} \beta,
\end{equation}
где первое равенство следует из  $y = X\beta+u$ (см. (4.12)), а второе (сходимость) использует $\plim N^{-1}X'u=0$.

Оценка МНК, скорее всего, будет \textbf{несостоятельна}, если ошибки спецификации модели приводят либо к неправильной спецификации модели для $y$, нарушая условие (1), или ошибки коррелируют с регрессорами, так, что нарушается условие (2).

\subsection{Ошибки спецификации функциональной формы}

Линейная спецификация функции условного математического ожидания является не более чем приближением в $\mathcal{R}^K$ истинной неизвестной функции условного математического ожидания в пространстве параметров неопределённой размерности. Даже если выбраны корректные регрессоры, это ещё не означает, что условное математическое ожидание специфицировано корректно.

Предположим, что процесс, порождающий данные --- нелинейная регрессионная функция 
$$ y = g(x) + \upsilon, $$
где зависимость $g(x)$ от неизвестных параметров скрыта, и предположим, что $\E[u|X] = 0$. Модель линейной регрессии
$$ y = x'\beta +u$$
специфицирована некорректно. Вопрос состоит в том, можно ли дать оценке МНК какую-либо содержательную интерпретацию, даже если модель данных в действительности нелинейна.

Обычным способом интерпретации коэффициентов регрессии является истинная \textit{микро взаимосвязь}, в данном случае ---
$$\E[y_i|x_i] = g(x_i).$$
Тогда $\hat{\beta}_{\text{OLS}}$ не измеряет влияния на микроуровне изменений в $x_i$ на $\E[y_i|x_i]$, так как она не сходится к $\partial g(x_i)/\partial x_i$. Поэтому обычная интерпретация $\hat{\beta}_{\text{OLS}}$ невозможна.

Уайт (1980b) показал, что оценка МНК сходится к такому значению $\beta$, которое минимизирует среднеквадратическую ошибку предсказания
$$\E_x[(g(x)-x'\beta)^2].$$
Следовательно, оценка МНК является наилучшим линейным прогнозом для нелинейной функции регрессии, если использовать среднеквадратичную функцию потерь. Это полезное свойство уже было отмечено в Разделе 4.2.3, но оно мало добавляет к интерпретации $\hat{\beta}_{\text{OLS}}$.

В целом, если истинная функция регрессии нелинейна, МНК бесполезна для индивидуальных прогнозов. МНК по-прежнему можно использовать для прогнозирования агрегированных изменений, так как он даёт среднее по выборке изменений $\E [y|x]$ вследствие изменения $x$ (см. Стокер, 1982). Однако микроэконометрический анализ обычно стремится найти модели, содержательные на микроуровне.

Значительная часть этой книги представляет альтернативы линейной модели, которые с большей вероятностью будут специфицированы корректно. Например, Глава 14 о бинарных зависимых переменных представляет спецификации модели, которые гарантируют, что предсказанная вероятность будет лежать в пределах между 0 и 1. Также предпочтительными могут быть модели, которые опираются на минимальные допущения о распределении, так как они оставляют меньше места для ошибок спецификации.

\subsection{Эндогенность}
Эндогенность формально определена в Разделе 2.3. В широком смысле говорят, что регрессор эндогенный, если он коррелирует со случайным членом. Если какой-либо из регрессоров эндогенный, то в общем случае МНК-оценки всех параметров регрессии несостоятельны (кроме случаев, когда экзогенный регрессор некоррелирован с эндогенным регрессором). 

Яркие примеры эндогенности, которые в этой книге подробно рассмотрены в контексте как линейных, так и нелинейных моделей, включают смещение в одновременных уравнениях (Раздел 2.4), ошибку пропущенных переменных (Раздел 4.7.4), ошибку неслучайного отбора (Раздел 16.5), и ошибки измерения (Глава 26). Эндогенность часто встречается при использовании пространственных данных, и экономисты весьма обеспокоены этим затруднением.

Довольно общим подходом к учёту эндогенности является метод инструментальных переменных, представленный в Разделах 4.8 и 4.9 и в Разделах 6.4 и 6.5.  Этот метод, однако, не всегда можно применить, так как необходимые инструменты не всегда доступны.

Другие методы учёта эндогенности, перечисленные в Разделе 2.8, включают учет скрытых переменных, модель <<разность разностей>> (если доступны повторные пространственные или панельные данные --- см. Главу 21), фиксированные эффекты (если доступны панельные данные, а причинной эндогенности является пропущенная переменная, не меняющаяся во времени --- см. Раздел 21.6), и разрывную модель регрессии (см. Раздел 25.6).

\subsection{Пропущенные переменные}
\textit{Пропуск переменной} в модели линейной регрессии часто приводят в качестве первого примера несостоятельности оценки МНК во вводных курсах. Такой пропуск может быть следствием ошибочного исключения переменной, данные по которой доступны, или исключения переменной, которая напрямую ненаблюдаема. Например, пропуск способностей в регрессии заработной платы (или, что более типично, её логарифма) на образование обычно объясняется недоступностью полноценной меры способностей.

Пусть процесс, порождающие данные, имеет вид
\begin{equation}
y = x'\beta + z\alpha + \upsilon,
\end{equation}
где $x$ и $z$ --- регрессоры, $z$, для простоты, скаляр, а $\upsilon$ --- случайный член, который мы полагаем некоррелированным с $x$ и $z$. Регрессия МНК $y$ на $x$ и $z$ оценит параметры $\beta$ и $\alpha$ состоятельно. 

Предположим, однако, что строится регрессия $y$  только на $x$, а $z$ опущен вследствие недоступности. Тогда слагаемое $z\alpha$ становится частью <<случайного>> члена. Оценивается модель
\begin{equation}
y = x'\beta + (z\alpha + \upsilon),
\end{equation}
где ошибкой является $(z\alpha + \upsilon)$. Как и раньше,$\upsilon$  некоррелирован с $x$, но если $z$ коррелирует с $x$, то ошибка $(z\alpha + \upsilon)$  также коррелирует с регрессором $x$. Таким образом, оценка $\beta$ с помощью МНК будет несостоятельна, если $z$ коррелирует с $x$.

В этом примере достаточно информации, чтобы определить направление смещения. Собрав все наблюдения в одну матрицу, получим процесс, порождающий данные $y = X'\beta +z\alpha + v$. Подставив это в $\hat{\beta}_{\text{OLS}} = (X'X)^{-1}X'y$, получим
$$
\hat{\beta}_{\text{OLS}} = \beta + (N^{-1}X'X)^{-1} (N^{-1}X'z)\alpha + (N^{-1}X'X)^{-1} (N^{-1}X'v).
$$
При обычной предпосылке о некоррелированности $X$ и $v$, последнее слагаемое сходится по вероятности к нулю. Однако $X$ коррелирован с $z$, и 
\begin{equation}
\plim \hat{\beta}_{\text{OLS}} = \beta +\delta \alpha,
\end{equation}
где 
$$
\delta = \plim [(N^{-1}X'X)^{-1} (N^{-1}X'z)]
$$
является пределом по вероятности оценки МНК регрессии пропущенного регрессора ($z$) на оставшиеся регрессоры ($X$).

Эта несостоятельность называется \textbf{смещением пропущенной переменной}, где общепринятая терминология называет последствия различных ошибок спецификации смещением, тогда как формально они приводят к несостоятельности. Несостоятельность существует, пока $\delta \neq 0$, то есть пока пропущенная переменная коррелирует со включёнными регрессорами. В общем случае несостоятельность может быть как положительной, так и отрицательной, и может даже приводить к смене знака коэффициента МНК.

Схожей ошибкой спецификации является \textbf{включение лишних регрессоров}. Например, можно построить регрессию $y$  на $x$ и $z$, тогда как истинная модель данных просто $y = x'\beta +u$. В этом случае несложно показать, что оценка МНК состоятельна, но имеет место потеря эффективности. 

Учёт смещения пропущенных переменных необходим, если оценкам параметров приписывается причинно-следственная интерпретация. Поскольку лишние регрессоры причиняют относительно мало вреда, но исключение нужных регрессоров может приводить к несостоятельности, микроэконометрические модели, оцениваемые на больших выборках, обычно включают много регрессоров. Если некоторые переменные по-прежнему опущены, необходимо использовать один из методов, приведённых в конце Раздела 4.7.3.

\subsection{Псевдоистинное значение}
В примере с пропущенными значениями оценка МНК подвергается \textit{искажению} в том смысле, что она оценивает не $\beta$, а вместо этого некую функцию $\beta$, $\delta$ и $\alpha$. 

Оценку МНК нельзя использовать в качестве оценки $\beta$, которая, например, измеряет эффект от экзогенного изменения регрессора $x$, такого, как образование, при неизменных остальных переменных, включая ненаблюдаемые способности.

Из (4.40), однако, следует, что $\hat{\beta}_{\text{OLS}}$ --- состоятельная оценка функции $(\beta +\delta \alpha)$ и имеет содержательную интерпретацию. Предел по вероятности $\hat{\beta}_{\text{OLS}}$, равный $\beta{*} = (\beta +\delta \alpha)$ называют \textbf{псевдоистинным значением} (см. формальное определение в Разделе 5.7.1), соответствующим $\hat{\beta}_{\text{OLS}}$

Более того, можно найти распределение $\hat{\beta}_{\text{OLS}}$, несиотря на то, что эта оценка несостоятельна относительно $\beta$. Оценка асимптотической ковариацонной матрицы для $\hat{\beta}_{\text{OLS}}$ измеряет дисперсию вокруг $(\beta +\delta \alpha)$ и рассчитывается с помощью обычной оценки, например, $s^2 (X'X)^{-1}$, если ошибка в (4.38) гомоскедастична.

\subsection{Неоднородность параметров}
До сих пор наш текст допускал различие регрессоров и случайных ошибок для разных индивидов, но требовал, чтобы параметры регрессии $\beta$ были одинаковыми для всех индивидов. 

Предположим вместо этого, что процесс, порождающий данные
\begin{equation}
y_i = x_i'\beta_i + u_i, 
\end{equation}
с индексом $i$ при параметрах. Это пример \textbf{неоднородности параметров}, где предельный эффект $\E[\partial y_i/\partial x_i| x_i]=\beta_i$ теперь может различаться у разных индивидов.

\textbf{Модель со случайными коэффициентами}, или \textbf{модель со случайными параметрами}, специфицирует $\beta_i$ как независимые и одинаково распределённые случайные величины, распределение которых не зависит от $x_i$. Обозначим за $\beta$ общее математическое ожидание $\beta_i$. Тогда модель можно переписать как 
$$
y_i = x_i'\beta + (u_i +x'i(\beta_i -\beta)),
$$
и сделанных предположений достаточно, чтобы регрессоры $x$ были некоррелированы с ошибкой $(u_i +x'i(\beta_i -\beta))$. Регрессия МНК $y$ на $x$, таким образом, сможет состоятельно оценить $\beta$, но заметим, что в этой модели ошибки гетероскедастичны, хотя $u_i$ гомоскедастичны.

Для панельных данных стандартом является модель случайных эффектов (см. Раздел 21.7), где свободный член может быть различным для разных индивидов, а коэффициенты наклона не случайны.

Для нелинейных моделей подобного результата не существует, и можно предпочесть модели со случайными параметрами, так как они делают возможной более богатую параметризацию. Модели с рандомными параметрами согласуются со существованием неоднородной реакции индивидов на изменения $x$. Ярким примером является логит со случайными параметрами в Разделе 15.7.

Более серьёзные затруднения могут возникнуть, когда параметры регрессии $\beta_i$ связаны с наблюдаемыми характеристиками индивидов. Примером является модель с фиксированными эффектами для панельных данных (см. Раздел 21.6), для которых оценка регрессии МНК $y$ на $x$ несостоятельна. В этом примере, но не во всех таких примерах, доступны альтернативные оценки для подмножества параметров регрессии, являющиеся состоятельными.

\section{Инструментальные переменные}

Основным затруднением, подчёркиваемым в микроэконометрике, является возможность несостоятельной оценки параметров вследствие эндогенности регресоров. В этом случае регрессия измеряет только степень ассоциации между переменными, а не силу и направление причинно-следственной связи, что требуется для анализа политик.

Оценка методом инструментальных переменных, тем не менее, предоставляет способ получения состоятельных оценок параметров. Этот метод, широко используемый в микроэконометрике и редко используемый где-либо ещё, концептуально сложен, и им часто злоупотребляют.

Мы приводим подробное изложение, в котором определяем инструментальные переменные и объясняем, как метод инструментальных переменных работает в простых задачах.

\subsection{Несостоятельность МНК}

Рассмотрим скалярную модель регрессии с зависимой переменной $y$ и единственным регрессором $x$. Целью регрессионного анализа является оценка функции условного математического ожидания $\E[y|x]$. Линейная модель условного среднего (без свободного члена ради удобства обозначений) определяет:
\begin{equation}
\E[y|x] = \beta x.
\end{equation}

Эта модель без свободного члена может заменить собой модель со свободным членом, если зависимая и объясняющая переменные выражены в отклонениях от своих средних значений. Интерес представляет получение состоятельной оценки $\beta$, которая отражает изменение в условном математическом ожидании вследствие \textit{экзогенного} изменения $x$. Например, интерес может представлять изменение доходов, вызванное увеличением образования по экзогенным причинам, таким, как увеличение минимального возраста, в котором ученики могут закончить школу, которые не являются выбором индивида.

Модель регрессии МНК специфицирует 
\begin{equation}
y = \beta x +u,
\end{equation}
где $u$ является случайной ошибкой. Регрессия $y$ на $x$ даёт $\hat{\beta}$, МНК оценку $\beta$. 

В стандартных результатах о регрессии делается предположение, что регрессоры некоррелированы со случайным членом в модели (4.4.3). Тогда всё воздействие $x$ на $y$ является прямым эффектом через слагаемое  $\beta x$. У нас возникает следующая диаграмма направления воздействия:

\vspace{4cm}
\begin{figure}
\end{figure}

где между $x$ и $y$ нет взаимосвязи. Таким образом, $x$ и $u$ являются независимыми факторами, влияющими на $y$.

Однако в некоторых ситуациях может существовать ассоциация между регрессором и случайным членом. Например,  рассмотрим регрессию логарифма заработной платы $y$ на годы образования $x$. Ошибка $u$ включает все факторы, кроме образования, которые определяют заработную плату, такие, как способности индивида. Рассмотрим индивида, имеющего высокий уровень $u$ в результате выдающихся (ненаблюдаемых) способностей. Это приведёт к увеличению заработной платы, поскольку $y = \beta x +u$, но это также может привести к увеличению уровня $x$, так как образование у тех, кто обладает высокими способностями, может длиться дольше. Более подходящей диаграммой причинно-следственных связей  тогда будет следующая:

\vspace{4cm}
\begin{figure}
\end{figure} 
где теперь есть связь между $x$ и $u$.

Каковы последствия этой корреляции между  $x$ и $u$? Теперь высокий уровень $x$ связан c $y$ двумя путями. Из (4.4.3), есть как прямой эффект через $\beta x$, так и косвенный эффект через $u$, влияющий на $x$, который, в свою очередь, влияет на $y$. Целью регрессии является оценка только первого эффекта, чтобы получить оценку $\beta$. Вместо этого оценка МНК совместит оба эффекта, оценив $\hat{\beta}>\beta$ в данном конкретном примере, где оба эффекта положительны. Используя математический анализ, для выражения $y = \beta x +u(x)$ можно получить полную производную:
\begin{equation}
\frac{dy}{dx} = \beta + \frac{du}{dx}.
\end{equation}
Из данных можно извлечь информацию о $\frac{dy}{dx}$, и потому МНК измеряет совокупный эффект $\beta + \frac{du}{dx}$, а не $\beta$ саму по себе. Таким образом, оценка МНК является смещённой и несостоятельной оценкой $\beta$, если между $x$ и $u$ существует взаимосвязь.

Более формальное рассмотрение модели линейной регрессии с $K$ регрессорами приводит к тем же выводам. Согласно Разделу 4.7.1, необходимым условием состоятельности МНК является $\plim N^{-1}X'u=0$. Для состоятельности требуется, чтобы регрессоры были асимптотически некоррелированы с ошибками. Из (4.37) величина несостоятельности равна $(X'X)^{-1}X'u$, коэффициент МНК из регрессии $u$ на $X$. Это просто оценка МНК величины $\frac{du}{dx}$, что подтверждает наш интуитивный результат в (4.44).

\subsection{Инструментальные переменные}

Несостоятельность МНК обусловлена эндогенностью $x$, в том смысле, что изменения $x$ связаны не только с изменениями $y$, но и с изменениями в ошибке $u$. То, что нам необходимо, это метод порождающий исключительно экзогенные изменения в $x$. Очевидным способом является рандомизированный эксперимент, однако в большей части экономических приложений такие эксперименты чересчур дороги или вообще невозможны.
\begin{center}
Определение инструмента
\end{center}
Чистый экспериментальный подход всё же возможен при использовании данных, полученных из наблюдений, если существует \textbf{инструмент} $z$, обладающий свойством, что изменения $z$ связаны с изменениями $x$, но не приводят напрямую к изменениям $y$ (а только косвенно, через изменение $x$). Это приводит к следующей диаграмме:

\vspace{4cm}
\begin{figure}
\end{figure}
где вводится переменная $z$, причинно связанная с $x$, но не с $u$. По-прежнему, $z$ и $y$  коррелируют, однако единственным источником этой корреляции является непрямой путь через корреляцию $z$ с $x$, который, в свою очередь, коррелирует с $y$. Более прямой путь, где $z$ является регрессором в модели для $y$, исключён.

Более формально, $z$ называют \textbf{инструментом} или \textbf{инструментальной переменной}   для регрессора $x$ в скалярной регрессионной модели $y = \beta x +u $, если (1) $z$ не коррелирует с ошибкой $u$ и (2) $z$ коррелирует с регрессором $x$.

Первое предположение исключает возможность того, что $z$ сам является регрессором в модели для $y$, так как если бы $y$ зависел и от $x$, и от $z$, но регрессия была бы построена только на $x$, то $z$ был бы включён в ошибку и потому коррелировал бы с ней.  Второе предположение требует, чтобы между инструментом и переменной, к которой он применяется, существовала связь
\begin{center}
Примеры инструментов
\end{center}
Во многих микроэконометрических приложениях сложно найти обоснованные инструменты. Здесь мы приводим два примера таковых.

Представим, что мы хотим измерить реакцию рыночного спроса на экзогенные изменения в рыночной цене. Величина спроса, очевидно, зависит от цены, однако цены не заданы экзогенно, так как они частично определяются рыночным спросом. Приемлемый инструмент для цены должен быть переменной, которая коррелирует с ценой, но напрямую не влияет на величину спроса. Очевидными кандидатами являются переменные, влияющие на рыночное предложение, так как они также влияют на цены, но не являются прямыми факторами спроса. Примером может служить мера благоприятности погодных условий, если моделируется сельскохозяйственный продукт. Такой выбор инструмента безукоризнен, если погодные условия не влияют напрямую на спрос, и пользуется поддержкой формальной экономической модели спроса и предложения.

Далее представим, что мы хотим измерить отдачу от экзогенных изменений в количестве образования. В большей части данных, полученных в результате наблюдения, отсутствует мера индивидуальных способностей. Поэтому регрессия зарплаты на образование имеет ошибку, в которую включены ненаблюдаемые способности, и потому коррелирующую с регрессором --- образованием. Нам нужен инструмент $z$, который коррелирует с образованием, не коррелирует со способностями и с ошибкой вообще, а значит, не может напрямую влиять на зарплату.

Одним из популярных кандидатов на роль $z$ является близость колледжа или университета (Кард, 1995). Она, очевидно, удовлетворяет условию 2, поскольку люди, которые живут на большом расстоянии от муниципального колледжа или государственного университета, с меньшей вероятностью будут их посещать. Скорее всего, она удовлетворяет и условию 1, хотя можно возразить, что люди, живущие далеко от образовательных учреждений, вероятно, находятся на рынках труда с низкими зарплатами. Поэтому необходимо оценивать множественную регрессию $y$ включающую дополнительные переменные, такие, как индикатор столичного региона.

Вторым кандидатом на роль инструмента является месяц рождения (Ангрист и Крюгер, 1991). Он явно удовлетворяет условию 1, так как нет причин полагать, что заработная плата напрямую может зависеть от месяца рождения, если в регрессию включён возраст в годах. Удивительно, но условие 2 также может быть удовлетворено, так как в США месяц рождения определяет возраст поступления в школу, который, в свою очередь, может влиять на годы обучения, так как законы часто определяют минимальный возраст окончания школы. Баунд, Джагер и Бейкер (1995) предоставляют критический анализ этого инструмента.

Последствия выбора плохих инструментов подробно рассмотрены в Разделе 4.9.

\subsection{Оценка методом инструментальных переменных}

Для регрессии со скалярным регрессором $x$ и скалярным инструментом $z$ \textbf{оценка метода инструментальных переменных} (instrumental variable, IV) определена как
\begin{equation}
\hat{\beta}_{IV} = (z'x)^{-1}z'y,
\end{equation}
где, в случае скалярных регрессоров, $x$, $y$ и $z$ --- векторы размером $N \times 1$. Этот метод даёт состоятельную оценку коэффициента наклона $\beta$ в линейной модели $y = \beta x +u$, если $z$ коррелирует с $x$ и не коррелирует с ошибкой регрессии.

Есть несколько способов вывода (4.45). Мы приводим интуитивный вывод, отличающийся от стандартного, представленного в Разделе 6.2.5.

Вернёмся к примеру с образованием и доходом. Предположим, что увеличение инструмента $z$ на единицу соответствует  увеличению длительности образования в среднем на 0.2 года и с возрастанию годового дохода на \$500. Это увеличение дохода является следствием косвенного воздействия $z$ через увеличение образование, которое привело к росту дохода. Тогда можно считать, что увеличение образования на 0.2 года связано с возрастанием дохода на \$500, то есть увеличение образования на один год соответствует росту дохода на $\$500/0.2 = \$2500$. Говоря языком математики, мы оценили изменения $\frac{dx}{dz}$ и $\frac{dy}{dz}$ и вычислили причинно-следственную оценку как
\begin{equation}
\hat{\beta}_{IV} = \frac{dy/dz}{dx/dz}.
\end{equation}
Этот подход к идентификации причинно-следственного параметра $\beta$ описан в работе Хекмана (2000, стр. 58), см. также пример в Разделе 2.4.2.

Остаётся только состоятельно оценить $\frac{dx}{dz}$ и $\frac{dy}{dz}$. Очевидным способом оценки $\frac{dy}{dz}$ является регрессия МНК $y$ на $z$ с оценкой коэффициента наклона $(z'z)^{-1}z'y$. Аналогично, $\frac{dx}{dz}$ можно оценить с помощью регрессии МНК $x$ на $z$, получив оценку $(z'z)^{-1}z'x$. Тогда
\begin{equation}
\hat{\beta}_{IV} =\frac{(z'z)^{-1}z'y}{(z'z)^{-1}z'x} = (z'x)^{-1}z'y,
\end{equation}

\subsection{Оценка Вальда}
Простым заметным примером метода инструментальных переменных является такая оценка, когда переменная $z$ является \textbf{бинарным инструментом}. Обозначим средние значения $x$ и $y$ как, соответственно, $\bar{x}_1$ и $\bar{y}_1$ для подвыборки, где $z=1$ и как $\bar{x}_0$ и $\bar{y}_0$, для подвыборки, где $z=0$. Тогда $\Delta y /\Delta z = (\bar{y}_1-\bar{y}_0)$, а $\Delta x /\Delta z = (\bar{x}_1-\bar{x}_0)$, и из (4.46) следует 
\begin{equation}
\hat{\beta}_{Wald} = \frac{(\bar{y}_1-\bar{y}_0)}{(\bar{x}_1-\bar{x}_0)}.
\end{equation}
Эта оценка называется \textbf{оценкой Вальда}, по работе Вальда (1940), или \textbf{группировочной оценкой}.

Оценку Вальда также можно вывести из формулы (4.45). В модели без свободного члена переменные выражены в отклонениях от своих средних значений, а потому $z'y = \sum_i (z_i - \bar{z})(y_i - \bar{y})$. Для бинарного $z$ отсюда следует, что $z'y = N_1 (\bar{y}_1-\bar{y}) = N_1 N_0 (\bar{y}_1-\bar{y}_0)/N$, где $N_0$ и $N_1$ --- число наблюдений, для которых, соответственно, $z=0$ и $z=1$.  Мы использовали равенство $\bar{y}_1 - \bar{y} = (N_0 \bar{y}_1 + N_1 \bar{y}_1)/N - (N_0 \bar{y}_0 + N_1 \bar{y}_1)/N = N_0 (\bar{y}_1-\bar{y}_0)/N$. Аналогично, $z'x = N_0 N_1 (\bar{x}_1-\bar{x}_0)/N$. Объединив эти результаты с (4.45), получаем (4.48).

Для примера с образованием и доходом предполагается, что мы определили две группы, принадлежность к которым напрямую не влияет на доходы, но оказывает влияние на образование и потому косвенно оказывает влияние на доходы. Тогда оценка метода инструментальных переменных равна отношению разностей средних доходов х и среднего уровня образования в двух группах.

\subsection{Анализ выборочных ковариации и корреляции}
Оценку метода инструментальных переменных также можно интерпретировать через ковариации или корреляции. 

Для выборочной ковариации из (4.45) напрямую следует, что 
\begin{equation}
\hat{\beta}_{IV} = \frac{\Cov[z,y]}{\Cov[z,x]},
\end{equation}
где под $\Cov[\cdot,\cdot]$ мы понимаем выборочную ковариацию.

Что касается корреляций, заметим, что оценку МНК в модели (4.43) можно выразить как $\hat{\beta}_{\text{OLS}} = r_{xy}\sqrt{y'y}/\sqrt{x'x}$, где $r_{xy} = x'y/\sqrt{(x'x)(y'y)}$ --- \textbf{выборочная корреляция} между $x$ и $y$.  Отсюда следует интерпретация коэффициента МНК как показывающего, что изменение $x$ на одно стандартное отклонение соответствует изменению $y$ на $r_{xy}$ стандартных отклонений. Проблема заключается в том, что $r_{xy}$ <<загрязнена>> корреляцией между $x$ и $u$. Альтернативный подход измеряет корреляцию между $y$ и $x$ косвенно, через корреляцию между $z$ и $y$, делённую на корреляцию между $z$ и $x$.  Тогда
\begin{equation}
\hat{\beta}_{IV} = \frac{r_{zy}\sqrt{(y'y)}}{r_{zx}\sqrt{(x'x)}},
\end{equation}
и можно показать, что эта оценка совпадает с $\hat{\beta}_{IV}$ в (4.45).

\subsection{Оценка множественной регрессии методом инструментальных переменных}

Теперь рассмотрим модель множественной регрессии с типичным наблюдением 
$$ y =x'\beta +u,$$
с $K$ регрессорами, так, что $x$ и $\beta$ --- векторы $K \times 1$.

Допустим, что существует вектор \textbf{инструментов} $z$ размером $r \times 1$, где $r\geq K$, удовлетворяющий условиям:
\begin{small}
\begin{enumerate}
\item $z$ не коррелирует с ошибкой $u$.
\item $z$ коррелирует с вектором регрессоров $x$.
\item $z$ скорее сильно коррелирует, чем слабо коррелирует с вектором регрессоров $x$.
\end{enumerate}
\end{small}
Первые два условия являются необходимыми для состоятельности и были рассмотрены ранее для скалярного случая. Третье свойство, определённое в Разделе 4.9.1, является усилением второго, призванным обеспечить хорошее поведение оценки IV в конечных выборках.

В случае множественной регрессии у $x$ и $z$ могут быть общие компоненты. Некоторые компоненты $x$, называемые \textbf{экзогенными регрессорами}, могут не коррелировать с $u$. Эти компоненты, очевидно, годятся на роль инструментов, так как они удовлетворяют условиям 1 и 2. Другие компоненты $x$, называемые \textbf{эндогенными регрессорами}, могут коррелировать с $u$. Эти компоненты делают МНК несостоятельным, и, очевидно, не могут быть инструментами, так как не удовлетворяют условию 1. Разделим $x$ на $x = [x_1' x_2']'$, где $x_1$ содержит эндогенные регрессоры, а $x_2$ --- экзогенные. Тогда годным инструментом будет $z = [z_1' x_2']'$, где $x_2$ может быть инструментом для самого себя, но нам требуется хотя бы столько инструментов $z_1$, сколько эндогенных переменных содержится в $x_1$. 
\begin{center}
Идентификация
\end{center}
Идентификация в модели с одновременными уравнениями была представлена в Разделе 2.5. Здесь у нас есть только одно уравнение. \textbf{Условие порядка} требует, чтобы число инструментов было не меньше числа эндогенных регрессоров, так, что $r \geq K$. Модель называется \textbf{точно идентифицированной}, если $r=K$, и \textbf{сверхидентифицированной}, если $r>K$.

Во многих приложениях множественной регрессии эндогенный регрессор только один. Например, регрессия дохода на образование  может включать множество других регрессоров, таких, как возраст, географическое расположение и семейный статус. Интерес предствляет коэффициент при образовании, но это эндогенная переменная, которая, скорее всего, коррелирует с ошибкой, поскольку способности ненаблюдаемы. Возможные кандидаты на роль требуемого единственного инструмента для образования были приведены в Разделе 4.8.2.

Если инструмент не удовлетворяет первому условию, это \textbf{негодный инструмент}. Если инструмент не удовлетворяет второму условию, его называют \textbf{нерелевантным инструментом}, и модель может быть \textbf{не идентифицирована}, если релевантных инструментов слишком мало.  Третье условие не выполняется, если корреляция инструмента с переменной, к которой он применяется, очень низкая. В этом случае модель называется \textbf{слабо идентифицируемой}, а инструмент называется \textbf{слабым инструментом}.
\begin{center}
Оценка методом инструментальных переменных
\end{center}
Если модель точно определена, то есть $r=K$, \textbf{оценка методом инструментальных переменных} является очевидным матричным обобщением (4.45):
\begin{equation}
\hat{\beta}_{IV} = (Z'X)^{-1}Z'y, 
\end{equation}
где $Z$ --- матрица размером $N\times K$ c $i$-ой строкой, равной $z_i'$. Подставив модель $y = X\beta+u$ в (4.51), получим

\[
\begin{array}{rcl}
\hat{\beta}_{IV} & = & (Z'X)^{-1} Z'[X\beta +u] \\
& = & \beta + (Z'X)^{-1} Z'u \\
& = & \beta + (N^{-1}Z'X)^{-1} N^{-1}Z'u.
\end{array}
\]

Из этого незамедлительно следует, что оценка методом инструментальных переменных состоятельна, если 
$$
\plim N^{-1}Z'u = 0
$$
и
$$
\plim N^{-1}Z'X \neq 0.
$$
По существу, это условия 1 и 2, говорящие, что $z$ не должен коррелировать с $u$ и должен коррелировать с $x$. Чтобы гарантировать существование матрицы, обратной к 	$N^{-1}Z'X$, предполагается, что $Z'X$ обладает полным рангом $K$ --- более сильное условие, чем условие порядка $r=K$.

C гетероскедастичными ошибками оценка IV асимптотически нормальна с математическим ожиданием $\beta$ и ковариационной матрицей, которую можно оценить как
\begin{equation}
\hat{\V}[\hat{\beta}_{IV}] = (Z'X)^{-1} Z'\hat{\Omega}Z(Z'X)^{-1},
\end{equation}
где $\hat{\Omega} = \mathrm{Diag}[\hat{u}_i^2]$. Этот результат получен способом, похожим на тот, который использовался для МНК в Разделе 4.4.4.

Оценка методом инструментальных переменных, хоть и состоятельная, приводит к потере эффективности, которая на практике может быть весьма значительной. Интуитивно, IV не будет хорошо работать, если корреляция инструмента $z$ с регрессором $x$ низкая (см. Раздел 4.9.3).

\subsection{Двухшаговый МНК}

Оценка методом инструментальных переменных в (4.51) требует, чтобы число инструментов было равно числу регрессоров. Для сверхидентифицированных моделей оценку IV можно использовать, если исключить часть инструментов так, чтобы модель стала точно идентифицированной. Однако такое исключение инструментов может привести к потере асимптотической эффективности.

Вместо этого общепринятой процедурой является \textbf{оценка двухшагового МНК} (two-stage least squares, 2SLS):
\begin{equation}
\hat{\beta}_{2SLS} = [X'Z(Z'Z)^{-1}Z'X]^{-1}[X'Z(Z'Z)^{-1}Z'y],
\end{equation}
представленная и обоснованная в Разделе 6.4.

Оценка 2SLS является оценкой методом инструментальных переменных. В точно идентифицированной модели она упрощается до оценки IV, приведённой в (4.51), с инструментами $Z$. В сверхидентифицированной модели оценка 2SLS равняется оценке IV, если последняя в качестве инструментов использует $\hat{X}=Z(Z'Z)^{-1}Z'X$, т.е. значения $x$, предсказанные из регрессии $x$ на $z$.

Оценка двухшагового МНК берёт своё название из того результата, что она может быть получена с помощью двух последовательных регрессий МНК: МНК-регрессии $x$ на $z$, чтобы получить $\hat{x}$, а затем МНК-регрессии $y$ на $\hat{x}$, чтобы получить $\hat{\beta}_{2SLS}$. Эта интерпретация не всегда обобщается на нелинейные модели; см. Раздел 6.5.6

Оценку 2SLS можно записать более компактно как 
\begin{equation}
\hat{\beta}_{2SLS} = [X'P_ZX]^{-1}[X'P_Zy],
\end{equation}
где
$$
P_Z = Z(Z'Z)^{-1}Z',
$$
идемпотентная матрица, удовлетворяющая $P_Z = P_Z'$, $P_Z P_Z' = P_Z$ и $P_Z Z = Z$. Можно показать, что оценка 2SLS асимптотически нормально распределена с оценкой асимптотической дисперсии
\begin{equation}
\hat{\mathrm{V}}[\hat{\beta}_{2SLS}] =N[X'P_ZX]^{-1}[X'Z(Z'Z)^{-1}\hat{S}(Z'Z)^{-1}Z'X][X'P_ZX]^{-1},
\end{equation}
где в обычном случае с гетероскедастичными ошибками $\hat{S} = N^{-1}\sum_i\hat{u}_i^2 z_i z_i'$, а $\hat{u}_i = y_i - x_i'\hat{\beta}_{2SLS}$. Общепринято использовать поправку для малых выборок, деля на $N-K$ вместо $N$ в формуле для $\hat{S}$.

В особом случае, когда ошибки гомоскедастичны, происходит упрощение до $\hat{\mathrm{V}}[\hat{\beta}_{2SLS}] = s^2 [X'P_ZX]^{-1}$. Последний результат даётся во многих вводных курсах, однако более общая формула (4.55) предпочтительна, так как в современном подходе принято  относиться к ошибкам как к потенциально гетероскедастичным.

Для сверхидентифицированных моделей с гетероскедастичными ошибками оценка, которую Уайт (1982) называет \textbf{двухшаговой оценкой методом инструментальных переменных} более эффективна, чем 2SLS. Более того, некоторые широко используемые тесты на спецификацию модели требуют применения именно этой оценки, а не 2SLS. Подробнее см. в Разделе 6.4.2.

\subsection{Пример оценки методом инструментальных переменных}

В качестве примера оценки IV рассмотрим оценивание коэффициента наклона при $x$ в модели	
$$ y = 0+0.5x_u $$,
$$x = 0+z+v $$,
где $z \sim \mathcal{N} [2,1]$, а $(u,v)$ распределены совместно нормально со средним 0, дисперсией 1 и ковариацией 0.8. 

Регрессия МНК $y$ на $x$ даёт несостоятельную оценку, так как $x$ коррелирует с $u$, поскольку по построению $x$ коррелирует с $z$, который, в свою очередь, коррелирует с  $u$. Оценка IV даёт состоятельные результаты. Переменная $z$ является годным инструментом, так как по построению она не коррелирует с $u$, но коррелирует с $x$. Трансформации $z$, такие, как $z^3$, также являются годными инструментами.

Различные оценки и соответствующие им стандартные ошибки из сгенерированной выборки размером 10000 даны в Таблице 4.4. Нас интересует коэффициент наклона. 

\begin{table}[h]
\caption{\label{tab:iv}Пример метода инструментальных переменных}
\begin{minipage}{\textwidth}
\begin{tabular}[t]{lcccc}
\hline
\hline
& \bf{МНК}\footnote{Сгенерированные данные для выборки размером 10000. МНК несостоятелен, но другие оценки состоятельны. Указаны робастные стандартные ошибки, хотя в них нет необходимости, т.к. ошибки гомоскедастичны. Стандартные ошибки 2SLS некорректны. Процесс, порождающий данные, описан в тексте.} & \bf{IV} & \bf{2SLS} & \bf{IV ($z^3$)}  \\
\hline
Константа & -0.804 & -0.017 & -0.017 & -0.014 \\
 & (-0.014) & (-0.022) & (-0.032) & (-0.025) \\
$x$ & 0.902 & 0.510 & 0.510 & 0.509 \\
 & (-0.006) & (-0.010) & (-0.014) & (-0.012) \\
$R^2$ & 0.709 & 0.576 & 0.576 & 0.574 \\
\hline
\hline
\end{tabular}
\end{minipage}
\end{table}

Оценка МНК несостоятельна, и её коэффициент наклона, равный 0.902, более, чем на 50 стандартных ошибок, удалён от истинного значения 0.5. Остальные оценки состоятельны и все находятся в пределах двух стандартных ошибок от 0.5.

Существует несколько способов вычисления оценки МНК. Коэффициент наклона из МНК-регрессии $y$ на $z$ равен 0.5168, а из МНК-регрессии $x$ на $z$ --- 1.0124, и оценка IV равна 0.5168/1.0124 = 0.510, по (4.47). На практике оценка IV вычисляется напрямую, с использованием (4.45) или (4.51), где $z$ используется как инструмент для $x$, а стандартные ошибки рассчитываются по (4.52). Оценка 2SLS (см.(4.54)) может быть вычислена с помощью регрессии $y$ на $\hat{x}$, где $\hat{x}$ --- прогноз из регрессии $x$ на $z$. Оценки 2SLS в точности равны оценкам МНК в этой точно идентифицированной модели, хотя стандартные ошибки из регрессии $y$ на $x$ некорректны, как будет объяснено в Разделе 6.4.5.

Последний столбец использует $z^3$, а не $z$ в качестве инструмента для $x$. Эта альтернативная оценка IV состоятельна, так как $z^3$ не коррелирует с $u$ и коррелирует с $x$. Однако она менее эффективна для конкретно этой модели данных, и стандартная ошибка коэффициента наклона увеличивается с 0.010 до 0.012. 

Оценивание методом инструментальных переменных приводит к потере эффективности по сравнению с МНК, см (4.61) для общего результата в случае одного регрессора и одного инструмента. Здесь $r^2_{x,z}=0.510$ (не указан в Таблице 4.4) высок, так что потеря невелика и 	стандартная ошибка коэффициента наклона увеличивается примерно с 0.006 до 0.010. На практике потеря эффективности может быть гораздо больше, чем здесь.


\section{Инструментальные переменные на практике}

Важные практические вопросы включают определение, необходимо ли использование инструментальных переменных, и если необходимо, какие инструменты годны. Подходящие тесты на спецификацию представлены в Разделе 8.4. К сожалению, применимость этих тестов ограничена. Они требуют, чтобы в точно идентифицированной модели инструменты были годны, и тестируют только сверхидентифицирующие ограничения.

Хотя с годными инструментами оценки IV состоятельны, как подробно описано ниже, оценки IV могут быть намного менее эффективны, чем оценки МНК и могут иметь распределение в конечных выборках, которое для стандартных размеров выборки существенно отличается от асимптотического. Эти проблемы только ухудшаются,  если инструменты слабо коррелируют с переменными, к которым они применяются. Одна из причин, по которой слабые инструменты могут возникнуть, это применение большего числа инструментов, чем необходимо. Эту проблему легко исправить, отказавшись от части инструментов (см. также работу Доналда и Ньюи, 2001). Более фундаментальные проблемы возникают, когда даже при минимальном количестве инструментов один или более из них слабые. 

Этот раздел посвящён проблемам, связанным со слабыми инструментами.

\subsection{Слабые инструменты}

Не существует единого определения слабого инструмента. Многие авторы используют следующие признаки \textbf{слабого инструмента}, представленные здесь в порядке возрастания сложности модели:
\begin{small}
\begin{itemize}
\item Скалярный регрессор $x$ и скалярный инструмент $z$: Слабый инструмент --- такой, что $r^2{x,z}$ мал.
\item Скалярный регрессор $x$ и вектор инструментов $z$: Инструменты слабые, если $R^2$ из регрессии $x$ на $z$, обозначаемый как $R^2_{x,z}$, мал, или $F$-статистика в тесте на значимость регрессии в целом мала.
\item Несколько регрессоров $x$, из которых эндогенный только один: слабый инструмент --- тот, для которого частный $R^2$ низок или частная $F$-статистика низка, где эти частные статистики определены в конце раздела 4.9.1. 
\item Несколько регрессоров $x$ и несколько инструментов $z$: Существуют различные меры.
\end{itemize}
\end{small}

\begin{center}
Меры $R^2$
\end{center}
Рассмотрим модель с одним уравнением
\begin{equation}
y = \beta_1 x_1 + x_2'\beta_2 +u,
\end{equation}
где только один регрессор $x_1$ эндогенный, а остальные регрессоры, собранные в вектор $x_2$, экзогенны. Предположим, что вектор инструментов $z$ включает экзогенные инструменты $x_2$ и хотя бы один инструмент, не входящий в $x_2$.

Одной из возможных мер $R^2$ является обычный $R^2$ из регрессии $x_1$ на $z$. Однако он может быть высоким из-за того, что $x_1$ коррелирует с $x_2$, тогда как интуитивно нам нужна высокая корреляция $x_1$ с инструментами, отличными от $x_2$.

По этой причине Баунд, Джагер и Бейкер (1995) предложили использовать \textbf{частный $R^2$}, обозначаемый как $R^2_p$, который очищен от влияния $x_2$. $R^2_p$ получается как $R^2$ из регрессии  
\begin{equation}
(x_1-\tilde{x}_1)= (z-\widetilde{z})'\gamma + \upsilon,
\end{equation}
где $\tilde{x}_1$и  $\tilde{z}$ --- предсказанные значения из регрессий $x_1$ и $z$ на $x_2$. В точно идентифицированном случае $z-\tilde{z}$ упростится до $z_1-\tilde{z}_1$, где $z_1$ --- единственный инструмент, кроме $x_2$, а $\tilde{z}_1$ --- предсказанное значение из регрессии $z_1$ на $x_2$.

Нередко $R^2_p$ получается значительно ниже, чем $R^2_{x,z}$. Формула для $R^2_p$ упрощается до $r^2_{x,z}$, где есть только один регрессор и он эндогенный. Она далее упрощается до $\mathrm{Cor}[x,z]$, где и инструмент только один.

Если число эндогенных переменных больше, чем одна, анализ менее прямолинеен, и было предложено несколько различных обобщений $R^2_p$.

Рассмотрим модель с более чем одной эндогенной переменной и сосредоточимся на оценке коэффициента при первой эндогенной переменной. Тогда в (4.56) $x_1$ эндогенный и вдобавок часть переменных в $x_2$ также эндогенны. Различные альтернативные меры заменяют правую часть (4.57) на остаток, учитывающий присутствие других эндогенных регрессоров. Шеа (1997) предложил частный $R^2$, скажем, $R_p^{*2}$, который вычисляется как квадрат выборочной корреляции между $(x_1-\tilde{x}_1)$ и $(\hat{x}_1-\tilde{\hat{x}}_1)$. Здесь $(x_1-\tilde{x}_1)$ снова обозначает остатки из регрессии $x_1$ на $x_2$, тогда как  $(\hat{x}_1-\tilde{\hat{x}}_1)$ --- остатки из регрессии $\hat{x}_1$ (прогнозного значения из регрессии $x_1$ на $z$) на $\hat{x}_2$ (предсказанное значение из регрессии $x_2$ на $z$). Поскитт и Скилс (2002) предложили альтернативный частный $R^2$, который, как и $R_p^{*2}$ Шеа, упрощается до $R^2_p$, если эндогенный регрессор только один. Холл, Рудебуш и Вилкокс (1996) вместо этого предлагают канонические корреляции.

Эти меры для коэффициента при первой эндогенной переменной можно повторно применить для всех остальных эндогенных переменных.  Поскитт и Скилс (2002) вдобавок рассматривают меру типа $R^2$, которая применяется одновременно ко всем эндогенным переменным и их инструментам.

Проблемы несостоятельности оценок и потери точности увеличиваются по мере того, как меры типа $R^2$ падают, как это показано в Разделах 4.9.2 и 4.9.3, в особенности в (4.60) и (4.62).
\begin{center}
Частные $F$-статистики
\end{center}
В случае с плохим поведением в конечных выборках, рассмотренном в Разделе 4.9.4, часто используют другую меру, $F$-статистику, тестирующую, что все коэффициенты в регрессии эндогенного регрессора на инструменты равны нулю. 

В случае с единственным регрессором, который является эндогенным, мы используем обычную общую $F$-статистику для теста, что $\pi=0$ в регрессии $x=z'\pi+\upsilon$ эндогенного регрессора на инструменты. Такая $F$-статистика является функцией от $R^2_{x,z}$.

Обычно в модели также появляются экзогенные регрессоры, и в модели (4.56) c одним эндогенным регрессором $x_1$ мы используем $F$-статистику, тестирующую, что $\pi_1=0$ в регрессии 
\begin{equation}
x = z_1'\pi_1 + x_2'\pi_2+\upsilon,
\end{equation}
где $z_1$ --- инструменты, не являющиеся экзогенными регрессорами, а $x_2$ --- экзогенные регрессоры. Это регрессия первого шага в интерпретации метода инструментальных переменных двухшагового  МНК.

Эту статистику используют как сигнал о потенциальном смещении конечной выборки в оценке IV.  В Разделе 4.9.4 мы объясняем результаты Стайгер и Сток (1997), которые предлагают понимать её значения, меньшие 10, как проблемные, а значения меньше 5 как признак сильного смещения конечной выборки, и мы рассматриваем его расширение на случай с более чем одним эндогенным регрессором.

\subsection{Несостоятельность оценок метода инструментальных переменных}

Необходимым условием для состоятельности IV является условие 1 в Разделе 4.8.6 о том, что инструмент должен не коррелировать с ошибкой. В случае с точно идентифицированной моделью протестировать это невозможно. В сверхидентифицированном случае можно провести тест на сверхидентифицирующие ограничения (см. Раздел 6.4.3). Отклонение нулевой гипотезы может свидетельствовать либо об эндогенности инструментов, либо о неверно специфицированной модели. Следовательно, условие 1 сложно проверить напрямую, и определение, является ли инструмент экзогенным, зачастую остаётся субъективным решением, хоть и опирающимся на экономическую теорию.

Всегда можно создать экзогенные инструменты с помощью \textbf{ограничений функциональной формы}. Для примера рассмотрим модель с двумя регрессорами $y = \beta_1 x_1 +beta_2 x_2 +u$, в которой $x_1$ не коррелирует с $u$, а $x_2$ коррелирует с $u$. Заметим, что во всём этом разделе переменные рассматриваются как отклонения от своих средних значений, и свободный член можно исключить без потери общности. Тогда МНК несостоятелен, т.к. $x_2$ является эндогенным. Очевидно хорошим инструментом для $x_2$ является $x_1^2$, так как $x_1^2$, скорее всего, не коррелирует с $u$, потому что с ним не коррелирует $x_1$. Однако годность этого инструмента требует ограничения на функциональную форму модели --- что $x_1$ включён в неё только линейно, а не квадратично. Но на практике линейную модель следует рассматривать только как приближение, и получение таких искусственных инструментов можно легко подвергнуть критике.

Более подходящим способом создания годных инструментов являются \textbf{исключающие ограничения}, которые не зависят в такой значительной степени от выбора функциональной формы. Некоторые примеры из практики были даны в Разделе 4.8.2.

Структурные модели, такие, как классическая модель линейных одновременных уравнений (см. Разделы 2.4 и 6.10.6), делают такие исключающие ограничения в явном виде. Даже тогда эти ограничения можно подвергнуть критике, как взятые с потолка, если только их не поддерживают доводы экономической теории.

В панельных данных может быть разумным предположение о том, что в интересующее нас уравнение могут входить только текущие значения переменных. Такое исключающее ограничение позволяет использовать в качестве инструментов лаговые значения переменных при допущении, что ошибки не испытывают серийной корреляции (см. Раздел 22.2.4). Аналогично, в моделях принятия решений при неопределённости (см. Раздел 6.2.7), лагированные переменные можно использовать в качестве инструментов, т.к. они являются частью информационного множества.

Не существует формального теста на экзогенность инструментов, который одновременно не тестировал бы гипотезу, является ли уравнение корректно специфицированным. Экзогенность инструментов неизбежно опирается на априорную информацию, такую, как экономическая или статистическая теория. В работе Бонда и др. по (1995, стр. 446-447) проверке годности инструментов, использованной Ангристом и Крюгером (1991), можно найти поучительный пример тонкостей, используемых при определении экзогенности инструментов.

Особенно важно, чтобы инструмент был экзогенным, если это слабый инструмент, так как в случае слабых инструментов даже вполне умеренная эндогенность может сделать оценки IV гораздо более несостоятельными, чем и так несостоятельные оценки МНК.

Для простоты рассмотрим линейное уравнение с одним регрессором и одним инструментом, т.е. $y = \beta x +u$. Тогда с помощью алгебры, которую мы оставим как упражнение, можно получить 
\begin{equation}
\frac{\plim \hat{\beta}_{IV}-\beta}{\plim \hat{\beta}_{IV}-\beta}
=\frac{\Cor[z,u]}{\Cor[x,u]}\times \frac{1}{\Cor[z,x]}.
\end{equation}
Таким образом, в случае негодного инструмента и низкой корреляции между инструментом и регрессором оценка IV может быть даже более несостоятельной, чем оценка МНК. К примеру, положим корреляцию между $z$ и $x$ равной 0.1, что не является необычным для пространственных данных. Тогда оценка метода инструментальных переменных становится более несостоятельной, чем оценка МНК, как только коэффициент корреляции между $z$ и $u$ превосходит всего лишь 0.1 помножить на корреляцию между $x$ и $u$. 

Результат (4.59) можно расширить на модель (4.56) с одним эндогенным и несколькими экзогенными регрессорами, независимыми и одинаково распределенными ошибками и инструментами, включающими экзогенные регрессоры. Тогда
\begin{equation}
\frac{\plim \hat{\beta}_{1, 2SLS}-\beta_1}{\plim \hat{\beta}_{1, OLS}-\beta_1}
=\frac{\Cor[\hat{x},u]}{\Cor[x,u]}\times \frac{1}{R_p^2},
\end{equation}
где $R_p^2$ определён после (4.56). Обобщение до случая с более чем одним эндогенным регрессором см. у Шеа (1997).

Эти результаты, подчёркнутые в работе Баунда и др. (1995), имеют серьёзные последствия для использования инструментальных переменных. Если инструменты слабые, то даже мягкая эндогенность регрессоров может приводить к тому, что оценки IV даже более несостоятельны, чем оценки МНК. Возможно, именно потому что этот вывод так печален, литература в основном умалчивала об этом аспекте слабых инструментов. Важным недавним исключением является работа Хана и Хаусмана (2003a).

В большей части работ предполагается, что условие 1 выполнено, и оценка метода инструментальных переменных состоятельна, и делается акцент на других затруднениях, связанных со слабыми инструментами.

\subsection{Низкая точность}

Хотя оценки методом инструментальных переменных могут быть состоятельными, когда несостоятелен МНК, они также приводят к потере точности. Интуитивно, из Раздела 4.8.2 следует, что инструмент $z$ является экзогенным воздействием, которое приводит к экзогенному сдвигу $x$, но делает это при значительным уровне шума. 

Потеря точности увеличивается и стандартные ошибки растут, если инструменты  более слабые. Это легко видеть в простейшем случае единственного регрессора и единственного инструмента с независимыми и одинаково распределенными ошибками. Тогда асимптотическая дисперсия равна
\begin{equation}
\begin{array}{rcl}
\V[\hat{\beta}_{IV}]& = & \sigma^2 (x'x)^{-1}z'z(x'x)^{-1} \\
 & = & [ \sigma^2 / (x'x)]/[(z'x)^2/(z'z)(x'x)] \\
 & = & \V [\hat{\beta}_{\text{OLS}}]/r^2_{xz}. \\
\end{array}
\end{equation}
Например, если квадрат выборочной корреляции между $x$ и $z$ равен 0.1, стандартные ошибки оценок IV в 10 раз больше, чем для МНК. Более того, дисперсия оценки метода инструментальных переменных больше дисперсии оценки МНК, кроме случая, когда $\Cor[x,z]=1$.

Результат (4.61) можно обобщить до модели (4.56) с одним эндогенным регрессором и несколькими экзогенными регрессорами, независимыми и одинаково распределенными ошибками и инструментами, включающими экзогенные регрессоры. Тогда
\begin{equation}
\mathrm{se}[\hat{\beta}_{1,2SLS}] = \mathrm{se}[\hat{\beta}_{1,OLS}]/R^2_p,
\end{equation}
где $\mathrm{se}[\cdot]$ обозначает асимптотические стандартные ошибки, а $R^2_p$ определён после (4.56). При обобщении до модели с более чем одним эндогенным регрессором этот $R^2_p$ заменяется на $R^{*2}_p$, предложенный Шеа (1997). Это предоставляет обоснование для тестовой  статистики Шеа.

Низкая точность затрагивает коэффициенты при эндогенных переменных. Для экзогенных переменных стандартные ошибки коэффициентов для оценки 2SLS схожи с  МНК. Интуитивно, экзогенные переменные являются инструментами для самих себя, а значит, очень сильными инструментами.

Для коэффициентов при эндогенных переменных низкий \textbf{частный} $R^2$, а не $R^2$ приводит к потере точности оценки. Это объясняет, почему стандартные ошибки 2SLS могут быть гораздо больше, чем для МНК, несмотря на высокую <<сырую>> корреляцию между эндогенной переменной и инструментами. С другой стороны, если стандартные ошибки 2SLS для коэффициентов при эндогенных переменных заметно выше, чем стандартные ошибки МНК, это является чётким сигналом, что инструменты слабые.

Статистики, используемые для распознавания низкой точности оценок методом инструментальных переменных, называются мерами \textbf{релевантности инструментов}. В некоторой степени они не являются необходимыми, так как проблему легко заметить, если стандартные ошибки IV намного выше, чем стандартные ошибки МНК.

\subsection{Смещённость в малых выборках}

Этот раздел резюмирует относительно многообещающий и до сих пор слабо изученный вопрос смещенности в малых выборках. Основная литература посвящена исследованию того факта, что даже в <<больших>> выборках асимптотическая теория может предоставлять плохое приближение распределения оценок методом инструментальных переменных. В малых выборках оценка IV смещена, даже если асимптотически она состоятельна. Это смещение может быть особенно заметным, если инструменты слабые.

Это смещение IV, направленное в сторону несостоятельной оценки МНК, может быть удивительно большим, как это было продемонстрировано в простом эксперименте Монте Карло в работе Нельсона и Старца (1990), и на  реальных данных, включающих несколько сотен тысяч наблюдений, но очень слабые инструменты в работе Баунда и др. (1995). Более того, стандартные ошибки также могут быть серьёзно смещёнными, что также было продемонстрировано  Нельсонон и Старцем (1990).

Теоретическая литература использует весьма специализированную и продвинутую эконометрическую теорию, так как получить выборочное среднее оценки IV действительно сложно. Чтобы увидеть это, рассмотрим адаптацию к оценке IV обычного доказательства несмещённости оценки МНК, приведённого в Разделе 4.4.8. Для $\hat{\beta}_{IV}$, определённой в (4.51), в точно идентифицированном случае следует
\[
\begin{array}{rcl}
\E[\hat{\beta}_{IV}] & = & \beta + \E_{Z,X,u}[(Z'X)^{-1}Z'u] \\
& = &  \beta + \E_{Z,X}[(Z'X)^{-1}Z' \times \E[u|Z,X]],
\end{array}
\]
где безусловное математическое ожидание по всем случайным переменным $Z, X$ и $u$ мы получили, сперва взяв ожидание по $u$ при фиксированных $Z$ и $X$, используя закон повторных математических ожиданий (см. раздел А.8.). Очевидным достаточным условием того, что математическое ожидание оценки IV равно $\beta$, является равенство $\E[u|Z,X]=0$. Однако это слишком сильное предположение, так как из него следует, что $\E[u|X]=0$, а в этом случае нет необходимости вообще использовать инструменты. Поэтому простого способа получить $\E[\hat{\beta}_{IV}]$ не существует. При доказательстве состоятельности подобной проблемы не возникает: $\hat{\beta}_{IV} = \beta +(N^{-1}Z'X)^{-1}N^{-1}Z'u$, где множитель $N^{-1}Z'u$ можно рассмотреть отдельно от $X$, и из предположения $\E[u|Z]=0$ следует $\plim N^{-1}Z'u=0$.

Таким образом, необходимо использовать альтернативные методы для получения математического ожидания оценки методом инструментальных переменных. Здесь мы просто перечисляем основные результаты.

В первоначальных исследованиях делалось сильное предположение о совместной нормальности переменных и гомоскедастичных ошибках. Тогда оценка IV имеет распределение Уишарта (определённое в Главе 13). Удивительно, но в случае точно определённой модели математического ожидания оценки IV вообще не существует --- это сигнал, что может существовать проблема конечных выборок. Математическое ожидание существует, если есть хотя бы одно сверхидентифицирующее ограничение, а дисперсия существует, если сверхидентифицирующих ограничений хотя бы два. Но даже если ожидание существует, оценка IV смещена, причём в сторону оценки МНК. С увеличением числа сверхидентифицирующих ограничений смещение увеличивается, в конце концов достигая смещения оценки МНК. Подробное обсуждение дано в учебнике Дэвидсона и МакКиннона (1993, стр. 221-224). Также использовались приближения, основанные на разложении в степенные ряды.

Что определяет размер смещения в конечных выборках? Для регрессии с единственным регрессором $x$, являющимся эндогенным и связанным с инструментами $z$ моделью в приведённой форме $x = z\pi +\upsilon$, \textbf{параметр концентрации} $\tau^2$ определяется как $\tau^2 = \pi'Z'Z\pi/\sigma^2_\upsilon$. Можно показать, что смещение оценки IV является возрастающей функцией от $\tau^2$. Величина $\tau^2/K$, где $K$ --- число инструментов, является аналогом $F$-статистики для генеральной совокупности, тестирующей ограничение $\pi=0$. Можно показать, что статистика $F-1$, где $F$ --- действительная $F$-статистика из регрессии первого шага модели в приведённой форме, является приблизительно несмещённой оценкой $\tau^2/K$. Это делает возможным тесты на смещение в конечной выборке, основанные на $F$-статистике, данной в Разделе 4.9.2.

Стайгер и Сток (1997) получили результаты при более слабых предположениях о распределении. В частности, нормальность больше не требуется. Их подход использует асимптотику слабых инструментов, находящую предельное распределение оценок IV  для последовательности моделей, где $\tau^2/K$ зафиксировано на постоянном уровне, а $N\longrightarrow \infty$. В простейшей модели $1/F$ является приблизительной оценкой для смещения в конечной выборке оценки IV по сравнению с оценкой МНК. В более общем случае величина смещения при данной $F$ зависит от числа эндогенных регрессоров и числа инструментов. Симуляции показывают, что, чтобы   смещение оценки IV не превышало 10\% смещения МНК, необходима $F>10$. Этот порог широко цитируется, но падает, например, до приблизительно 6.5, если смещение в 20\% смещения МНК является удовлетворительным. Таким образом менее строгим правилом является $F>5$. Шеа (1997) продемонстрировал, что низкий частный $R^2$ также связан со смещением в конечных выборках, но не существует похожего простого правила, позволяющего использовать частный $R^2$ для диагностики смещения в конечных выборках.

В моделях с более чем одним эндогенным регрессором $F$-статистики можно рассчитать для всех эндогенных регрессоров. В качестве совместной статистики Сток, Райт и Його (2002) предлагают использовать наименьшее собственное значение матричного аналога $F$-статистики для регрессии первого шага. Сток и Його (2003) представляют релевантные критические значения для этого собственного значения при различных уровнях удовлетворительного смещения, числа эндогенных регрессоров и числа сверхидентифицирущих ограничений. Эти таблицы включают случай с единственным эндогенным регрессором как особый, но предполагают как минимум два сверхидентифицирующих ограничения, так что они не применимы к точно идентифицированным моделям. 

Смещение в конечных выборках возникает не только для оценок коэффициентов методом инструментальных переменных, но также для оценок стандартных ошибок и тестовых статистик. Сток и др. (2002) представляют подход, аналогичный тестам Вальда, согласно которому тест $\beta = \beta_0$ на номинальном уровне 5\% имеет действительный размер не более 15\%. Сток и Його (2003) также представляют подробные таблицы, описывающие изменение размера теста, которые включают и точно идентифицированные модели. 

\subsection{Реакция на слабые инструменты}

Что может сделать практик, столкнувшись со слабыми инструментами?

Как уже было замечено, одним из подходов является ограничение числа используемых инструментов. Это можно осуществить, удалив часть инструментов из модели или объединив некоторые инструменты.

Если проблемой является смещение в конечной выборке, можно использовать альтернативные оценки, обладающие лучшими свойствами в конечных выборках, чем двухшаговый МНК. Несколько альтернатив, различных вариантов IV, представлены в Разделе 6.4.4.

Несмотря на акцент на смещении в малых выборках, другие проблемы, связанные со слабыми инструментами, могут иметь большую важность в приложениях. В достаточно большой выборке возможно, что значение $F$-статистики для регрессии первого шага в приведённой форме будет достаточно большим, и смещение в конечной выборке не представляет проблемы. В то же время частный $R^2$ может быть очень низким, приводя к чувствительности даже к очень слабой корреляции между инструментом и ошибкой модели. Эту трудность сложно проверить и преодолеть.

Также может иметь место значительное снижение точности оценок, как описано в Разделах 4.9.3 и 4.9.4. В таких случаях либо требуются большие выборки, либо необходимо использовать альтернативные подходы к оценке причинно-следственных предельных эффектов. Эти методы кратко описаны в Разделе 2.8 и представлены в различных местах этой книги.

\subsection{Пример использования инструментальных переменных}

Клинг (2001) подробно проанализировал использование близости колледжа как инструмента для образования. Здесь мы используем те же данные из Национальных лонгитюдных исследований (National Longinudinal Surveys, NLS) по когорте из 3010 мужчин в возрасте от 24 до 34 лет в 1976 году, какие были использованы для получения Таблицы 1 в работе Клинга (2001), и первоначально использовались у Карда (1995). Оценивается модель
$$
\mathrm{ln} w_i = \alpha + \beta_1 s_i + \beta_2 e_i + \beta_3 e_i^2 + x_{2i}'\gamma+u_i,
$$
где $s$ обозначает образование в годах, $e$ обозначает опыт работы в годах, $e^2$ обозначает квадрат опыта, а $x_2$ --- вектор из 26 контрольных переменных, состоящий в основном из географических индикаторов и меры образования родителей.

Переменная образования рассматривается как эндогенная вследствие отсутствия данных по способностям. Кроме того, две переменные, отвечающие за опыт работы также эндогенны, поскольку опыт работы вычислен как возраст минус длительность образования минус шесть, как принято в таких работах, и образование эндогенно. Таким образом, требуется как минимум три инструмента.

Здесь используется ровно три инструмента, то есть модель точно идентифицирована. Первым инструментом является $col4$, индикатор наличия поблизости четырёхлетнего колледжа. Этот инструмент уже был рассмотрен в Разделе 4.8.2. Двумя другими инструментами являются возраст и возраст в квадрате. Они высоко коррелируют с опытом работы и квадратом опыта, но предполагается, что их можно исключить из модели заработной платы, поскольку для работодателя важен именно опыт работы. Оставшийся вектор регрессоров $x_2$ используется как инструмент для самого себя. 

Хотя возраст очевидно экзогенен, некоторые ненаблюдаемые характеристики, такие, как социальные навыки, могут коррелировать одновременно и с возрастом, и с заработной платой. В таком случае использование возраста и квадрата возраста в качестве инструментов можно подвергнуть сомнению. Это иллюстрирует общую мысль о том, что относительно предположений о годности инструментов могут существовать разногласия.

\begin{table}[h]
\begin{minipage}{\textwidth}
\caption{  \label{tab:ivapp}Отдача от образования: оценки метода инструментальных переменных} 

    \begin{tabular}{lcc}
    \hline
	\hline
	& \bf{МНК}\footnote{Выборка из 3010 молодых людей. Зависимая переменная --- логарифм почасовой зарплаты. Даны коэффициенты и их стандартные ошибки для образования; оценки для опыта и опыта в квадрате, 26 контрольных переменных и свободного члена не указаны. Для трёх эндогенных регрессоров --- образования ($s$), опыта ($e$) и квадрата опыта ($e^2$) --- тремя инструментами являются индикатор близости четырёхлетнего колледжа, возраст и возраст в квадрате. Частный $R^2$ и $F$-статистика из регрессии первой стадии используются для диагностики слабых инструментов, объяснённой в тексте.} & \bf{IV} \\
	\hline
    Образование (s) & 0.073 & 0.132 \\
    	& (0.004) & (0.049) \\
    $R^2$	& 0.304 & 0.207 \\
    Частный $R^2$ Шеа & --- & 0.006 \\
    $F$-статистика для $s$ & --- & 8.07 \\
    \hline
	\hline
\end{tabular}
\end{minipage}
\end{table} 


Результаты приведены в Таблице 4.5. Оценка МНК $\beta_1$ равна 0.073, то есть зарплаты растут в среднем на 7.6\% ( $= 100 \times (e^{0.073}-1)$) с каждым дополнительным годом образования. Эта оценка является несостоятельной оценкой $\beta_1$, если способности не включены в регрессию. Оценка IV или эквивалентная ей оценка 2SLS (так как модель точно идентифицирована) равны 0.132. Дополнительный год образования приводит к увеличению зарплаты на 14.1\% ( $= 100 \times (e^{0.132}-1)$). 

Оценка IV  гораздо менее эффективна, чем оценка МНК. Формальный тест не отвергает гипотезу о гомоскедастичности, и мы, следуя Клингу (2001), используем обычные стандартные ошибки, которые здесь очень близки к стандартным ошибкам, устойчивым к гетероскедастичности. Стандартная ошибка $\hat{\beta}_{1, OLS}$ равна 0.004, тогда как $\hat{\beta}_{1, IV}$ превышает её более чем в 10 раз. Стандартные ошибки при других эндогенных регрессорах примерно в 4 раза больше, а стандартные ошибки при экзогенных регрессорах примерно в 1.2 раза больше. Величина $R^2$ падает с 0.304 до 0.207. 

Меры типа $R^2$ подтверждают, что инструменты не вполне релевантны для образования. В качестве простого теста заметим, что регрессия (4.58) образования на все инструменты даёт  $R^2=0.297$, который падает лишь немного до  $R^2=0.291$, если исключить три дополнительные инструмента. Более формально, частный  $R^2$ Шеа здесь равен $0.0064 = 0.08^2$, откуда по (4.62) следует, что стандартная ошибка $\hat{\beta}_{1, IV}$ будет домножена на $12.5 = 1/0.08$, очень близко к увеличению, наблюдаемому здесь. Это сокращает $t$-статистику при образовании с 19.64 до 2.68. Во многих приложениях такое сокращение привело бы к статистической незначимости. Вдобавок, согласно Разделу 4.9.2, даже лёгкая корреляция между инструментом $col4_i$ и ошибкой $u_i$ может привести к несостоятельности метода инструментальных переменных. 

Чтобы увидеть, что смещение в конечной выборке тоже является проблемой, мы строим регрессию (4.58) образования на все остальные инструменты. Тестирование совместной значимости трёх дополнительных инструментов даёт $F$-статистику 8.07, говорящую, что смещение оценки IV может составлять от 20\% до 10\% смещения оценки МНК. Аналогичная регрессия для двух других экзогенных переменных даёт гораздо более высокие  $F$-статистики,  например, возраст является хорошим инструментом для опыта работы. С учётом того, что эндогенных регрессора 3, в действительности лучше использовать метод Стока и др. (2002), описанный в Разделе 4.9.4, хотя здесь проблема ограничена до образования, поскольку для опыта и опыта в квадрате, соответственно, частный $R^2$ Шеа равен 0.0876 и 0.0138, тогда как $F$-статистики для первого шага равны 1.772 и 1.542.

Если доступны дополнительные инструменты, модель становится сверхидентифицированной, и стандартной процедурой является дополнительный тест на сверхидентифицирующие ограничения (см. Раздел 8.4.4).
 
\section{Практические соображения}

Процедуры оценки, описанные в этой главе, присутствуют во всех стандартных статистических пакетах, работающих с пространственными данными, за исключением того, что не во всех пакетах реализована квантильная регрессия. Большая часть пакетов реализует робастные стандартные ошибки как опцию, а не по умолчанию.

Самой сложной в применении является оценка методом инструментальных переменных, так как во многих потенциальных приложениях может быть сложно найти инструменты, не коррелирующие с ошибкой, но существенно коррелирующие с регрессором или регрессорами, к которым они применяются. Такие инструменты можно получить через спецификацию полной структурной модели, такой, как модель одновременных уравнений. Современные прикладные исследования подчёркивают альтернативные подходы, такие, как естественные эксперименты.

\section{Библиографические примечания}

Результаты из этой главы представлены во многих текстах для первого курса магистратуры, таких, как тексты Дэвидсона и МакКиннона (2004), Грина (2003), Хаяши (2000), Джонстона и Динардо (1997), Миттельхаммера, Джаджа, и Миллера (2000) и Рууда (2000). Мы сделали акцент на регрессии со стохастическими регрессорами, робастных стандартных ошибках, квантильной регрессии, эндогенности и инструментальных переменных.

\begin{itemize}
\item [$4.2$] Мански (1991) приводит отличное обсуждение общей постановки задачи, включающее обсуждение функций потерь, данных в Разделе 4.2.
\item [$4.3$] Пример с отдачей от образования хорошо изучен. Ангрист и Крюгер (1999) и Кард
(1999) приводят свежие обзоры этой темы.
\item [$4.4$] Историю метода наименьших квадратов см. у Стиглера (1986). Этот метод был введён Лежандром в 1805 году. Гаусс в 1810 году применил метод наименьших квадратов к модели с нормально распределённой ошибкой и предложил метод исключения переменных для нахождения решения, а в дальнейшей работе он предложил теорему, называемую теоремой Гаусса-Маркова. Гальтон предложил понятие регрессии, имея в виду возвращение к среднему в контексте наследования семейных черт, в 1887 году. 
Раннее <<современое>> описание с приложением к пауперизму и доступности благосостояния см. у Юла (1897). Статистические выводы, основанные на оценках МНК линейной регрессионной модели, были разработаны в особенной степени Фишером. Устойчивые к гетероскедастичности оценки ковариационной матрицы оценки МНК, благодаря Уайту (1980a), основывавшемуся на более ранней работе Айкера (1963), оказали глубокое воздействие на статистические заключения в микроэконометрике и были обобщены для множества задач.
\item [$4.6$] Боскович в 1757 году предложил оценку метода наименьших модулей ошибок, опередившую метод наименьших квадратов, см. работу Стиглера (1986). Обзор квантильной регрессии, введённой Коэнкером и
Бассеттом (1978), дан в работе Бучински (1994). Более элементарное рассмотрение дано в работе Коэнкера and Халлока (2001).
\item [$4.7$] Наиболее раннее из известных использование инструментальных переменных для обеспечения идентификации в модели одновременных уравнений относится к работе Райта (1928). Другим часто цитируемым источником является Райерсол (1941), который использовал метод инструментальных переменных для поправки ошибки измерения в регрессорах. Сарган (1958) даёт классическое раннее описание оценки IV.  Сток и Требби (2003) приводят дополнительные ссылки на ранние работы.
\item [$4.8$] Оценка методом инструментальных переменных представлена в эконометрических текстах с акцентом на алгебре, но не всегда на интуиции. Этот метод широко используется в эконометрике благодаря желательности получения оценок, обладающих причинно-следственной интерпретацией.
\item [$4.9$] К проблеме слабых инструментов внимание прикладных исследователей привлекли Нельсон и Старц (1990) и Баунд и др. (1995). Существует некоторое число их теоретических предшественников, особенно стоит упомянуть работу Нагара (1959). Эта проблема убавила энтузиазм по отношению к оцениванию IV, а смещение в конечных выборках вследствие слабых инструментов является сейчас очень активной темой исследований. Результаты часто предполагают нормальные независимые одинаково распределенные ошибки и ограничивают анализ до случая с одним эндогенным регрессором. Исследование Стока и др. (2002)  содержит много ссылок с акцентом на асимптотику слабых инструментов. Исследование Хана и Хаусмана (2003b) содержит дополнительные методы и результаты, которые мы не привели здесь. Недавние исследования по смещению в стандартных ошибках см. в работах Бонда и Виндмейера (2002). Пример аккуратного применения см. К.-И. Ли (2001).
\end{itemize}

\begin{center}
Упражнения
\end{center} 
\begin{small}
\begin{enumerate}
\item [$4-1$] Рассмотрим модель линейной регрессии $y_i = x_i'\beta +u_i$ с нестохастическими регрессорами $x_i$ и ошибкой $u_i$, которая имеет нулевое среднее, но коррелирует следующим образом: $\E[u_i u_j] = \sigma^2$, если $i=j$, $\E[u_i u_j] = \rho \sigma^2$, если $|i-j|=1$, и $\E[u_i u_j] = 0$, если $|i-j|>1$. Таким образом, ошибки для непосредственно соседствующих наблюдений коррелируют, тогда как остальные ошибки не коррелируют. В матричных обозначениях $y = x'\beta +u$, где $\Omega = \E [u'u]$. Ответьте на вопросы для этой модели, используя результаты из Раздела 4.4.
\begin{enumerate}
\item Проверьте, что  $\Omega$ является  матрицей с ненулевыми элементами только на диагонали и в напрямую смежных с ней ячейках и приведите эти ненулевые элементы. 
\item Получите асимптотическое распределение $\hat{\beta}_{\text{OLS}}$, используя (4.19).
\item Объясните, как получить состоятельную оценку $\V[\hat{\beta}_{\text{OLS}}]$, не зависящую от неизвестных параметров.
\item Является ли обычная оценка $s^2 (X'X)^{-1}$, выводимая с МНК, состоятельной оценкой $\V[\hat{\beta}_{\text{OLS}}]$?
\item Является ли оценка Уайта $\V[\hat{\beta}_{\text{OLS}}]$, устойчивая к гетероскедастичности, состоятельной здесь?
\end{enumerate}
\item [$4-2$] Предположим, что мы оцениваем модель $y_i = \mu + u_i$, где $u_i \sim \mathcal{N}[0,\sigma^2_i]$.
\begin{enumerate}
\item Покажите, что оценка МНК для $\mu$ упрощается до $\bar{y}$.
\item Посчитайте дисперсию $\bar{y}$ напрямую. Покажите, что она равна оценке дисперсии Уайта, устойчивой к гетероскедастичности, данной в (4.21). 
\end{enumerate}
\item [$4-3$] Предположим, что процесс, порождающий данные $y_i = \beta_0 x_i + u_i$, $x_i \sim \mathcal{N}[0,1]$, $u_i = x_i \epsilon_i$, и $\epsilon_i \sim \mathcal{N}[0,1]$. Предположим, что данные независимы по $i$, а $x_i$ не зависит от $\epsilon_i$. Заметим, что первые четыре центральных момента $\mathcal{N}[0,1]$ равны $0$, $\sigma^2$, $0$ и $3 \sigma^4$.
\begin{enumerate}
\item Покажите, что ошибки $u_i$ условно гетероскедастичны.
\item Получите $\plim N^{-1}X'X$. [Подсказка: получите $\E[x_i^2]$ и примените закон больших чисел].
\item Получите $\sigma_0^2 = \V[u_i]$, где математическое ожидание берётся по всем стохастическим переменным в модели.
\item Получите $\plim N^{-1}X'\Omega_0 X = \lim N^{-1}\E[X'\Omega_0 X]$, где $\Omega_0 = \mathrm{Diag}[\V[u_i|x_i]]$
\item Используя ответы на предыдущие вопросы, рассчитайте оценку по умолчанию (4.22), рассчитываемую для МНК, ковариационной матрицы в предельном распределении $\sqrt{N}(\hat{\beta}_{\text{OLS}}-\beta_0)$, игнорируя потенциальную гетероскедастичность. Ваш конечный ответ должен быть выражен числом. 
\item Теперь рассчитайте дисперсию  $\sqrt{N}(\hat{\beta}_{\text{OLS}}-\beta_0)$, учитывая гетероскедастичность. Ваш конечный ответ должен быть выражен числом.
\item Согласуется ли расхождение между ответами на (e) и (f) с вашими априорными предположениями?
\end{enumerate}
\item [$4-4$] Рассмотрим модель линейной регрессии со скалярным регрессором $y_i=\beta x_i +u_i$, где данные $(y_i, x_i)$ распределены одинаково и независимо от $i$, хотя ошибка может быть условно гетероскедастичной.
\begin{enumerate}
\item Покажите, что $(\hat{\beta}_{\text{OLS}}-\beta) = (N^{-1}\sum_i x_i^2)^{-1}N^{-1}\sum_i x_i u_i$.
\item Примените закон больших чисел Колмогорова (Теорема A.8) к средним значениям $x_i^2$ и $x_i u_i$, чтобы показать, что $\hat{\beta}_{\text{OLS}} \xrightarrow{p} \beta$. Сформулируйте дополнительные допущения о процессе, порождающем данные для $x_i$ и $u_i$.
\item Примените центральную теорему Линдеберга-Леви (Теорема A.14) к средним значениям $x_i u_i$, чтобы показать, что $N^{-1}\sum_i x_i u_i /N^2 \sum_i \E [u_i^2 x_i^2] \xrightarrow{p} \mathcal{N}[0,1]$. Сформулируйте дополнительные допущения о процессе, порождающем данные для $x_i$ и $u_i$.
\item Используйте правило произведения для предела нормальных распределений (Теорема A.17), чтобы показать, что из части (с) следует $N^{-1/2}\sum_i x_i u_i \xrightarrow{p} \mathcal{N}[0,\lim  \sum_i \E [u_i^2 x_i^2]]$ Сформулируйте дополнительные допущения о процессе, порождающем данные для $x_i$ и $u_i$.
\item Объедините результаты, используя (2.14) и правило произведения для предельных нормальных распределений (Теорема A.17), чтобы получить предельное распределение $\beta$. 
\end{enumerate}
\item [$4-5$] Рассмотрим линейную модель регрессии $y = X\beta +u$.
\begin{enumerate}
\item Получите формулу для $\hat{\beta}$, минимизирующую $Q(\beta) = u'Wu$, где $W$ --- матрица полного ранга. [Подсказка:  правило дифференцирования матриц для столбцов $x$ и $z$ выглядит как $\partial f(x)/\partial x = (\partial z')/\partial x) \times (\partial f(z)/\partial z)$, если $f(x) = f(g(x)) = f(z)$, где $z = g(x)$].
\item Покажите, что эта формула упрощается до оценки МНК, если $W=I$.
\item Покажите, что эта формула даёт оценку ОМНК, если $W= \Omega^{-1}$.
\item Покажите, что эта формула даёт оценку 2SLS, если $W= Z(Z'Z)^{-1}Z'$.
\end{enumerate}
\item [$4-6$] Рассмотрим оценивание IV (Раздел 4.8) модели $y = x'\beta +u$ с использованием инструмента $z$ в точно идентифицированном случае, когда $Z$ является матрицей $N \times K$ полного ранга.
\begin{enumerate}
\item Каким существенным предположениям должна удовлетворять $z$, чтобы оценка IV была состоятельной для $\beta$? Объясните.
\item Покажите, что в случае точной идентификации оценка 2SLS, определённая в (4.53), упрощается до оценки IV, данной в (4.51).
\item Приведите пример из реальной жизненной ситуации, когда оценка IV необходима из-за несостоятельности МНК, и специфицируйте подходящие инструменты. 
\end{enumerate}
\item [$4-7$] (Адаптировано из работы Нельсона и Старца, 1990.) Рассмотрим модель с тремя уравнениями $y = \beta x +u$; $x = \lambda u + \epsilon$; $z = \gamma \epsilon + v$, где взаимно независимые ошибки $u$, $\epsilon$ и $v$ распределены одинаково, независимо и нормально со средним $0$ и дисперсиями, соответственно, $\sigma^2_{u}$,$\sigma^2_{\epsilon}$ и $\sigma^2_{v}$.
\begin{enumerate}
\item Покажите, что $\plim (\hat{\beta}_{\text{OLS}}-\beta) = \lambda \sigma^2_{u}/(\lambda^2 \sigma^2_{u} +\sigma^2_{\epsilon})$.
\item Покажите, что $\rho_{XZ}^2 = \gamma \sigma^2_{\epsilon} /(\lambda^2 \sigma^2_{u} +\sigma^2_{\epsilon})\lambda^2 \sigma^2_{\epsilon} +\sigma^2_{v}$.
\item Покажите, что $\hat{\beta}_{IV} = m_{ZY}/m_{ZX} = \beta + m_{ZY}/(\lambda m_{ZU}+m_{ZX})$, где, например, $m_{ZU} = \sum_i z_i y_i$.
\item Покажите, что $\hat{\beta}_{IV}\rightarrow 1/\lambda$ по мере того, как $\gamma$ (или $\rho_{XZ}$) $\rightarrow 0$.
\item  Покажите, что $\hat{\beta}_{IV}\rightarrow \infty$ по мере того, как $m_{ZU} \rightarrow -\gamma \sigma^2_{\epsilon}/\lambda$.
\item К чему приводят последние два результата в контексте смещения в конечной выборке и моментов $\hat{\beta}_{IV}-\beta$, когда инструменты плохие?
\end{enumerate}
\item [$4-8$] Выберите 50\% случайную подвыборку из данных Раздела 4.6.4 с логарифмом расходов на здравоохранение ($y$) и логарифмом совокупных расходов $x$.
\begin{enumerate} 
\item Получите оценки МНК и сопоставьте обычную оценку стандартных ошибок коэффициента наклона и ошибку по Уайту.
\item Получите оценки медианной регрессии и сравните их с оценками МНК. 
\item Получите оценки квантильной регрессии для $q=0.25$ и $q=0.75$.
\item Воспроизведите Рис. 4.2, используя свои ответы из частей (a)---(c).
\end{enumerate}
\item [$4-9$] Выберите 50\% случайную подвыборку из данных Раздела 4.9.6 с доходами и образованием, воспроизведите как можно большую часть Таблицы 4.5  и дайте подходящую интерпретацию.
\end{enumerate}

\end{small}

