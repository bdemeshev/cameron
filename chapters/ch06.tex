\chapter{Обобщённый метод моментов и системы уравнений}

\section{Вступление}

Предыдущая глава была посвящена М-оценкам, в том числе оценкам ММП и НМНК. Теперь рассмотрим более широкий класс экстремальных оценок, основанных на методе моментов (ММ) и обобщённом методе моментов (ОММ).

Основой ММ и ОММ является спецификация набора теоретических условных моментов,  которая включает в себя данные и неизвестные параметры. Оценки ММ являются решением условий для выборочных моментов, которые соответствуют условиям для теоретических моментов. Например, среднее по выборке --- ММ оценка математического ожидания в генеральной совокупности. В некоторых случаях возможно, что нет явного аналитического решения для ММ оценки, но есть численное. Тогда оценка является примером оценки методом оценочных уравнений, кратко введённой в Разделе 5.4.

В некоторых ситуациях, однако, невозможно найти оценку ММ, поскольку есть больше условий моментов и, следовательно, больше уравнений для решения, чем параметров. Ярким примером является оценка инструментальных переменных в сверх-идентифицированной модели. ОММ оценки, введённые Хансеном (1982), являются расширением ММ подхода для работы с этим случаем.

ОММ определяет класс оценок. Различные оценки ОММ получаются при выборе различных теоретических моментов так же, как различные спецификации плотности или вероятности приводят к различным оценкам ММП. Мы подчёркиваем подход к оцениванию, основанный на моментах, даже в тех случаях, когда возможны альтернативные представления, так как он обеспечивает единство подход и может предоставить очевидный способ расширить методы от линейной к нелинейной модели.

Основы ОММ оценивания определены в Разделах 6.2 и 6.3, в которых представлены, соответственно, разъяснительные примеры и асимптотические результаты для статистических выводов. Остальная часть главы концентрируется на более специализированных оценках. Оценки инструментальных переменных представлены в Разделах 6.4 и 6.5. Для линейных моделей то, что представлено в Разделах 4.8 и 4.9, может быть достаточно, но для нелинейных моделей нужно использовать ОММ подход. Раздел 6.6 охватывает методы для вычисления стандартных ошибок последовательных двуступенчатых М-оценок. Разделы 6.7 и 6.8 представляют оценку минимального расстояния, вариант ОММ, и эмпирические оценки правдоподобия, то есть альтернативные оценки ОММ. Оценивание систем уравнений, используемое в относительно небольшой доле микроэконометрических исследований, обсуждается в Разделах 6.9 и 6.10.

В этой главе рассматриваются многие методы оценивания с точки зрения ОММ. Примеры использования этих методов на реальных данных включают применение линейного метода инструментальных переменных в Разделе 4.9.6 и применение линейного ОММ для панелей в Разделе 22.3.

\section{Примеры}

ОММ оценки основаны на принципе аналогии (см. Раздел 5.4.2). Условия для теоретических моментов приводят к условиям для выборочных моментов, которые могут быть использованы для оценки параметров. В этом разделе приводится несколько основных применений этого принципа, при этом свойства полученной оценки будут рассматриваться только в Разделе 6.3.

\subsection{Линейная регрессия}

Классическим примером метода моментов является оценка теоретического момента, когда
$y$ является одинаково и независимо распределённым с математическим ожиданием $\mu$. Для генеральной совокупности:
\[
\E[y-\mu]=0.
\]
Замена оператора математического ожидания $\E[\cdot]$ для генеральной совокупности на средний оператор $N^{-1} \sum_{i=1}^{N} (\cdot)$ для выборки даёт соответствующий выборочной момент:
\[
\frac{1}{N} \sum_{i=1}^{N} (y_i-\mu)=0.
\]
Решение для $\mu$ приводит к оценке $\hat{\mu}_{MM}=N^{-1} \sum_i y_i=\bar{y}$. Оценка ММ
математического ожидания генеральной совокупности --- выборочное среднее.

Этот подход может быть обобщён на линейную регрессионную модель $y=x'\beta+u$, где
$x$ и $\beta$ --- векторы размера $K \times 1$. Предположим, что ошибки $u$ имеет нулевое математическое ожидание при фиксированных регрессорах. Одно ограничение на условный момент $\E[u|x]=0$ приводит к $K$ уравнениям на безусловное математическое ожидание $\E[xu]=0$, так как
\begin{equation}
\E[xu]=\E_{x}[\E[xu|x]]=\E_{x}[x\E[u|x]]=\E_{x}[x \cdot 0].
\end{equation}
Здесь мы использовали закон повторного  математического ожидания (см. Раздел А.8) и предполагали, что $\E[u|x]=0$. Таким образом,
\[
\E[x(y-x'\beta)]=0,
\]
если условное математическое ожидание ошибки нулевое. Оценки ММП являются решением  соответствующих условий для выборочных моментов:
\[
\frac{1}{N} \sum_{i=1}^{N} x_i(y_i-x'_i\beta)=0.
\]
Мы получаем $\hat{\beta}_{MM}=(\sum_i x_i x'_i)^{-1} \sum_i x_i y_i$.

Поэтому МНК-оценка --- частный случай оценки ММ. Вывод формулы для ММ-оценки, несмотря на её совпадение с МНК оценкой, существенно отличаются от обычной минимизации суммы квадратов остатков.

\subsection{Нелинейная регрессия}

Для нелинейной регрессии подход метода моментов сводится к НМНК, если ошибки регрессии являются аддитивными. Для более общей нелинейной регрессии с неаддитивными ошибками (определённой далее) метод моментов даёт состоятельную оценку в то время, как НМНК даёт несостоятельную оценку.

Из Раздела 5.8.3 нелинейная регрессионная модель с аддитивной ошибкой является моделью, которая определяет $y$ в виде:
\[
y=g(x,\beta)+u.
\]
Подход моментов, аналогичный подходу для линейной модели, в которой мы получили, что $\E[u|x]=0$, здесь приводит к условию $\E[h(x)(y-x'\beta)]=0$, где $h(x)$ --- любая функция от $x$. Конкретный выбор $h(x)=\partial g(x,\beta)/ \partial \beta$, мотивированный в Разделе 6.3.7, приводит к условию на выборочные моменты, совпадающие с условиями первого порядка для оценки НМНК, приведённому в Разделе 5.8.2.

Более общая нелинейная регрессионная модель с неаддитивными ошибками специфицирует
\[
u=r(y,x,\beta),
\]
где снова $\E[u|x]=0$, но теперь $y$ уже необязательно должен быть аддитивной функцией от $u$. Например, в регрессии Пуассона можно определить стандартизированные ошибки как $u=[y-\exp(x'\beta)]/[\exp(x'\beta)]^{1/2}$, который имеет $\E[u|x]=0$ и $\Var[u|x]=1$, так как условное математическое ожидание и дисперсия $y$ равны $\exp(x'\beta)$.

Оценка НМНК несостоятельна при неаддитивных ошибках. Минимизация $N^{-1} \sum_i u^2_i=$

$N^{-1} \sum_i r(y_i,x_i,\beta)^2$ приводит к условиям первого порядка:
\[
\frac{1}{N} \sum_{i=1}^{N} \frac{\partial r(y_i,x_i,\beta)}{\partial \beta} r(y_i,x_i,\beta)=0.
\]

Здесь появляется $y_i$ в обоих сомножителях, и нет никакой гарантии, что математическое ожидание этого произведения равно нулю, даже если $\E[r(\cdot)|x]=0$. Эта несостоятельность не возникала при аддитивных ошибках $r(\cdot)=y-g(x,\beta)$, поскольку тогда $\partial r(\cdot) / \partial \beta=-\partial g(x,\beta) / \partial \beta$, то есть только второй сомножитель зависит от $y$.

Подход метода моментов даёт состоятельную оценку. Из предположения $\E[u|x]=0$ следует, что
\[
\E[h(x)r(y,x,\beta)]=0,
\]
где $h(x)$ --- функция от $x$. Если $\dim[h(x)]=K$, то соответствующий выборочный момент: 
\[
\frac{1}{N} \sum_{i=1}^{N} h(x_i)r(y_i,x_i,\beta)=0
\]
даёт состоятельную оценку $\beta$, где решение получается численными методами.

\subsection{Метод максимального правдоподобия}

Информационный критерий Кульбака - Лейблера  был определён в Разделе 5.7.2. Из этого определения локальный максимум критерия достигается, если $\E[s(\theta)]=0$, где $s(\theta)= \partial \ln f(y|x,\theta) / \partial \theta$ и $f(y|x,\theta)$ --- условная плотность.

Замена теоретических моментов на выборочные моменты даёт оценку $\hat{\theta}$, которая является решением $N^{-1} \sum_i s_i(\theta)=0$. Это условия первого порядка ММП, так что оценка ММП может быть рассмотрена как оценка ММ.

\subsection{Дополнительные ограничения моментов}

Использование дополнительных моментов может повысить эффективность оценки, но требует адаптирования обычного метода моментов, если есть больше условий моментов, чем параметров для оценивания.

Простой пример неэффективной оценки ---  математическое ожидание выборки. Это неэффективная оценка математического ожидания генеральной совокупности, за исключением случая, когда данные являются случайной выборкой нормального распределения или другого распределения экспоненциального семейства. Одним из способов повышения эффективности является использование альтернативных оценок. Медиана выборки, состоятельная для $\mu$, если распределение симметрично, может быть более эффективной. Очевидно, что может быть использована оценка ММП, если распределение задано полностью, но вместо этого здесь повышается эффективность с помощью дополнительных условий моментов.

Рассмотрим оценку $\beta$ в линейной регрессионной модели. МНК-оценка неэффективна, даже при гомоскедастичных ошибках, крому случая нормально распределенных ошибок. Из Раздела 6.2.1, МНК-оценка --- оценка ММ, основанная на $\E[xu]=0$. Теперь введём предположение о дополнительных моментах такое, что ошибки условно симметричны, $\E[u^3|x]=0$ и, следовательно, $\E[x u^3]=0$. Тогда оценка $\beta$ может быть основана на $2K$ условных моментах 
\[
\begin{bmatrix} \E[x(y-x'\beta)] \\ \E[x(y-x'\beta)^3] \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}.
\]
ММ подход мог бы оценивать $\beta$ как решение соответствующих уравнений для выборочных моментов $N^{-1} \sum_i x_i(y_i-x'_i\beta)=0$ и $N^{-1} \sum_i x_i(y_i-x'_i\beta)^3=0$. Тем не менее, с $2K$ уравнениями и только $K$ неизвестными параметрами $\beta$, невозможно, чтобы были удовлетворены все эти условия для выборочных моментов.

Подход ОММ вместо этого устанавливает выборочные моменты как можно ближе к нулю с использованием функции квадратичных потерь. Другими словами $\hat{\beta}_{MM}$ минимизирует
\begin{equation}
\mathcal{Q}_{N}(\beta)= \begin{bmatrix}\frac{1}{N} \sum_i x_i u_i \\ \frac{1}{N} \sum_i x_i u^3_i \end{bmatrix}' W_N \begin{bmatrix}\frac{1}{N} \sum_i x_i u_i \\ \frac{1}{N} \sum_i x_i u^3_i \end{bmatrix},
\end{equation}
где $u_i=y_i-x'_i\beta$ и $W_N$ --- матрица весов размера $2K \times 2K$. Для некоторых $W_N$ эта оценка является более эффективной, чем оценка МНК. Этот пример анализируется в Разделе 6.3.6.

\subsection{Регрессия инструментальных переменных}

Оценивание методом инструментальных переменных является одним из главных примеров обобщённого метода моментов.

Рассмотрим линейную регрессионную модель $y=x'\beta+u$ с усложнением, что некоторые компоненты $x$ коррелируют с ошибками, поэтому МНК даёт несостоятельную оценку для $\beta$. Предположим существование инструментов $z$ (введенных в Разделе 4.8), которые коррелируют с $x$, но удовлетворяют условию $\E[u|z]=0$. Тогда $\E[y-x'\beta|z]=0$. Используя алгебраические преобразования, аналогичные тем, которые используются для получения (6.1) для МНК, мы умножаем на $z$, чтобы получить $K$ условий для безусловных теоретических моментов
\begin{equation}
\E[z(y-x'\beta)]=0.
\end{equation}
Оценки метода моментов являются решением соответствующих условий для выборочных моментов:
\[
\frac{1}{N} \sum_{i=1}^{N} z_i(y_i-x'_i\beta)=0.
\]
Если $\dim(z)=K$, то $\hat{\beta}_{MM}= (\sum_i z_i x'_i)^{-1} \sum_i z_i y_i$, что является оценкой линейного метода инструментальных переменных, введённой в Разделе 4.8.6.

Единственного решения не существует, если есть больше потенциальных инструментов, чем регрессоров, поскольку $\dim(z)>K$ и уравнений больше, чем неизвестных. Одна из возможностей
заключается в использовании только $K$ инструментов, но тогда происходит потеря эффективности. Оценки ОММ вместо этого выбирают $\hat{\beta}$ так, чтобы вектор $N^{-1} \sum_i z_i(y_i-x'_i\beta)$ был как можно меньшим, используя квадратичные потери, чтобы $\hat{\beta}_{GMM}$ минимизировала
\begin{equation}
\mathcal{Q}_{N}(\beta)= \left[ \frac{1}{N} \sum_{i=1}^{N} z_i(y_i-x'_i\beta) \right]' W_N \left[ \frac{1}{N} \sum_{i=1}^{N} z_i(y_i-x'_i\beta) \right],
\end{equation}
где $W_N$ --- матрица весов размера $\dim(z) \times \dim(z)$. Оценка двухшагового МНК (см. Раздел 4.8.6) соответствует определённой матрице $W_N$.

Метод инструментальных переменных для линейных моделей представлен в деталях в Разделе 6.4. Преимуществом подхода ОММ является то, что он предоставляет возможность указать оптимальный выбор матрицы весов $W_N$, что приводит к более эффективной оценке, чем оценка двухшагового МНК.

Раздел 6.5 охватывает метод инструментальных переменных для нелинейных моделей. Одним из преимуществ подхода ОММ является то, что обобщение на случай нелинейной регрессии является прямым. Тогда мы просто заменяем $y-x'\beta$ в предыдущем выражении для $\mathcal{Q}_{N}(\beta)$ на нелинейную модель ошибки $u=y-g(x'\beta)$ или $u=r(y,x,\beta)$.

\subsection{Панельные данные}

Другое важное применение ОММ и связанных с ним методов оценивания --- регрессии панельных данных.

В качестве примера предположим, что $y_{it}=x'_{it}\beta+u_{it}$, где $i$ обозначает индивидов и $t$ обозначает время. Из Раздела 6.2.1 сквозная МНК регрессия $y_{it}$ на $x_{it}$ является оценкой ММ, основанной на условии $\E[x_{it}u_{it}]=0$. Предположим дополнительно, что ошибка $u_{it}$ не коррелирует с регрессорами в период времени, выходящий за рамки текущего периода. Тогда условие $\E[x_{is}u_{it}] = 0$ для $s \not= t$ создаёт дополнительные условия моментов, которые могут быть использованы для получения более эффективных оценок.

Главы 22 и 23 содержат множество применений ОММ для панельных данных.

\subsection{Условия моментов из экономической теории}

Экономическая теория может генерировать условия на моменты, которые могут быть использованы в качестве основы для оценивания.

Начнём с модели:
\[
y_t=\E[y_t|x_t,\beta]+u_t,
\]
где первое слагаемое в правой части измеряет <<ожидаемую>> компоненту $y$, зависимого от $x$, а второе слагаемое измеряет <<непредвиденную>> компоненту. Например, $y$ может означать доходность актива или уровень инфляции. При выполнении предположений о рациональности ожиданий и о равновесии на рынке или о рыночной эффективности, непредвиденная компонента непредсказуема с использованием любой информации, которая была доступна в момент времени $t$ для определения $\E[y|x]$. Тогда
\[
\E[(y_t-\E[y_t|x_t,\beta])|\mathcal{I}_{t}]=0,
\]
где $\mathcal{I}_{t}$ обозначает информацию, доступную на момент $t$.

На основе закона о повторном математическом ожидании, $\E[z_t(y_t-\E[y_t|x_t,\beta])]=0$, где $z_t$ формируется на базе информации из $\mathcal{I}_{t}$. Поскольку любая часть информации может быть использована в качестве инструмента, это обеспечивает много условий моментов, которые могут быть основой оценки. Если временные ряды недоступны, тогда ОММ минимизирует квадратичную форму:
\[
\mathcal{Q}_{T}(\beta)= \left[ \frac{1}{T} \sum_{t=1}^{T} z_t u_t \right]' W_T \left[ \frac{1}{T} \sum_{t=1}^{T} z_t u_t \right],
\]
где $u_t=y_t-\E[y_t|x_t,\beta]$. Если пространственные данные доступны для единственного момента времени $t$, то ОММ минимизирует квадратичную форму:
\[
\mathcal{Q}_{N}(\beta)= \left[ \frac{1}{N} \sum_{i=1}^{N} z_i u_i \right]' W_T \left[ \frac{1}{N} \sum_{i=1}^{N} z_i u_i \right],
\]
$u_i=y_i-\E[y_i|x_i,\beta]$, а индекс $t$ может быть удалён, поскольку анализируется только один период времени.

Этот подход не ограничивается только аддитивной структурой, используемой в мотивации. Нужно только чтобы для ошибки $u_t$ выполнялось свойство $\E[u_t|\mathcal{I}_{t}]=0$. Такие условия
вытекают из условий Эйлера в межвременной модели принятия решений в условиях неопределенности. Например, Хансен и Синглтон (1982) предлагают модель максимизации ожидаемой полезности в течение жизни, в которой выполнено условие Эйлера $\E[u_t|\mathcal{I}_{t}]=0$, где $u_t=\beta g^{\alpha}_{t+1} r_{t+1}-1,g_{t+1}=c_{t+1}/c_t$ представляет собой отношение потребления в двух периодах и $r_{t+1}$ --- доходность актива.
Параметры $\alpha$ и $\beta$, межвременная ставка дисконтирования и коэффициент относительного неприятия риска, соответственно, могут быть оценены с использованием ОММ на временных рядах или на пространственных данных, как это делалось раньше, с этим определением $u_t$. Хансен (1982) и Хансен и Синглтон (1982) рассматривают временные ряды; Макарди (1983) моделировал как потребление, так и предложение рабочей силы с использованием панельных данных.

\begin{table}[h]
\begin{center}
\caption{\label{tab:GMM} Обобщённый метод моментов: примеры}
\begin{tabular}[t]{ll}
\hline
\hline
\bf{Функция моментов $h(\cdot)$} & \bf{Метод оценки} \\
\hline
$y-\mu$ & Метод моментов для  генеральной совокупности \\
& математического ожидания \\
$x(y-x'\beta)$ & Регрессия методом наименьших квадратов \\
$z(y-x'\beta)$ & Регрессия с инструментальными переменными \\
$\partial ln f(y|x,\theta) / \partial \theta$ & Метод максимального правдоподобия \\
\hline
\hline
\end{tabular}
\end{center}
\end{table}

\section{Обобщённый метод моментов}

В этом разделе представлена общая теория оценивания ОММ. Обобщённый метод моментов определяет класс оценок. Другой выбор условия моментов и матрицы весов приводит к различным оценкам ОММ так же, как различный выбор распределения приводит к различным оценками ММП. Мы рассмотрим эти проблемы, в дополнение к рассмотрению обычных свойств состоятельности и асимптотической нормальности и методов оценки ковариационной матрицы оценки ОММ.

\subsection{Оценка метода моментов}

Отправной точкой является наличие условий для $r$ моментов для $q$ параметров, где \begin{equation}
\E[h(w_i,\theta_0)]=0,
\end{equation}
где $\theta$ --- вектор размера $q \times 1$, $h(\cdot)$ --- векторная функция размера $r \times 1$ с $r \ge q$, $\theta_0$ указывает значение $\theta$ в процессе, порождающем данные. Вектор $w$ включает в себя все наблюдаемые величины. В том числе, в соответствующих случаях, зависимую переменную $y$, потенциально эндогенные регрессоры $x$, а также $z$ инструментальных переменных. Зависимая переменная $y$ может быть вектором, чтобы приложения с системами уравнений или с панельными данными были частными случаями этого подхода. Математическое ожидание зависит от всех стохастических компонент $w$ и, следовательно, $y$, $x$ и $z$.

Выбор функциональной формы $h(\cdot)$ качественно похож на выбор модели, и потому будет зависеть от области приложения. Таблица 6.1 суммирует некоторые примеры с одним уравнением $h(w)=h(y,x,z,\theta)$, уже представленные в Разделе 6.2.

Если $r=q$, то может быть применён метод моментов. Равенство нулю теоретического момента заменяется на равенство нулю соответствующего выборочного момента, и оценка метода моментов $\hat{\theta}_{MM}$ определяется как решение:
\begin{equation}
\frac{1}{N} \sum_{i=1}^{N} h(w_i,\hat{\theta})=0.
\end{equation}

Это оценка метода оценочных уравнений, которая эквивалентно минимизирует 
\[
\mathcal{Q}_{N}(\theta)= \left[ \frac{1}{N} \sum_{i=1}^{N} h(w_i,\theta) \right]' \left[ \frac{1}{N} \sum_{i=1}^{N} h(w_i,\theta) \right],
\]
с асимптотическим распределением, представленным в Разделе 5.4 и воспроизведённым в (6.13) в Разделе 6.3.3 .

\subsection{Оценка ОММ}

Оценки ОММ основаны на $r$ независимых условиях моментов (6.5) при оценивании $q$ параметров.

Если $r=q$, модель называют точно идентифицированной и могут быть использованы ММ оценки в (6.6). Более формально $r=q$ является лишь необходимым условием для достаточной идентификации, и мы дополнительно требуем, чтобы $G_0$ в предложении 5.1 имела ранг $q$. Идентификация рассматривается в Разделе 6.3.9.

Если $r>q$, модель называется сверх-идентифицированной и (6.6) не имеет решения для $\hat{\theta}$, так как уравнений $(r)$ больше, чем неизвестных $(q)$. Вместо этого $\hat{\theta}$ выбирается так, чтобы квадратичная форма $N^{-1} \sum_i h(w_i,\hat{\theta})$ была как можно ближе к нулю. В частности, оценка обобщённого
метода моментов $\hat{\theta}_{GMM}$ минимизирует целевую функцию:
\begin{equation}
\mathcal{Q}_{N}(\theta)= \left[ \frac{1}{N} \sum_{i=1}^{N} h(w_i,\theta) \right]' W_N \left[ \frac{1}{N} \sum_{i=1}^{N} h(w_i,\theta) \right],
\end{equation}
где матрица весов $W_N$ размера $r \times r$ является симметричной положительно определённой, возможно стохастической с конечным пределом по вероятности и не зависит от $\theta$. Индекс $N$ в $W_N$ используется для указания, что её значение может зависеть от выборки. Однако размерность $r$ $W_N$ фиксирована, так как $N \rightarrow \infty$. Целевая функция также может быть выражена в матричном виде как $\mathcal{Q}_{N}(\theta)= N^{-1}I'H(\theta) \times W_N \times N^{-1}H(\theta)'I$, где $I$ представляет собой вектор  из единиц размера $N \times 1$ и $H(\theta)$ --- матрица размера $N \times r$ с $i$-ой строкой равной $h(y_i,x_i,\theta)'$.

Различные варианты матрицы весов $W_N$ приводят к различным оценками, которые, хотя и состоятельные, имеют различные дисперсии, если $r>q$. Простой выбор, хотя часто и не очень хороший, взять в качестве $W_N$ единичную матрицу. Тогда $\mathcal{Q}_{N}(\theta)={\bar{h}_1}^2+{\bar{h}_2}^2+\cdots+{\bar{h}_r}^2$ --- сумма $r$ квадратов средних арифметических, где $\bar{h_j}=N^{-1} \sum_i h_j(w_i,\theta)$ и $h_j(\cdot)$ --- $i$-ый элемент $h(\cdot)$. Оптимальный выбор $W_N$ приведён в Разделе 6.3.5.

Дифференцирование $\mathcal{Q}_{N}(\theta)$ в (6.7) по $\theta$ даёт условия первого порядка для ОММ:
\begin{equation}
\left[ \left. \frac{1}{N} \sum_{i=1}^{N} \frac{\partial h_i(\hat{\theta})'}{\partial \theta} \right|_{\hat{\theta}} \right] \times W_N \times \left[ \frac{1}{N} \sum_{i=1}^{N} h_i(\hat{\theta}) \right]=0,
\end{equation} 
где $h_i(\theta)=h_i(w_i,\theta)$ и для масштабирования мы умножили на коэффициент $1/2$. Эти уравнения обычно будут нелинейными по $\hat{\theta}$ и могут быть достаточно сложными для решения, так как $\hat{\theta}$  может появиться как в первом, так и в третьем слагаемом. Численные методы решения представлены в Главе 10.

\subsection{Распределение оценок ОММ}

Асимптотическое распределение оценки ОММ даётся в следующих утверждениях, доказываемых в Разделе 6.3.9.

\begin{proposition}[Распределение оценок ОММ] Сделаем следующие допущения:
\begin{enumerate}
\item Процесс, порождающий данные, удовлетворяет условиям на моменты (6.5), то есть $\E[h(w,\theta_0)]=0$.
\item Вектор-функция $h(\cdot)$ размера $r \times 1$ удовлетворяет $h(w,\theta^{(1)})=h(w,\theta^{(2)})$, если и только если $\theta^{(1)}=\theta^{(2)}$.
\item  Следующая матрица размера $r \times q$ существует и конечна с рангом $q$: 
\begin{equation}
G_0=\plim \frac{1}{N} \sum_{i=1}^{N} \left[ \left. \frac{\partial h_i}{\partial \theta'} \right|_{\theta_0} \right].
\end{equation}
\item  $W_N \xrightarrow{p} W_0$, где $W_0$ конечная симметричная положительно определённая матрица. 
\item $N^{-1/2} \sum_{i=1}^{N} h_i|_{\theta_0} \xrightarrow{d} \mathcal{N}[0,S(\theta_0)]$, где 
\begin{equation}
S_0=\plim N^{-1} \sum_{i=1}^{N} \sum_{j=1}^{N} \left[ h_i h'_j|_{\theta_0} \right].
\end{equation}
\end{enumerate}
Тогда оценка ОММ $\hat{\theta}_{GMM}$, которая определяется как решение условий первого порядка $\partial \mathcal{Q}_{N}(\theta) / \partial \theta=0$, приведенных в (6.8), состоятельна $\theta_0$ и
\begin{equation}
\sqrt{N} (\hat{\theta}_{GMM}-\theta_0) \xrightarrow{d} \mathcal{N}[0,(G'_0 W_0 G_0)^{-1} (G'_0 W_0 S_0 W_0 G_0) (G'_0 W_0 G_0)^{-1}].
\end{equation}
\end{proposition}

Некоторые важные специализации представлены ниже.

Во-первых, в микроэконометрическом анализе данные, как правило, предполагаются независимыми по $i$, поэтому (6.10) упрощается до следующего вида:
\begin{equation}
S_0= \plim N^{-1} \sum_{i=1}^{N} \left[ h_i h'_i|_{\theta_0} \right].
\end{equation}

Если ещё предполагается, что данные одинаково распределены, тогда (6.9) и (6.10) упрощаются до $G_0=\E[\partial h/ \partial \theta'|_{\theta_0}]$ и $S_0=\E[hh'|_{\theta_0}]$. Такие обозначения используются многими авторами.

Во-вторых, в случае точной идентификации, $r=q$, случая верного для многих оценок, в том числе ММП и МНК, результаты упрощаются до тех, которые уже представлены в Разделе 5.4 для оценки оценочных уравнений. Чтобы убедиться в этом обратим внимание, что при $r=q$ матрицы $G_0$, $W_0$ и $S_0$ --- квадратные матрицы, которые являются обратимыми, поэтому $(G'_0 W_0 G_0)^{-1} = G^{-1}_0 W^{-1}_0 {(G'_0)}^{-1}$ и ковариационная матрица (6.11) упрощается. Отсюда следует, что для ММ оценки в (6.6), 
\begin{equation}
\sqrt{N} (\hat{\theta}_{MM}-\theta_0) \xrightarrow{d} \mathcal{N}[0,{G}^{-1}_0 S_0 (G'_0)^{-1}].
\end{equation}

Оценки ММ всегда могут быть вычислены как оценки ОММ и будет инвариантны к выбору матрицы весов полного ранга.

В-третьих, лучший выбор матрицы $W_N$ --- такой, что $W_0=S^{-1}_0$. Тогда ковариационная
матрица в (6.11) упрощается до $(G'_0 S^{-1}_0 G_0)^{-1}$. Это рассматривается подробно в Разделе 6.3.5.

\subsection{Оценка ковариационной матрицы}

Статистические выводы для оценки ОММ возможны при состоятельных оценок $\hat{G}$ для $G_0$,$\hat{W}$ для $W_0$, $\hat{S}$ для $S_0$ в (6.11). Состоятельные оценки легко получаются при относительно слабых предположениях о распределении.

Для $G_0$ очевидной оценкой является следующая:
\begin{equation}
\hat{G}=\frac{1}{N} \left. \sum_{i=1}^{N} \frac{\partial h_i}{\partial \theta'} \right|_{\hat{\theta}}.
\end{equation}

Для матрицы $W_0$ используется матрица весов $W_N$ для выборки. Оценка матрицы $S_0$  размера $r \times r$ меняется в зависимости от стохастических предположений, сделанных относительно процесса, генерирующего данные. Микроэконометрический анализ обычно предполагает независимость по $i$, поэтому $S_0$ имеет более простой вид (6.12). Очевидной оценкой будет:
\begin{equation}
\hat{S}=\frac{1}{N} \sum_{i=1}^{N} h_i(\hat{\theta}) h_i(\hat{\theta})'.
\end{equation}
Поскольку $h(\cdot)$ имеет размер $r \times 1$, количество уникальных значений в $S_0$, подлежащих оцениванию, конечно и не превосходит $r(r+1)/2$. Поэтому $\hat{S}$ --- состоятельная оценка при $N \rightarrow  \infty$. Необходимости параметризации дисперсии $\E[h_i h'_i]$ меньшим числом параметров не появляется, хотя требуется её существование дисперсии. Всё, что необходимо, это некоторые дополнительные мягкие предположения, чтобы $\plim N^{-1} \sum_i \hat{h}_i \hat{h'}_i= \plim N^{-1} \sum_i h_i h'_i$. Например, если $\hat{h}_i=x_i \hat{u}_i$, где $\hat{u}_i$ --- остатки МНК, то, как мы знаем из Раздела 4.4, необходимо предполагать существование четвёртых моментов регрессоров.

Объединяя эти результаты, мы получаем, что оценки ОММ асимптотически нормально распределены с математическим ожиданием $\theta_0$ и оценочной асимптотической ковариационной матрицей:
\begin{equation}
\widehat{\Var}[\hat{\theta}_{MM}]=\frac{1}{N} (\hat{G'} W_N \hat{G})^{-1} \hat{G'} W_N \hat{S}  W_N  \hat{G}(\hat{G'} W_N \hat{G})^{-1}.
\end{equation}
Эта оценка ковариационной матрицы является робастной оценкой и является расширением устойчивой к гетероскедастичности оценке Эйкера-Уайта для случая наименьших квадратов.

Можно также взять математическое ожидание и использовать $\hat{G}_{E}=N^{-1} \sum_i \E[\partial h_i / \partial \theta']|_{\hat{\theta}}$ для $G_0$ и $\hat{S}_{E}= N^{-1} \sum_i \E[h_i h'_i]|_{\hat{\theta}}$ для $S_0$. Тем не менее, обычно для того, чтобы взять математические ожидания, требуются дополнительные предположения о распределении, и оценка ковариационной матрицы не будет столь же устойчивой к неправильной спецификации распределения.

В случае временных рядов $h_t$ имеет индекс $t$, и асимптотическая теория основана на количестве периодов времени $T \rightarrow \infty$. Для временных рядов, где  $h_t$ --- векторный $MA(q)$ процесс, обычный оценкой $\Var[\hat{\theta}_{GMM}]$ является предложенная Ньюи и Уэстом (1987b), которая использует (6.16) при $\hat{S}=\hat{\Omega_0} + \sum_{j=1}^{q}(1-\frac{j}{q+1})(\hat{\Omega}_j+\hat{\Omega'}_j)$, $\hat{\Omega}_j=T^{-1} \sum_{i=j+1}^{T} \hat{h}_t \hat{h'}_{t-j}$. Это допускает корреляцию рядов во времени в $h_t$ помимо одновременной корреляции. Более подробная информация об оценке ковариационной матрицы, в том числе об изменениях для случая временных рядов, приведена у Дэвидсона и МакКиннона (1993, Разделе 17.5), Гамильтона (1994), и Хаана и Левина (1997).

\subsection{Оптимальная матрица весов}

Применение ОММ требует спецификации функции моментов $h(\cdot)$ и матрицы весов $W_N$ в (6.7).

Легкая часть заключается в выборе $W_N$, чтобы получить оценки ОММ с наименьшей асимптотической дисперсией при заданной функции $h(\cdot)$. Это часто называют оптимальным ОММ, хотя это ограниченная форма оптимальности, поскольку плохой выбор $h(\cdot)$ может всё же привести к очень неэффективным оценкам.

Для точно идентифицированных моделей одинаковые оценки (ММ оценки) получаются для любой матрицы весов полного ранга, то есть можно просто приравнять $W_N=I_q$.

Для сверх-идентифицированных моделей с $r>q$ и при известной $S_0$ наиболее эффективные оценки  ОММ получают путём выбора матрицы весов $W_N=S^{-1}_0$. Тогда ковариационная матрица, заданная в утверждении, упрощается и
\begin{equation}
\sqrt{N} (\hat{\theta}_{GMM}-\theta_0) \xrightarrow{d} \mathcal{N}[0,(G'_0 S^{-1}_0 G_0)^{-1}], 
\end{equation}
результат Хансена (1982).

Этот результат можно получить, используя аргументы, аналогичные тем, с помощью которых доказывалось, что оценка ОМНК является наиболее эффективной оценкой ВНМК в линейной модели. Ещё проще, можно работать непосредственно с целевой функцией. Для оценок МНК, которые минимизируют квадратичную форму $u'Wu$, наиболее эффективной оценка является оценка ОМНК, для которой $W=\Sigma^{-1}=\Var[u]^{-1}$. Целевая функция ОММ в (6.7) является такой же квадратичной формой с $u=N^{-1} \sum_i h_i(\theta)$ и поэтому оптимальная матрица $W$ $(\Var[N^{-1} \sum_i h_i(\theta)])^{-1}=S^{-1}_0$. Веса оптимальных ОММ оценок равны обратной ковариационной матрице условий выборочных моментов.

\begin{center}
Оптимальный ОММ
\end{center}

На практике $S_0$ неизвестна, и мы предполагаем $W_N={\hat{S}}^{-1}$, где $\hat{S}$ состоятельная оценка для $S_0$. Оптимальные оценки ОММ можно получить с помощью двухшаговой процедуры. На первом шаге оценки ОММ получают с использованием субоптимального выбора $W_N$, например, $W_N=I_r$ для простоты. С этого первого шага формируют оценку $\hat{S}$, используя (6.15). На втором этапе получают оптимальную оценку ОММ при применении оптимальной матрицы весов $W_N={\hat{S}}^{-1}$.

В этом случае оптимальные оценки ОММ или двухступенчатые оценки ОММ $\hat{\theta}_{OGMM}$, основанные на $h_i(\theta)$, минимизирует
\begin{equation}
\mathcal{Q}_{N}(\theta)=\left[ \frac{1}{N} \sum_{i=1}^{N} h_i(\theta) \right]' {\hat{S}}^{-1}  \left[ \frac{1}{N} \sum_{i=1}^{N} h_i(\theta) \right].
\end{equation}
Предельное распределение дано в (6.17). Оптимальная оценка ОММ является асимптотически нормально распределённой с математическим ожиданием $\theta_0$ и оценочной асимптотической ковариационной матрицей с относительно простой формулой:
\begin{equation}
\Var[\hat{\theta}_{OGMM}]=N^{-1}(\hat{G'} {\tilde{S}}^{-1} \hat{G})^{-1}.
\end{equation}

Обычно матрицы $\hat{G}$ и $\tilde{S}$ оцениваются в точке $\hat{\theta}_{OGMM}$, поэтому $\tilde{S}$ использует ту же формулу, что и $\hat{S}$, за исключением точки оценивания $\hat{\theta}_{OGMM}$. В качестве альтернативы можно считать (6.19) в точке, полученной на первом шаге оценки, поскольку любая состоятельная оценка для $\theta_0$ может быть использована.

Примечательно, что оптимальная оценка ОММ в (6.18) не требует никаких дополнительных стохастических предположений помимо тех, которые необходимы, чтобы использовать (6.16) для оценки ковариационной матрицы субоптимального ОММ. В обоих случаях $\hat{S}$ должна быть состоятельной оценкой $S_0$, и из обсуждения после (6.15) ясно, что для этого требуется несколько дополнительных предположений. Здесь видно значительное отличие от дополнительных предположений, необходимых для ОМНК, чтобы оценки были более эффективными, чем оценки МНК при гетероскедастичности. Однако гетероскедастичность в ошибках повлияет на оптимальный выбор $h_i(\theta)$(см. Раздел 6.3.7).

\begin{center}
Смещение двухступенчатого ОММ в небольших выборках 
\end{center}

Теория предполагает, что для сверх-идентифицированных моделей лучше всего использовать оптимальный ОММ. Однако при применении теоретически оптимальная матрица весов $W_N={S_0}^{-1}$ должна быть заменена на состоятельную оценку ${\hat{S}}^{-1}$. Эта замена не меняет асимптотики, но будет играть существенную роль в конечных выборках. В частности, отдельные наблюдения, которые увеличивают $h_i(\theta)$ в (6.18), вероятно, увеличивают $\hat{S}=N^{-1} \sum_i \hat{h}_i \hat{h'}_i$, что ведёт к корреляции между $N^{-1} \sum_i h_i(\theta)$ и $\hat{S}$. Следует отметить, что аналогичного эффекта на $S_0=\plim N^{-1} \sum_i h_i h'_i$ нет, потому что берётся предел по вероятности.

Алтони и Сигал (1996) продемонстрировали эту проблему для оценки моделей ковариационной
структуры с использованием панельных данных (см. Раздел 22.5). Они использовали соответствующие оценки минимального расстояния (см. раздел 6.7), но в литературе их результаты интерпретируются как относящиеся к ОММ с пространственными данными или короткими панелями. В моделировании оптимальная оценка была более эффективна, чем оценка одношагового метода, как и ожидалось. Тем не менее, у оптимальной оценки было смещение в конечной выборке настолько большое, что корень из среднеквадратической ошибки был гораздо больше, чем для оценки одного шага.

Алтони и Сигал (1996) также предложили вариант независимо взвешенной оптимальной оценки, которая формирует матрицу весов, используя наблюдения, отличные от тех, которые используются для построения выборочных моментов. Они разделили выборку на $G$ групп, например, подойдет простой выбор $G=2$, и минимизировали:
\begin{equation}
\mathcal{Q}_{N}(\theta)= \frac{1}{G} \sum_g h_g(\theta) {\hat{S}_{(-g)}}^{-1} h_g(\theta),
\end{equation}
где $h_g(\theta)$ вычисляется для $g$-ой группы и $\hat{S}_{(-g)}$ вычисляется с использованием всех групп, кроме $g$-ой. Эта оценка является менее смещённой, так как матрица весов ${\hat{S}_{(-g)}}^{-1}$ по построению независима от $h_g(\theta)$. Тем не менее, деление выборки приводит к потере эффективности. Горовиц (1998a) вместо этого использовал бутстрэп (см. Раздел 11.6.4). 

У Алтони и Сигала (1996) в примере в $h_i$ использовались вторые моменты, поэтому в $\hat{S}$ задействованы четвёртые моменты. Проблемы для оптимальной оценки в конечной выборке могут быть не столь значительными в других примерах, когда $h_i$ включает в себя только первые моменты. Тем не менее, результаты Алтони и Сигала действительно призывают к осторожности при использовании оптимального ОММ и что различия между оценкой одношагового ОММ и оптимальной оценки ОММ может указывать на проблемы смещения в оптимальном ОММ в конечной выборке.

\begin{center}
Количество ограничений на моменты
\end{center}

Как правило добавление дополнительных ограничений на моменты улучшает асимптотическую эффективность, так как это снижает предел дисперсии $(G'_0 S^{-1}_0 G_0)^{-1}$ оптимальной оценки ОММ или в худшем случае не меняет её.

Преимущества добавления дополнительных условий моментов меняются в зависимости от применения. Если, например, речь идёт об оценке ММП, то нет никакой выгоды, так как оценка ММП уже полностью эффективна. Много литературы посвящено оценке метода инструментальных переменных, где выгода может быть значительной, потому что переменная, которая является инструментальной, может быть гораздо более коррелированной с комбинацией многих инструментов, чем с одним инструментом.

Однако существует предел, так как количество моментных ограничений не может превышать число наблюдений. Кроме того, добавление нескольких условий моментов увеличивает вероятность смещения в конечной выборке и связанных с этим проблем, аналогичных таким, как слабые инструменты в линейных моделях (см. Раздел 4.9). Сток и другие (2002) кратко рассматривают слабые инструменты в нелинейных моделях.

\subsection{Пример регрессии с симметричными ошибками}

Чтобы продемонстрировать асимптотические результаты ОММ вернёмся к примеру дополнительных моментных ограничений, введённому в Разделе 6.2.4. Для этого примера целевая функция $\hat{\beta}_{GMM}$ была уже дана в (6.2). Всё, что требуется, это спецификация $W_N$ такая, как, например, $W_N=I$.

Чтобы получить распределение этой оценки, мы используем общие обозначения Раздела 6.3. Функция $h(\cdot)$ в (6.5) может быть представлена таким образом:
\[
h(x,y,\beta)= \begin{bmatrix} x(y-x'\beta)  \\ x(y-x'\beta)^3 \end{bmatrix} \Rightarrow \frac{\partial h(x,y,\beta)}{\partial \beta'}= \begin{bmatrix} -xx' \\ -3xx'(y-x'\beta)^2 \end{bmatrix}.
\]
Эти выражения ведут непосредственно к выражениям для $G_0$ и $S_0$, используя (6.9) и (6.12), и формулы (6.14) и (6.15) дают состоятельные оценки:
\begin{equation}
\hat{G}= \begin{bmatrix} -\frac{1}{N} \sum_i x_i x'_i \\ -\frac{1}{N} \sum_i 3 {\hat{u}_i}^2 x_i x'_i \end{bmatrix}
\end{equation}
и
\begin{equation}
\hat{S}= \begin{bmatrix} \frac{1}{N} \sum_i {\hat{u}_i}^2 x_i x'_i & \frac{1}{N} \sum_i {\hat{u}_i}^4 x_i x'_i \\ \frac{1}{N} \sum_i {\hat{u}_i}^4 x_i x'_i & \frac{1}{N} \sum_i {\hat{u}_i}^6 x_i x'_i \end{bmatrix},
\end{equation}
где $\hat{u}_i=y-x'_i \hat{\beta}$. Альтернативные оценки могут быть получены с помощью вычисления первых математических ожиданий в $G_0$ и $S_0$, но это потребует предположений о $\E[u^2|x]$, $\E[u^4|x]$ и $\E[u^6|x]$. Подстановка $\hat{G}$ и $\hat{S}$ и $W_N$ в (6.16) даёт оценку асимптотической ковариационной матрицы для $\hat{\beta}_{GMM}$.

Теперь рассмотрим ОММ с оптимальной матрицей весов. Снова минимизируется выражение (6.2), но из (6.18) теперь $W_N={\hat{S}}^{-1}$, где $\hat{S}$ определена в (6.22). Вычисление $\hat{S}$ требует состоятельной оценки $\hat{\beta}$ на первом шаге. Естественный выбор --- ОММ с $W_N=I$. 
В этом примере МНК-оценка также состоятельна и также могла бы быть использована. Использование (6.19) даёт двухступенчатую оценку с оценкой асимптотической ковариационной матрицы $\widehat{\Var}[\hat{\beta}_{OGMM}]$, равной
\[
\left( \begin{bmatrix} \sum_i \tilde{u}_i x_i x'_i \\ \sum_i {\tilde{u}_i}^3 x_i x'_i \end{bmatrix}' \begin{bmatrix} \sum_i {\tilde{u}_i}^2 x_i x'_i & \sum_i {\tilde{u}_i}^4 x_i x'_i \\  \sum_i {\tilde{u}_i}^4 x_i x'_i & \sum_i {\tilde{u}_i}^6 x_i x'_i \end{bmatrix}^{-1} \begin{bmatrix} \sum_i \tilde{u}_i x_i x'_i \\ \sum_i {\tilde{u}_i}^3 x_i x'_i \end{bmatrix} \right)^{-1},
\]
где $\tilde{u}_i=y-x'_i \hat{\beta}_{OGMM}$ и деления на $N$ сокращаются. Выигрыш эффективности оптимального ОММ в этом примере можно легко посчитать в нерегрессионном случае, где $y$ независимы и одинаково распределены с математическим ожиданием $\mu$. Предположим, что $y$ распределён по Лапласу с параметром масштаба, равным единице, в этом случае плотность $f(y)=(1/2) \times \exp \{-|y-\mu| \}$ c $\E[y]=\mu$, $\Var[y]=2$, и центральные моменты высших порядков $\E[(y-\mu)^{r}]$ равны нулю для нечётных $r$ и равны $r!$ для чётных $r$. Выборочная медиана является полностью эффективной, так как это оценка ММП, и может быть показано, что у неё асимптотическая дисперсия равна $1/N$. Выборочное среднее $\bar{y}$ неэффективно с дисперсией $\Var[\bar{y}]=\Var[y]/N=2/N$. Оптимальная оценка ОММ $\hat{\mu}^{opt}$, основанная на условиях двух моментов $\E[(y-\mu)]=0$ и $\E[(y-\mu)^3]=0$, имеет матрицу весов, которая даёт намного меньший вес условию второго момента, потому что оно имеет относительно высокую дисперсию. У матрицы весов будут отрицательные недиагональные элементы. Может быть показано, что у оптимальной оценки ОММ $\hat{\mu}_{OGMM}$ асимптотическая дисперсия равна $1.7143/N$ (см. упражнение 6.3). Поэтому она более эффективна, чем выборочное среднее (дисперсия $2/N$), хотя она всё ещё значительно менее эффективна, чем выборочная медиана.

Для этого примера единичная матрица является исключительно плохим выбором матрицы весов. Она даёт слишком большой вес условию второго момента, давая субоптимальную ОММ оценку для $\mu$  с асимптотической дисперсией $19.14/N$, что во много раз больше, чем даже $\Var[\bar{y}]=2/N$. Для получения дополнительной информации см. упражнение 6.3.

\subsection{Оптимальные моментные условия}

Раздел 6.3.5 даёт удивительный результат, что оптимальный ОММ требует по существу не больше предположений, чем в ОММ без оптимальный матрицы весов. Однако эта оптимальность очень ограничена, так как она зависит от выбора функции моментов $h(\cdot)$ в (6.5) или (6.18).

ОММ определяет класс оценок.  Различный выбор $h(\cdot)$, соответствует разным представителям этого класса. Некоторые функции $h(\cdot)$ лучше, чем другие, и зависит это от предположений о распределениях. Например, $h_i=x_i u_i$ даёт МНК-оценки, в то время как $h_i=x_i u_i / \Var[u_i|x_i]$ даёт оценку ОМНК, когда ошибки гетероскедастичны. Из-за множества потенциальных вариантов для $h(\cdot)$ может казаться, что любая ОММ оценка выбирается ad hoc. Тем не менее, качественно аналогичные решения должны быть сделаны и при выборе М-оценки, например, минимизировать сумму квадратов ошибок, или взвешенную сумму квадратов ошибок или сумму абсолютных отклонений ошибок.

Если сделаны полные допущения о распределении, наиболее эффективная оценка --- оценка ММП. Таким образом, оптимальный выбор $h(\cdot)$ в ( 6.5) --- это
\[
h(w,\theta)=\frac{ \partial \ln f(w,\theta)}{\partial \theta},
\]
где $f(w,\theta)$ является совместной плотностью $w$. Для регрессии с зависимой(ыми) переменной(ми) $y$ и регрессорами $x$ это оценка безусловного ММП на основе безусловной совместной плотности $f(y,x,\theta)$ $y$ и $x$. Во многих случаях $f(y,x,\theta)=f(y|x,\theta)g(x)$, где опущенные параметры частной функции плотности $x$ не зависят от параметров $\theta$, которые нас интересуют. Тогда так же эффективно использовать оценку условного ММП, основанную на условной плотности $f(y|x,\theta)$. На основе этой идеи можно построить ММ оценку или оценку ОММ с матрицей весов $W_N=I_q$, хотя любая матрица полного ранга также даст оценку ММП. Однако этот результат имеет ограниченное практическое применение, так как цель оценки ОММ заключается в том, чтобы избежать полного набора предположений о распределении.

При неполной спецификации распределении стандартной отправной точкой является спецификация уравнений для условных моментов, экзогенные переменные при этом полагаются фиксированными. Как правило, это условие на моменты низких порядков для ошибок модели, например, $\E[u|x]=0$ или $\E[u|z]=0$. Эти уравнения для условного момента могут привести к большому количеству уравнений для безусловных моментов, которые могут стать основой для оценки ОММ, например, $\E[zu]=0$. Ньюи (1990a, 1993) получил результаты касающиеся оптимального выбора уравнений для безусловного момента на данных независимых по $i$.

В частности, начнём с $s$ уравнений для условных моментов:
\begin{equation}
\E[r(y,x,\theta_0)|z]=0,
\end{equation}
где $r(\cdot)$ является векторной функцией, например, ошибок модели, размера $s \times 1$, которая была введена в Разделе 6.2.2. Скалярный пример --- $\E[y-x'\theta_0|z] = 0$. Мы используем обозначения принятые для инструментальных переменных, здесь $x$ --- это регрессоры, некоторые могут быть  эндогенными, а $z$ --- инструменты, включающие экзогенные компоненты $x$. В более простых моделях без эндогенности $z=x$.

Оценки ОММ $q$ параметров $\theta$, основанные на (6.23), невозможны, так как обычно существует только несколько ограничений условных моментов и часто только одно такое, что $s \le q$. Вместо этого, мы вводим матричную функцию инструментов $D(z)$ размера $r \times s$, где $r \ge q$, и необходимо обратить внимание, что по закону повторного математического ожидания $\E[D(z)r(y,x,\theta_0)]=0$. Эту идею можно использовать в качестве основы для оценки ОММ. Может быть показано, что оптимальные инструменты или оптимальный выбор матричной функции $D(z)$ --- это матрица размера $q \times s$:
\begin{equation}
D^*(z,\theta_0)=\E \left[ \frac{\partial r(y,x,\theta_0)'}{\partial \theta}|z \right] \{ \Var[r(y,x,\theta_0)|z]\}^{-1}.
\end{equation}
Вывод приведён, например, у Дэвидсона и МакКиннона (1993, с.604). Матрица оптимальных инструментов $D^*(z)$ --- это матрица размера $q \times s$, поэтому условие для безусловного момента $\E[D^*(z) r(x,y,\theta_0)]=0$ даёт ровно столько условий моментов, сколько параметров. Оптимальная оценка ОММ является решением соответствующих условий выборочных моментов:
\begin{equation}
\frac{1}{N} \sum_{i=1}^{N} D^*(z_i,\theta) r(y_i,x_i,\theta)=0.
\end{equation}

Оптимальная оценка требует дополнительных предположений, а именно математические ожидания, используемые при образовании $D^*(z,\theta_0)$ в ( 6.24), и реализация требуют замены неизвестных параметров на известные параметры, чтобы были использованы сгенерированные регрессоры $\hat{D}$.

Например, если $r(y,x,\theta)=y-\exp(x'\theta)$, то $\partial r / \partial \theta = -\exp(x'\theta)x$ и (6.24) требует спецификации $\E[\exp(x'\theta_0)x|z]$ и $\Var[y-\exp(x'\theta)|z]$. Одна из возможностей заключается в предположении, что $\E[\exp(x'\theta_0)x|z]$ является многочленом меньшего порядка по $z$, в этом случае будет больше условий моментов, чем параметров, и поэтому оценки являются оценками ОММ, а не просто результатом решения (6.25), и необходимо предположить гомоскедастичность ошибок. Если эти дополнительные предположения неверны, тогда оценка по-прежнему остаётся состоятельной при условии выполнения (6.23) и состоятельные стандартные ошибки можно получить с помощью скорректированной формы ковариационной матрицы в (6.16). Часто для простоты применяют $z$, а не $D^*(z,\theta)$ в качестве инструмента.

\begin{center}
Пример оптимальных моментных условий для нелинейной регрессии 
\end{center}

Результат (6.24) полезен в некоторых случаях, особенно в тех, где $z=x$. Здесь мы подтверждаем,
что оценка ОМНК является наиболее эффективной оценкой ОММ, основанной на $\E[u|x]=0$.

Рассмотрим нелинейную регрессионную модель $y=g(x,\beta)+u$. Если отправной точкой является ограничение условного момента $\E[u|x]=0$ или $\E[y-g(x,\beta)|x]=0$, то $z=x$ в (6.23) и (6.24) даёт
\[
D^*(x,\beta)=\E \left[ \frac{\partial}{ \partial \beta} (y- g(x,\beta_0))|x \right] \{ \Var[y-g(x,\beta_0)|x] \}^{-1}=-\frac{\partial g(x,\beta_0)}{\partial \beta} \times \frac{1}{\Var[u|x]},
\]
что требует лишь спецификации $\Var[u|x]$. Из (6.25) оптимальная оценка ОММ является решением соответствующего условия для выборочного момента
\[
\frac{1}{N} \sum_{i=1}^{N} - \frac{\partial g(x_i,\beta)}{\partial \beta} \times \frac{(y_i-g(x_i,\beta))}{{\sigma_i}^2}=0,
\]
где ${\sigma_i}^2=\Var[u_i|x_i]$ функционально независимы от $\beta$. Это условия первого порядка обобщённого НМНК, когда ошибки гетероскедастичны. Реализация возможна с использованием состоятельной оценки ${\hat{\sigma_i}}^2$ для ${\sigma_i}^2$, в таком случае оценка ОММ такая же, как оценка ДОНМНК. Можно получить стандартные ошибки, устойчивые к неправильной спецификации ${\hat{\sigma_i}}^2$, как описано в Разделе 5.8.

Для линейной модели, $g(x,\beta)=x'\beta$, оптимальная оценка ОММ, основанная на $\E[u|x]=0$, --- оценка ОМНК, при этом для случая гомоскедастичных ошибок, оптимальная оценка ОММ, основанная на $\E[u|x]=0$, --- оценка МНК. Как уже видно из примера в Разделе 6.3.6, более эффективные оценки могут быть получены, если  используются дополнительные условия для условных моментов.

\subsection{Тесты на сверх-идентифицирующие ограничения}

Тестирование гипотез для $\theta$ можно выполнить с помощью теста Вальда (см. Раздел 5.5) или с помощью других методов, приведённых в Разделе 7.5.

Кроме того, есть достаточно общий тест на спецификацию модели, который может быть использован для сверх-идентифицированных моделей с количеством моментных условий $(r)$ большим, чем параметров $q$. Тест является одним из тестов близости $N^{-1} \sum_i \hat{h}_i$ к 0, где $\hat{h}_i=h(w_i,\hat{\theta})$. Это естественный тест для гипотезы $H_0$: $\E[h(w,\theta_0)]=0$ об исходных условиях для теоретических моментов. Для точно идентифицированных моделей всегда выполнено условие $N^{-1} \sum_i \hat{h}_i=0$, поэтому невозможно провести тест. Для сверх-идентифицированных моделей, однако, условия первого порядка (6.8) устанавливают, что матрица размера $q \times r$, умноженная на $N^{-1} \sum_i \hat{h}_i$, равна нулю при $q < r$, поэтому $N^{-1} \sum_i \hat{h}_i \not= 0$.

В специальном случае, когда $\theta$ оценивается с помощью $\hat{\theta}_{OGMM}$, определённой в (6.18), Хансен (1982) показал, что статистика сверх-идентифицирующих ограничений (over identifying restrictions, OIR)
\begin{equation}
\text{OIR}= \left( N^{-1} \sum_i \hat{h}_i \right)' {\hat{S}}^{-1} \left( N^{-1} \sum_i \hat{h}_i \right)
\end{equation}
асимптотически распределена по $\chi^2(r-q)$ при $H_0: \E[h(w,\theta_0)]=0$. Обратите внимание, что OIR равна целевой функции ОММ (6.18), оценённой в $\hat{\theta}_{OGMM}$. Если OIR велика, то условия теоретических моментов отвергаются и оценки ОММ несостоятельны для $\theta$.

Априори не очевидно, что квадратичная форма $N^{-1} \sum_i \hat{h}_i$, приведённая в (6.26), распределена по $\chi^2(r-q)$ при $H_0$. Формальное доказательство приведено в следующем разделе, и интуитивное объяснение в случае оценки линейным методом инструментальных переменных приведено в Разделе 8.4.4.

Классическое применение --- модели жизненного цикла для потребления (см. Раздел 6.2.7), в этом случае условия ортогональности являются уравнениями Эйлера. Большое значение тестовой статистики хи-квадрат часто трактуют в пользу того, что гипотеза о жизненном цикле отвергается. Следует, однако, вместо этого более узко интерпретировать данный факт, как отвержение конкретной спецификации функции полезности и набора стохастических допущений, используемых в исследовании.

\subsection{Вывод оценки ОММ}

Выкладки можно упростить путём введения более компактных обозначений. Оценка ОММ минимизирует 
\begin{equation}
\mathcal{Q}_{N}(\theta)=g_N(\theta)'W_N g_N(\theta),
\end{equation}
где $g_N(\theta)=N^{-1} \sum_i h_i(\theta)$. Тогда условие первого порядка для ОММ из (6.8):
\begin{equation}
G_N(\hat{\theta})'W_N g_N(\hat{\theta})=0,
\end{equation}
где $G_N(\theta)= \partial g_N(\theta)/ \partial \theta'= N^{-1} \sum_i \partial h_i(\theta) / \partial \theta'$.

Для состоятельности мы рассматриваем неформальное условие, что предел по вероятности $\partial \mathcal{Q}_{N}(\theta) / \partial \theta|_{\theta_0}$ равен нулю. Из (6.28) оно будет следовать, так как $G_N(\theta_0)$ и $W_N$ имеют конечные пределы по вероятности по предположениям 3 и 4 в утверждении 6.1 и $\plim g_N(\theta_0)=0$ как следствие предположения
5. Более интуитивно $g_N(\theta_0)=N^{-1} \sum_i h_i(\theta)$ имеет предел по вероятности, равный 0, если закон больших чисел может быть применён и $\E[h_i(\theta_0)]=0$, что предполагалось в (6.5).

Параметр $\theta_0$ идентифицируется с помощью ключевого предположения 2 и дополнительно с помощью предположений 3 и 4, согласно которым пределы по вероятности $G_N(\theta_0)$ и $W_N$ должны быть матрицами полного ранга. Предположение, что $G_0=\plim G_N(\theta_0)$ является матрицей полного ранга, называется условием ранга для идентификации. Более слабое необходимое условие для идентификации --- условие порядка, что $r \ge q$.

Для асимптотической нормальности требуется более общая теория, чем для М-оценки, основанной на целевой функции $\mathcal{Q}_{N}(\beta)=N^{-1} \sum_i q(w_i,\theta)$, которая включает в себя только одну сумму. Мы меняем масштаб в (6.28) умножением на $\sqrt{N}$, чтобы
\begin{equation}
G_N(\hat{\theta})'W_N \sqrt{N} g_N(\hat{\theta})=0.
\end{equation}

Подход общей теоремы 5.3 --- взять разложение в ряд Тейлора в окрестности $\theta_0$ всей левой части (6.28). Поскольку $\hat{\theta}$ есть и в первом, и третьем члене, это является сложным и требует наличие первых производных $G_N(\theta)$ и, следовательно, вторых производных $g_N(\theta)$. Так как $G_N(\hat{\theta})$ и $W_N$  имеют конечные пределы по вероятности достаточно просто взять более точное разложение в ряд Тейлора только для $\sqrt{N} g_N(\hat{\theta})$. Мы получаем выражение, аналогичное тому, которое дано в Главе 5 при обсуждении М-оценки, с
\begin{equation}
\sqrt{N} g_N(\hat{\theta})= \sqrt{N} g_N(\theta_0)+G_N(\theta^+) \sqrt{N} (\hat{\theta}-\theta_0),
\end{equation}
где $G_N(\theta)=\partial g_N(\theta) / \partial \theta'$, а $\theta^+$ является точкой между $\theta_0$ и $\hat{\theta}$. Подстановка (6.30) обратно в (6.29) даёт следующее:
\[
G_N(\hat{\theta})' W_N \left[ \sqrt{N} g_N(\theta_0)+G_N(\theta^+) \sqrt{N} (\hat{\theta}-\theta_0) \right]=0.
\]
Выражаем $\sqrt{N} (\hat{\theta}-\theta_0)$:
\begin{equation}
\sqrt{N} (\hat{\theta}-\theta_0)=- \left[ G_N(\hat{\theta})' W_N G_N(\theta^+) \right] ^{-1} G_N(\hat{\theta})' W_N \sqrt{N} g_N(\theta_0).
\end{equation}

Уравнение (6.31) является ключевым результатом для получения предельного распределения оценки ОММ. Мы получаем пределы по вероятности каждого из первых пяти членов, используя $\hat{\theta} \xrightarrow{p} \theta_0$ при состоятельности, в этом случае $\theta^+ \xrightarrow{p} \theta_0$. Последний член в правой части (6.31) имеет предельное нормальное распределение по предположению 5. Таким образом,
\[
\sqrt{N} (\hat{\theta}-\theta_0) \xrightarrow{d} -(G'_0 W_0 G_0)^{-1} G'_0 W_0 \times \mathcal{N}[0,S_0],
\]
где $G_0$, $W_0$ и $S_0$ были определены в утверждении 6.1. Применение теоремы о нормальности предела  произведения (теорема А.17) даёт (6.11).

Для этих преобразований мы  рассматриваем условия первого порядка ОММ как $q$ линейных комбинаций $r$ выборочных моментов $g_N(\hat{\theta})$, так как $G_N(\hat{\theta})' W_N$ --- матрица размера $q \times r$. Оценка ММ является частным случаем при $q=r$, поскольку $G_N(\hat{\theta})' W_N$ является квадратной матрицей полного ранга, поэтому $G_N(\hat{\theta})' W_N g_N(\hat{\theta})=0$ требует, чтобы $g_N(\hat{\theta})=0$.

Для вывода распределения тестовой статистики OIR в (6.26) начнём с разложения первого порядка в ряд Тейлора $\sqrt{N} g_N(\hat{\theta})$ в окрестности $\theta_0$, чтобы получить
\[
\sqrt{N} g_N(\hat{\theta}_{OGMM}) = \sqrt{N} g_N(\theta_0)+G_N(\theta^+) \sqrt{N} (\hat{\theta}_{OGMM}-\theta_0)
\]
\[
= \sqrt{N} g_N(\theta_0)-G_0(G'_0 S^{-1}_0 G_0)^{-1} G'_0 S^{-1}_0 \sqrt{N} g_N(\theta_0) +o_{p}(1)
\]
\[
=[I-M_0 S^{-1}_0] \sqrt{N} g_N(\theta_0) +o_{p}(1),
\]
где второе равенство использует (6.31) с  $W_N$ состоятельной для $S^{-1}_0$, $M_0=G_0(G'_0 S^{-1}_0 G_0)^{-1} G'_0$ и $o_{p}(1)$ определяется в определении А.22. Отсюда следует, что 
\begin{equation}
\begin{matrix}
S^{-1/2}_0 \sqrt{N} g_N(\hat{\theta}_{OGMM})= S^{-1/2}_0 [I-M_0 S^{-1}_0] \sqrt{N} g_N(\theta_0) + o_{p}(1) \\
= [I-S^{-1/2}_0 M_0 S^{-1/2}_0 ] S^{-1/2}_0  \sqrt{N} g_N(\theta_0) + o_{p}(1).
\end{matrix}
\end{equation}
Теперь $[I-S^{-1/2}_0 M_0 S^{-1/2}_0 ] = [I-S^{-1/2}_0  G_0 (G'_0 S^{-1}_0 G_0)^{-1} G'_0 S^{-1/2}_0]$ является идемпотентной матрицей ранга $(r-q)$ и $S^{-1/2}_0 \sqrt{N} g_N(\theta_0) \xrightarrow{d} \mathcal{N} [0,I]$, если $\sqrt{N} g_N(\theta_0) \xrightarrow{d} \mathcal{N} [0,S_0]$. Из стандартных результатов для квадратичных форм нормальных переменных следует, что скалярное произведение
\[
\tau_N = (S^{-1/2}_0 \sqrt{N} g_N(\hat{\theta}_{OGMM}))'(S^{-1/2}_0 \sqrt{N} g_N(\hat{\theta}_{OGMM}))
\]
сходится к распределению $\chi^2(r-q)$.

\section{Линейный метод инструментальных переменных}

Корреляция регрессоров с ошибкой приводит к несостоятельности метода наименьших квадратов. Примерами появления такой корреляции являются пропущенные переменные, одновременность, погрешность измерения регрессоров и смещение выборки. Методы инструментальных переменных обеспечивают общий подход, который может работать с любой из этих проблем при условии существования подходящих инструментов.

Методы инструментальных переменных могут быть применены в рамках ОММ, так как избыток инструментов приводит к избытку моментных условий, которые могут быть использованы для оценивания. Многие результаты метода инструментальных переменных могут быть легче получены в рамках ОММ.

Линейный метод инструментальных переменных является достаточно важным, поэтому к нему мы будем обращаться во многих местах этой книги. Введение было дано в Разделах 4.8 и 4.9. В этом разделе представлен линейный метод инструментальных переменных с одним уравнением как пример конкретного применение ОММ. Для полноты раздел также представляет ранее описанный в литературе особый случай, оценку двухступенчатого метода наименьших квадратов. Линейный метод инструментальных переменных для систем уравнений приведен в Разделе 6.9.5. Тесты на эндогенность и сверх-идентифицирующие ограничения для линейных моделей подробно описаны в Разделе 8.4. Глава 22 представляет оценки линейного метода инструментальных переменных в панельных данных.

\subsection{Линейный ОММ с инструментами}

Рассмотрим модель линейной регрессии
\begin{equation}
y_i = x'_i \beta+ u_i, 
\end{equation}
где каждый элемент $x$ рассматривается как экзогенный регрессор, если он не коррелирует с ошибкой в модели (6.33), или как эндогенный регрессор, если он коррелирует. Если все регрессоры экзогенны, то могут быть использованы оценки МНК, но если хотя бы один регрессор $x$ является эндогенным, то оценки МНК несостоятельны для $\beta$.

Как сказано в Разделе 4.8, состоятельные оценки могут быть получены с помощью метода инструментальных переменных. Ключевым предположением является наличие вектора инструментов $z$ размера $r \times 1$, который удовлетворяет
\begin{equation}
\E[u_i|z_i]=0.
\end{equation}
Экзогенные регрессоры могут быть инструментами для самих себя. Поскольку должно быть по крайней мере столько же инструментов сколько регрессоров, задача состоит в том, чтобы найти дополнительные инструменты, количество которых по меньшей мере равны числу эндогенных переменных в модели. Некоторые примеры таких инструментов были приведены в Разделе 4.8.2.

\begin{center}
Оценки линейного ОММ
\end{center}

Согласно разделу 6.2.5, из ограничения на условный момент (6.34) и модели (6.33) следует ограничение для безусловного момента
\begin{equation}
\E[z_i(y_i-x'_i\beta)]=0,
\end{equation}
где для простоты обозначений мы далее используем обозначение $\beta$, а не более формальное обозначение $\beta_0$ для обозначения истинного значения параметра. Квадратичная форма для соответствующих выборочных моментах приводит к целевой функции ОММ $\mathcal{Q}_{N}(\beta)$, приведённой в (6.4).

В матричном виде определим $y=X \beta+u$, как обычно, и обозначим $Z$ матрицу инструментов размера $N \times R$ с $i$-ой строкой $z'_i$. Тогда $\sum_i z_i(y_i-x'_i\beta)=Z'u$, и (6.4) преобразуется до
\begin{equation}
\mathcal{Q}_{N}(\beta)= \left[ \frac{1}{N} (y-X\beta)'Z \right] W_N \left[ \frac{1}{N} Z'(y-X\beta) \right],
\end{equation}
где $W_N$ --- симметричная матрица весов полного ранга и размера $r \times r$. Основные примеры приведены в конце этого раздела. Условия первого порядка:
\[
\frac{\partial \mathcal{Q}_{N}(\beta)}{\partial \beta}=-2 \left[ \frac{1}{N} X'Z \right] W_N \left[ \frac{1}{N} Z'(y-X \beta) \right]=0
\] 
действительно могут быть решены относительно $\beta$ в этом частном случае ОММ, что приведёт к оценкам ОММ линейной модели инструментальных переменнных
\begin{equation}
\hat{\beta}_{GMM}=[X' Z W_N Z' X]^{-1} X' Z W_N Z' y,
\end{equation}
где деление на $N$ сократилось.

\begin{center}
Распределение линейной оценки ОММ
\end{center}

Общие результаты Раздела 6.3 могут быть использованы для получения асимптотического распределения. Кроме того, поскольку существует явное решение для $\hat{\beta}_{GMM}$, может быть адаптирован анализ для МНК, приведённый в Разделе 4.4. Подстановка $y=X\beta+u$ в ( 6.37 ) даёт
\begin{equation}
\hat{\beta}_{GMM}=\beta+[(N^{-1}X'Z) W_N (N^{-1}Z'X)]^{-1} (N^{-1}X'Z) W_N (N^{-1}Z'u).
\end{equation}

Из последнего члена состоятельность оценки ОММ обязательно требует, чтобы $\plim N^{-1}Z'u=0$. При полностью случайной выборке для этого необходимо условие (6.35), тогда как при других общих схемах проведения выборки (см. Раздел 24.3) требуется более сильное предположение (6.34).

Кроме того, условие ранга для идентифицируемости $\beta$, что $\plim N^{-1}Z'X$ имеет ранг $K$, гарантирует, что правая часть обратима при условии, что $W_N$ имеет полный ранг. Более слабым условием порядка является то, что $r \ge K$.

Предельное распределение основано на выражении для $\sqrt{N} (\hat{\beta}_{GMM}-\beta)$, полученном с помощью простого преобразования (6.38). Получаем асимптотическое нормальное распределение для $\hat{\beta}_{GMM}$ с математическим ожиданием $\beta$ и асимптотической ковариационной матрицей:
\begin{equation}
\widehat{\Var}[\hat{\beta}_{GMM}]=N[X' Z W_N Z'X]^{-1}[X' Z W_N \hat{S} W_N Z'X] [X' Z W_N Z'X]^{-1},
\end{equation}
где $\hat{S}$ является состоятельной оценкой:
\[
S= \lim \frac{1}{N} \sum_{i=1}^{N} \E[u^2_i z_i z'_i],
\]
при обычном в пространственных данных предположении о независимости по $i$. Необходимое дополнительное условие, нужное для (6.39), заключается в том, что $N^{-1/2}Z'u \xrightarrow{d} \mathcal{N}[0,S]$. Результат (6.39) также следует из утверждения 6.1 с $h(\cdot)=z(y-x'\beta)$ и, следовательно, $\partial h / \partial \beta'=-zx'$.

Для пространственных данных с гетероскедастичными ошибками, $S$ может быть состоятельно оценена с помощью:
\begin{equation}
\hat{S}=\frac{1}{N} \sum_{i=1}^{N} {\hat{u}_i}^2 z_i z'_i =Z'DZ/N,
\end{equation}
где $\hat{u}_i=y_i-x'_i \hat{\beta}_{GMM}$ --- остатки ОММ и $D$ --- диагональная матрица размера $N \times N$ с ${\hat{u}_i}^2$ по диагонали. Обычно используется поправка для небольших выборок --- деление на $N-K$, а не на $N$ в формуле $\hat{S}$.

В более простом случае гомоскедастичных ошибок, $\E[{\hat{u}_i}^2|z_i]={\sigma}^2$ и поэтому $S=\lim N^{-1} \sum_i {\sigma}^2 \E[z_i z'_i]$, что приводит к оценке
\begin{equation}
\hat{S}=s^2 Z'Z/N,
\end{equation}
где $s^2=(N-K)^{-1} \sum_{i=1}^{N} {\hat{u}_i}^2$ состоятельна для ${\sigma}^2$. Эти результаты очень похожи на результаты для МНК, представленные в Разделе 4.4.5.

\subsection{Различные оценки линейного ОММ}

Применение результатов Раздела 6.4.1 требует спецификации матрицы весов $W_N$. Для точно идентифицированных моделей любой выбор $W_N$ приводит к одной и той же оценке. Для сверх-идентифицированных моделей есть два распространённых выбора $W_N$, которые приведены в таблице 6.2.

Таблица 6.2 приводит эти оценки и даёт соответствующую специализацию формулы оценки ковариационной матрицы, приведённой в (6.39), считая ошибки независимыми и гетероскедастичными.

\begin{table}[h]
\begin{center}
\caption{\label{tab:GMMest} Оценки ОММ в линейной модели инструментальных переменных и оценки их асимптотических ковариационных матриц}
\begin{minipage}{\textwidth}
\begin{tabular}[t]{ll}
\hline
\hline
\bf{Оценка}\footnote{Уравнения основаны на линейной регрессионной модели с зависимой переменной $y$, регрессорами $X$ и инструментами $Z$. $\hat{S}$ определена в (6.40) и $s^2$ определяется в (6.41). Все оценки ковариационных матриц предполагают независимые и гетероскедастичные ошибки, без упрощения о гомоскедастичных ошибках, приведённого для двухшаговой МНК-оценки. Оптимальный ОММ использует оптимальную матрицу весов.} & \bf{Определение и оценка} \\
& \bf{асимптотической ковариационной матрицы} \\
\hline
ОММ & $\hat{\beta}_{GMM}= [X' Z W_N Z' X]^{-1} X' Z W_N Z' y$ \\
(общая $W_N$) & $\widehat{\Var}[\hat{\beta}]= N [X' Z W_N Z' X]^{-1} [X' Z W_N \hat{S} W_N Z' X] [X' Z W_N Z' X]^{-1}$ \\
Оптимальный ОММ & $\hat{\beta}_{OGMM}= [X' Z {\hat{S}}^{-1} Z' X]^{-1} X' Z {\hat{S}}^{-1} Z' y$ \\
$(W_N={\hat{S}}^{-1})$ & $\widehat{\Var}[\hat{\beta}]=N[X' Z {\hat{S}}^{-1} Z' X]^{-1}$ \\
Двухшаговый МНК & $\hat{\beta}_{2SLS}=[X' Z (Z' Z)^{-1} Z' X ]^{-1} X' Z (Z' Z)^{-1} Z' y$ \\
$(W_N=[N^{-1} Z' Z]^{-1})$ & $\widehat{\Var}[\hat{\beta}]=N[X' Z (Z' Z)^{-1} Z' X] ^{-1} [X' Z (Z' Z)^{-1} \hat{S} (Z' Z)^{-1} Z' X]$ \\ 
& $\times [X' Z (Z' Z)^{-1} Z' X]^{-1} $ \\
& $\widehat{\Var}[\hat{\beta}]=s^2  [X' Z (Z' Z)^{-1} Z' X]^{-1}$ \\
& \text{если ошибки гомоскедастичны}\\
Метод инструментальных & $\hat{\beta}_{IV}=[Z' X]^{-1}Z'y$\\ 
переменных & \\
(точно идентифицирована) & $\widehat{\Var}[\hat{\beta}]=N(Z'X)^{-1} \hat{S} (X' Z)^{-1}$\\
\hline
\hline
\end{tabular}
\end{minipage}
\end{center}
\end{table}

\begin{center}
Оценки метода инструментальных переменных 
\end{center}

В случае точной идентификации $r=K$ и $X'Z$ представляет собой квадратную матрицу, которая обратима. Тогда $[X' Z W_N Z' X]^{-1}=(Z'X)^{-1} {W_N}^{-1} (X'Z)^{-1}$, и (6.37) упрощается до оценки метода инструментальных переменных:
\begin{equation}
\hat{\beta}_{IV}=(Z'X)^{-1} Z'y,
\end{equation}
введённой в Разделе 4.8.6. Для таточно идентифицированных моделей оценки ОММ для любого
выбора $W_N$ равны оценкам метода инструментальных переменных.

Оценка простого метода инструментальных переменных также может быть использована в сверх-идентифицированной модели путём отбрасывания некоторых инструментов, модель при этом становится точно идентифицированной, но это приводит к потере эффективности по сравнению с использованием всех инструментов.

\begin{center}
Оптимальный взвешенный ОММ
\end{center}

Из раздела 6.3.5 для точно идентифицированных моделей наиболее эффективной будет оценка ОММ,  то есть ОММ с оптимальным выбором матрицы весов, $W_N={\hat{S}}^{-1}$ в (6.37).

Оценки оптимального ОММ или двухшагового ОММ в линейной модели метода инструментальных переменных:
\begin{equation}
\hat{\beta}_{OGMM}=[(X'Z){\hat{S}}^{-1} (Z'X)]^{-1} (X'Z){\hat{S}}^{-1} (Z'y).
\end{equation}

Для гетероскедастичных ошибок $\hat{S}$ вычисляется с использованием (6.40), с использованием состоятельной оценки первого шага $\hat{\beta}$ такой, как двухшаговый МНК, определённый в (6.44). Уайт (1982) назвал эту оценку двухшаговой оценкой метода инструментальных переменных, так как оба шага влекут за собой оценки метода инструментальных переменных.

Оценки асимптотический ковариационной матрицы для оптимального ОММ, приведённые в таблице 6.2, имеют относительно простую форму, поскольку (6.39) упрощается, когда $W_N={\hat{S}}^{-1}$. При расчёте оценки ковариационной матрицы можно использовать $\hat{S}$, представленную в таблице 6.2, но более распространённым является использование вместо этого оценки $\tilde{S}$, которая также вычисляется, используя (6.40), но оценивает остатки при оптимальной оценке ОММ, а не оценке первого шага, используемой для формирования $\hat{S}$ в (6.43).

\begin{center}
Двухшаговый метод наименьших квадратов
\end{center}

Если ошибки гомоскедастичны, а не гетероскедастичны, ${\hat{S}}^{-1}=[s^2 N^{-1} Z' Z]^{-1}$ из (6.41). Тогда $W_N=(N^{-1}Z'Z)^{-1}$ в ( 6.37), что приводит к оценке двухшагового метода наименьших квадратов, введённой в разделе 4.8.7, которая может быть выражена более компактно как:
\begin{equation}
\hat{\beta}_{2SLS}=[X' P_{Z} X]^{-1} [X' P_{Z} y],
\end{equation}
где $P_{Z}=Z(ZZ')^{-1}Z'$. Причина названия двухшаговый метод наименьших квадратов представлена в следующем разделе. Оценки двухшагового МНК также называются оценками обобщённого метода инструментальных переменных, поскольку они обобщают оценки метода инструментальных переменных на случай сверх-идентификации при большем числе инструментов, чем регрессоров. Его также называют одношаговым ОММ, потому что (6.44) может быть посчитано в один шаг, тогда как оптимальный ОММ требует двух шагов.

Оценка двухшагового МНК распределена асимптотически нормально с оценкой асимптотической ковариационной матрицы, приведённой в таблице 6.2. Общий вид следует использовать, если мы хотим защититься от гетероскедастичных ошибок, в то время как более простой вид, представленный во многих вводных учебниках, состоятелен, только если ошибки действительно гомоскедастичны.

\begin{center}
Оптимальный ОММ и двухшаговый МНК: сравнение
\end{center}

И оценки оптимального ОММ, и оценки двухшагового МНК приводят к повышению эффективности в сверх-идентифицированных моделях. Оптимальный ОММ имеет то преимущество, что он более эффективен, чем двухшаговый МНК, если ошибки гетероскедастичны, хотя повышение эффективности не должно быть большим. Некоторые из процедур тестирования ОММ приведены в Разделе 7.5, и Глава 8 предполагает оценки c использованием оптимальной матрицы весов. Недостаток оптимального ОММ состоит в том, что он требует дополнительных вычислений по сравнению с двухшаговым МНК. Более того, как обсуждалось в Разделе 6.3.5, асимптотический подход может привести к более плохой аппроксимации для малых выборок оптимальной оценки ОММ.

В пространственных данных обычно используют менее эффективные оценки двухшагового МНК, хотя и с использованием устойчивых к гетероскедастичности стандартных ошибок.

\begin{center}
Ещё более эффективная оценка ОММ
\end{center}

Оценка $\hat{\beta}_{OGMM}$ является наиболее эффективной оценкой, основанной на безусловном моментном уравнении $\E[z_i u_i]=0$, где $u_i=y_i-x'_i\beta$. Однако это не лучшее моментное условие, если отправной точкой является уравнение для условного момента $\E[u_i|z_i]=0$ и ошибки гетероскедастичны, то есть $\Var[u_i|z_i]$ меняется с $z_i$.

Применяя общие результаты Раздела 6.3.7, мы можем написать оптимальное моментное условие для оценки ОММ, основанное на $\E[u_i|z_i]=0$, поскольку
\begin{equation}
\E[\E[x_i|z_i]u_i/ \Var[u_i|z_i]]=0.
\end{equation}
Как и в примере регрессии МНК в Разделе 6.3.7, следует разделить на дисперсию ошибки $\Var[u|z]$. Однако реализация гораздо сложнее, чем в случае МНК, поскольку модель для $\E[x|z]$ необходимо указать в дополнение к модели для $\Var[u|z]$. Для этого требуется указание новой структуры. В частности линейная система одновременных уравнений $\E[x_i|z_i]$ линейна по $z$, поэтому оценивание основывается на $\E[x_i u_i/\Var[u_i|z_i]]=0$.

Для линейных моделей оценки ОММ, как правило, основаны на более простом условии $\E[z_i u_i]=0$. При этом условии оптимальные оценки ОММ, определённые в (6.43), являются наиболее эффективными оценками ОММ.

\subsection{Альтернативные подходы к двухшаговому МНК}

Оценка двухшагового МНК, стандартная оценка метода инструментальных переменных для сверх-идентифицированных моделей, была получена в Разделе 6.4.2 как оценка ОММ.

Здесь мы представляем три других способа получить оценки двухшагового МНК. Один из этих способов, придуманный Тейлом, содержит исходную мотивацию  двухшагового МНК, предшествующую ОММ. Подход Тейла обычно излагается во вводных курсах. Тем не менее, он не обобщается на нелинейные модели, в то время как ОММ подход обобщается.

Рассмотрим линейную модель:
\begin{equation}
y=X \beta+u,
\end{equation}
с $\E[u|Z]=0$ и, кроме того $\Var[u|Z]=\sigma^2 I$.

\begin{center}
ОМНК в преобразованной модели
\end{center}

Умножение (6.46) на инструменты $Z'$ даёт преобразованную модель
\begin{equation}
Z'y=Z'X \beta+Z'u.
\end{equation}
Эта преобразованная модель часто используется в качестве мотивации для оценки метода инструментальных переменных при $r=K$, поскольку опускается $Z'u$, так как $N^{-1} Z'u \rightarrow 0$ и решение даёт $\hat{\beta}=(Z'X)^{-1}Z'y$.

Здесь вместо этого мы рассмотрим сверх-идентифицированный случай. Зависимые от $Z$ ошибки $Z'u$ имеют нулевое математическое ожидание и дисперсию $\sigma^2 Z'Z$ при предположении (6.46). Эффективная оценка ОМНК $\beta$ в модели (6.46) тогда имеет вид:
\begin{equation}
\hat{\beta}=[X' Z (\sigma^2 Z'Z)^{-1} Z' X]^{-1} X' Z (\sigma^2 Z'Z)^{-1} Z' y,
\end{equation}
которая равна оценке двухшагового МНК в (6.44), так как множители $\sigma^2$ сокращаются. В целом, отметим, что если преобразованная модель (6.47) вместо этого оценивается с помощью ВМНК с матрицей весов $W_N$, то более общая оценка (6.37) может быть получена.

\begin{center}
Интерпретация Тейла 
\end{center}

Тейл (1953) предложил получать оценку с помощью регрессии МНК в первоначальной модели (6.46) за исключением того, что регрессоры $X$ заменяются на предсказанные $\hat{X}$, которое асимптотически не коррелирует с остаточным членом.

Предположим, что в приведенной форме модели регрессоры $X$ являются линейной комбинацией инструментов плюс некоторые ошибки, 
\begin{equation}
X = Z \Pi + v,
\end{equation}
где $\Pi$  --- матрица размера $K \times r$. Многомерная регрессия МНК $X$ на $Z$ даёт оценки  $\hat{\Pi}=(Z' Z)^{-1} Z' X$ и прогноз МНК $\hat{X}=Z\hat{\Pi}$ или
\[
\hat{X}= P_{Z} X,
\]
где $P_{Z}=Z(Z' Z)^{-1} Z'$. МНК регрессия $y$ на $\hat{X}$, а не $y$ на $X$ даёт оценку:
\begin{equation}
\hat{\beta}_{Theil}=(\hat{X'}\hat{X})^{-1} \hat{X'}y.
\end{equation}
Интерпретации Тейла даёт возможность строить две регрессии обычным МНК. На первом этапе МНК даёт $\hat{X}$ и на втором шаге МНК даёт $\hat{\beta}$, что и приводит к термину оценка двухшагового метода наименьших квадратов.

Для проверки состоятельности этой оценки преобразуем линейную модель (6.46) как
\[
y= \hat{X} \beta+(X-\hat{X}) \beta + u,
\]

Второй шаг МНК регрессии $y$ на $X$ даёт состоятельную оценку $\beta$, если регрессоры $\hat{X}$ асимптотически не коррелируют с составными ошибками $(X-\hat{X}) \beta + u$. Если $\hat{X}$ --- любые прокси-переменные, то нет никакой причины для отсутствия корреляции, однако здесь $\hat{X}$ не коррелирует с $(X-\hat{X})$, поскольку МНК прогнозы ортогональны остаткам МНК. Таким образом, $\plim N^{-1} \hat{X'}(X-\hat{X})\beta=0$. Кроме того,
\[
N^{-1} \hat{X'} u= N^{-1} X' P_{Z} u = N^{-1} X' Z (N^{-1} Z' Z)^{-1} N^{-1} Z' u.
\]
Тогда $\hat{X}$ асимптотически не коррелирует с $u$ при условии, что $Z$ является хорошим инструментом, поэтому $N^{-1}Z'u=0$. Этот результат состоятельности для $\hat{\beta}_{Theil}$ сильно зависит от линейности модели и не может быть обобщён на нелинейные модели.

Оценки Тейла в (6.50) равны оценкам двухшагового МНК, определённым ранее в (6.44). Мы получаем
\[
\hat{\beta}_{Theil}=(\hat{X'}\hat{X})^{-1} \hat{X'}y
\]
\[
=(X' P'_{Z} P_{Z} X)^{-1} X' P_{Z} y 
\]
\[
=(X' P_{Z} X)^{-1} X' P_{Z} y, 
\]
оценку двухшагового МНК, используя $P'_{Z} P_{Z}=P_{Z}$ в конечном равенстве.

Необходимо проявлять осторожность в реализации двухшагового МНК методом Тейла. На втором этапе МНК ошибки будут иметь неправильные стандартные ошибки, даже если ошибки гомоскедастичны, поскольку будет оцениваться $\sigma^2$ с помощью использования остатков из регрессии МНК на втором шаге $(y-\hat{X} \hat{\beta})$, а не фактических остатков $(y- X \hat{\beta})$. На практике можно также сделать поправку на гетероскедастичность. Гораздо проще использовать программу, которая предлагает двухшаговый МНК в качестве опции и непосредственно вычисляет (6.44) и связанные с ними ковариационные матрицы, приведённые в таблице 6.2.

Интерпретация двухшагового МНК не всегда может быть перенесена на нелинейные модели, как описано в Разделе 6.5.4. Интерпретация ОММ позволяет это сделать, и поэтому на неё здесь делается больший акцент, чем на первоначальный вывод Тейла линейного двухшагового МНК.

Тейл на самом деле рассматривал модель, в которой только некоторые регрессоры $X$ являются эндогенными и все оставшиеся экзогенны. Предшествующий анализ всё ещё может быть применён, если все экзогенные компоненты $X$ включены в инструменты $Z$. Тогда МНК регрессия первого шага экзогенных регрессоров на инструменты даёт нулевые ошибки, и предсказания экзогенных регрессоров равняются их фактическим значениям. Таким образом, на практике на первом шаге только эндогенные переменные регрессируются на инструменты, а на втором шаге строится регрессия $y$ на экзогенные регрессоры и предсказания эндогенных регрессоров из первого шага.

\begin{center}
Интерпретация Басманна
\end{center}

Басманн (1957) предложил использовать в качестве инструментов прогнозы из модели МНК  $\hat{X}=P_{Z} X$  в  точно идентифицированном случае, поскольку тогда есть ровно столько инструментов $\hat{X}$, сколько регрессоров $X$. Мы получаем
\begin{equation}
\hat{\beta}_{Basmann}=(\hat{X'} X)^{-1} \hat{X'} y.
\end{equation}
Эта оценка состоятельна, так как $\plim N^{-1} \hat{X'} u =0$, как уже было показано для оценки Тейла. 

Оценка (6.51) фактически равна оценке двухшагового МНК, определённого в (6.44), так как $\hat{X'}=X' P_{Z}$.

Этот подход инструментальных переменных приведёт к правильным стандартным ошибкам и может быть расширен на нелинейные модели.

\subsection{Альтернативы стандартным оценкам метода инструментальных переменных}

Оценки оптимального ОММ, основанного на методе инструментальных переменных, и оценки двухшагового МНК, представленные в Разделе 6.4.2, --- это стандартные оценки, используемые при эндогенных регрессорах. Черножуков и Хансен (2005) представили оценки метода инструментальных переменных для квантильной регрессии.

Здесь мы кратко обсудим основные альтернативные оценки, которые вновь привлекли внимание, учитывая плохие свойства в конечной выборке двухшагового МНК со слабыми инструментами, подробно описанного в Разделе 4.9. Мы концентрируемся на линейных моделях с одним уравнением. На данном этапе не существует метода, который является относительно эффективным, и имеет небольшое смещение в малых выборках.

\begin{center}
Метод максимального правдоподобия с ограниченной информацией
\end{center}

Оценки метода максимального правдоподобия с ограниченной информацией (Limited Information ML, LIML) получают путём совместной оценки ММП одного уравнения (6.46), и редуцированной формы для эндогенных регрессоров в правой части (6.46) в предположении гомоскедастичных нормально распределённых ошибок. Деталльное обсуждение приводят Грин (2003, с.402) или Дэвидсон и МакКиннон (1993, с.644-651). В более общем случае $k$ класс оценок  (см., например, Грин, 2003, с.403) включает в себя оценки LIML, двухшагового МНК и МНК.

Оценки LIML предложенные Андерсону и Рубину (1949) появились раньше двухшагового МНК. В отличие от оценок двухшагового МНК оценки LIML инвариантны к нормализации, используемой в системе одновременных уравнений. Более того, оценки LIML и двухшагового МНК асимптотически эквивалентны при гомоскедастичных ошибках. Тем не менее, оценки LIML используются редко, поскольку этот метод более трудно осуществить и труднее объяснить, чем двухшаговый МНК. Беккер (1994) представляет результаты LIML для небольших выборок и обобщение LIML. См. также Ган и Хаусман (2002).

\begin{center}
Метод инструментальных переменных с делением выборки
\end{center}

Начнём с интерпретации Басманна оценки двухшагового МНК как оценки метода инструментальных переменных, представленной в (6.51). Подставив $y$ из (6.46), получим
\[
\hat{\beta}=\beta+(\hat{X'} X)^{-1} \hat{X'} u.
\]
По предположению $\plim N^{-1} Z' u=0$, поэтому $\plim  N^{-1} \hat{X'} u=0$ и $\hat{\beta}$ состоятельна. Однако корреляция между $X$ и $u$, причина выбора метода инструментальных переменных, означает, что $\hat{X}=P_{Z} X$ коррелирует с $u$. Таким образом, $\E[\hat{X'}u] \not= 0$, что приводит к смещению инструментальной оценки. Это смещение
возникает из-за использования $\hat{X}=Z \hat{\Pi}$, а не $\hat{X}=Z \Pi$ в качестве инструмента.

Вместо этого  можно использовать в качестве инструментов прогнозы $\tilde{X}$, у которых есть свойство, что $\E[\tilde{X'}u] = 0$ в дополнение к $\plim  N^{-1} \hat{X'} u=0$ и использовать оценку
\[
\tilde{\beta}=(\tilde{X'}X)^{-1} \tilde{X'} y.
\]
Поскольку $\E[\tilde{X'}u]=0$ не означает, что $\E[(\tilde{X'}X)^{-1} \tilde{X'} u]=0$, эта оценка всё равно будет смещённой, но смещение можно уменьшить.

Ангрист и Крюгер (1995) предложили получение таких инструментов путём деления выборки на две подвыборки $(y_1,X_1,Z_1)$ и $(y_2,X_2,Z_2)$. Первая выборка используется для получения оценки  $\hat{\Pi}$ из регрессии $X_1$ на $Z_1$. Вторая выборка используется для получения оценки метода инструментальных переменных, где инструмент $\tilde{X}_2=Z_2 \hat{\Pi}_1$ использует $\hat{\Pi}_1$, полученную из отдельной первой выборки. Ангрист и Крюгер (1995) определяют несмещённую оценку метода инструментальных переменных с делением выборки как:
\[
\tilde{\beta}_{USSIV}=(\tilde{X}_2' X_2)^{-1} \tilde{X}_2' y_2.
\]
Оценки метода инструментальных переменных с делением выборки $\tilde{\beta}_{USSIV}=(\tilde{X}_2' X_2)^{-1} \tilde{X}_2' y_2$ --- вариант, основанный на интерпретации Тейла двухшагового МНК. Эти оценки имеют смещение в конечной выборке в сторону нуля, в отличие от оценки двухшагового МНК, которая смещена по направлению к МНК. Однако есть значительные потери эффективности, поскольку только половина выборки используется на заключительном этапе.

\begin{center}
Джекнайф метод инструментальных переменных 
\end{center}

Более эффективный вариант этой оценки применяет аналогичную процедуру, но генерирует инструменты наблюдение за наблюдением.

Пусть индекс $(-i)$ обозначает выбрасывание одного $i$-го наблюдения. Тогда для $i$-ого наблюдения получаем оценку $\hat{\Pi}_i$ из регрессии $X_{(-i)}$ на $Z_{(-i)}$ и будем использовать в качестве инструмента $\tilde{x}'_i=z'_i \hat{\Pi}_i$. Повтор $N$ раз даёт вектор инструментов, который обозначают как $\tilde{X}_{(-i)}$ с $i$-ой строчкой $\tilde{x}_i'$. Таким образом мы получаем джекнайф оценку  метода инструментальных переменных:
\[
\tilde{\beta}_{JIV}=(\tilde{X'}_{(-i)}X)^{-1} \tilde{X'}_{(-i)} y_2.
\]

Эта оценка была первоначально предложена Филлипсом и Хейлом (1977). Ангрист, Имбенц и Крюгер (1999) и Бломквист и Дальберг (1999) называли её джекнайф оценкой, она (см. Раздел 11.5.5) является методом кросс-валидации с исключением отдельных наблюдений для снижения смещения. Вычисления требующиеся для получения $N$ предсказанных значений $\tilde{x'}_i$ методом джекнайф небольшие при использовании рекурсивной формулы, приведённой в Разделе 11.5.5. Результаты экспериментов Монте-Карло, приведённые в двух последних работах является неоднозначными, они указывают на возможность уменьшения смещения, но и на риск  увеличения дисперсии. Таким образом, метод джекнайф может быть не лучше, чем обычная версия с точки зрения среднеквадратичной ошибки. Предыдущая работа Филлипса и Хейла (1977) представляет аналитические результаты, что смещение в конечной выборке оценки метода инструментальных переменных джекнайф меньше, чем у оценки двухшагового МНК только для сверх-идентифицированных моделей с $r > 2(K+1)$. См. также Хана, Хаусмана и Кьерштейнера (2001).

\begin{center}
Независимо взвешенный двухшаговый МНК
\end{center}

Методом, близким к методу инструментальных переменных с делением выборки, является независимо взвешенный ОММ Алтоньи и Сигала (1996), приведённый в Разделе 6.3.5. Разделение выборки на $G$ групп и специализация на линейном методе инструментальных переменных даёт оценку независимо взвешенного метода инструментальных переменных:
\[
\hat{\beta}_{IWIV}=\frac{1}{G} \sum_{g=1}^{G} [X'_g Z_g {\hat{S}_{(-g)}}^{-1} Z'_g X_g]^{-1} X'_g Z_g {\hat{S}_{(-g)}}^{-1} Z'_g y_g,
\]
где $\hat{S}_{(-g)}$ вычисляется с использованием $\hat{S}$, определённой в (6.40) за исключением того, что наблюдения из $g$-ой группы исключаются. В применении к панелям Зилиак (1997) обнаружил, что оценка независимо взвешенного метода инструментальных переменных даёт результаты, которые намного лучше, чем несмещённые оценки метода инструментальных переменных с делением выборки.

\section{Нелинейный метод инструментальных переменных}
 
Нелинейные методы инструментальных переменных, в частности нелинейный двухшаговый МНК, предложенный Амэмия (1974), дают состоятельные оценки нелинейных регрессионных моделей в ситуациях, когда оценки НМНК не состоятельны, потому что регрессоры коррелируют с остаточным членом. Мы приводим эти методы, как прямое расширение подхода ОММ для линейных моделей.

В отличие от линейного случая оценки не имеют явной формулы, но асимптотическое распределение может быть получено как частный случай результатов Раздела 6.3. В этом разделе представлены результаты с одним уравнением, а результаты для систем уравнений приведены в Разделе 6.10.4. Фундаментально важным результатом является то, что естественное расширение двухшагового МНК Тейла для линейных моделей на нелинейные модели может привести к несостоятельным оценкам параметров (см. Раздел 6.5.4). Вместо этого должен быть использован подход ОММ.

Альтернативная нелинейность может возникнуть, когда модель для зависимой переменной является линейной моделью, но приведенная форма для эндогенных регрессоров является нелинейной моделью благодаря особенностям зависимой переменной. Например, эндогенный регрессор может быть счётным или бинарным. В этом случае линейные методы предыдущего раздела применимы. Один из подходов заключается в том, чтобы игнорировать особый характер эндогенного регрессора и просто делать регулярный линейный двухшаговый МНК или оптимальный ОММ. Кроме того, получить оценённые значения эндогенных регрессоров с помощью соответствующей нелинейной регрессии такой, как регрессия Пуассона на все инструменты, если эндогенный регрессор является счётным, а затем делать регулярный линейный метод инструментальных переменных с использованием этих оценённых значений в качестве инструмента для счётной переменной, следуя подходу Басманна. Обе оценки состоятельны, хотя они имеют различные асимптотические распределения. Первый более простой подход является более распространённым.

\subsection{Нелинейный ОММ с инструментами}

Рассмотрим достаточно общую нелинейную регрессионную модель, где ошибка может быть аддитивной или неаддитивной (см. Раздел 6.2.2). Таким образом,
\begin{equation}
u_i=r(y_i,x_i,\beta),
\end{equation}
где нелинейная модель с аддитивной ошибкой является частным случаем
\begin{equation}
u_i=y_i-g(x_i,\beta),
\end{equation}
где $g(\cdot)$ является заданной функцией. Оценки, приведённые в Разделе 6.2.2, являются несостоятельными, если $\E[u_i|x_i] \not=0$.

Предположим существование $r$ инструментов $z$, где $r \ge K$, которые удовлетворяют
\begin{equation}
\E[u_i|z_i]=0.
\end{equation}
Это то же самое уравнение для условного момента, как и в линейном случае, за исключением того, что $u_i=r(y_i,x_i,\beta)$, а не $u_i=y_i-x'_i\beta$.

\begin{center}
Оценки нелинейного ОММ
\end{center}

По закону повторного математического ожидания, (6.54) приводит к
\begin{equation}
\E[z_i u_i]=0.
\end{equation}
Оценка ОММ минимизирует квадратичную форму для соответствующих условий выборочного момента.

В матричном виде пусть $u$ обозначает вектор ошибок размера $N \times 1$ с $i$-ым элементом равным $u_i$, приведённом в (6.52) и пусть $Z$ --- матрица инструментов размера $N \times r$ с $i$-ой строчкой $z'_i$. Тогда $\sum_i z_i u_i =Z'u$ и оценка ОММ в нелинейной модели метода инструментальных переменных $\hat{\beta}_{GMM}$ минимизирует
\begin{equation}
\mathcal{Q}_{N}(\beta)= \left( \frac{1}{N} u'Z \right) W_N \left( \frac{1}{N} Z'u \right),
\end{equation}
где $W_N$ является матрицей весов размера $r \times r$. В отличие от линейного ОММ условия первого порядка не приводят к явному решению для $\hat{\beta}_{GMM}$.

\begin{center}
Распределение оценок нелинейного ОММ
\end{center}

Оценка ОММ состоятельна для $\beta$, заданного в (6.54), и асимптотически нормально распределена с оценкой асимптотической ковариационной матрицы:
\begin{equation}
\widehat{\Var}[\hat{\beta}_{GMM}]=N[\hat{D'} Z W_N Z' \hat{D}]^{-1} [\hat{D'} Z W_N \hat{S}_N  W_N Z' \hat{D}] [\hat{D'} Z W_N Z' \hat{D}]^{-1} 
\end{equation}
Здесь мы использовали результаты из Раздела 6.3.3 с $h(\cdot)=zu$, где $\hat{S}$ приведена далее и $\hat{D}$ является матрицей производных случайной ошибки размера $N \times K$ 
\begin{equation}
\hat{D}= \left. \frac{\partial u}{\partial \beta'} \right|_{\hat{\beta}_{GMM}}.
\end{equation}
С неаддитивными ошибками  $i$-ая строка матрицы $\hat{D}$ имеет вид $\partial r(y_i,x_i,\beta)/ \partial \beta'|_{\hat{\beta}}$. С аддитивными ошибками $i$-ая строка имеет вид  $\partial g(x_i,\beta)/ \partial \beta'|_{\hat{\beta}}$, знак минуса сокращается в (6.57).

Для независимых гетероскедастичность ошибок
\begin{equation}
\hat{S}=N^{-1} \sum_i {\hat{u}_i}^2 z_i z'_i,
\end{equation}
что похоже на линейный случай, только теперь $\hat{u}_i=r(y_i,x_i,\hat{\beta})$ или $\hat{u}_i=y_i-g(x,\hat{\beta})$.

Поэтому оценка асимптотической ковариационной матрицы оценки ОММ в нелинейной модели такая же, как и в линейном случае, приведённом в (6.39), с отличием в том, что матрица регрессоров $X$ заменяется на производную $\partial u / \partial \beta'|_{\hat{\beta}}$. Это точно такое же изменение, как в Разделе 5.8 при переходе от линейного к нелинейному методу наименьших квадратов. По аналогии с линейным методом инструментальных переменных условие рангов для идентификации состоит в том, что матрица $\plim N^{-1} Z' \partial u / \partial \beta'|_{\beta_0}$ имеет ранг $K$ и более слабое условие порядка --- $r \ge K$.

\subsection{Различные оценки нелинейных ОММ}

Две основных оценки ОММ, отличающиеся выбором матрицы весов, --- это оптимальный ОММ с $W_N={\hat{S}}^{-1}$, и оценка нелинейного двухшагового МНК с $W_N=(Z'Z)^{-1}$. В Таблице 6.3 приведены эти оценки и связанные с ними оценки ковариационных матриц, в предположении независимых гетероскедастичных ошибок, и приведены результаты для общей $W_N$ и результаты для нелинейного метода инструментальных переменных в точно идентифицированной модели.

\begin{table}[h]
\begin{center}
\caption{\label{tab:GMMestnl} Оценки ОММ в нелинейной модели инструментальных переменных и оценки их асимптотических ковариационных матриц}
\begin{minipage}{\textwidth}
\begin{tabular}[t]{ll}
\hline
\hline
\bf{Оценка}\footnote{Уравнения основаны на нелинейной регрессионной модели c ошибкой $u$, определённой в (6.53) или (6.52), и инструментами $Z$. $\hat{D}$ --- производная вектора ошибок по $\beta'$ в точке $\hat{\beta}$ и может быть упрощена для моделей с аддитивными ошибками до вида производной функции условного математического ожидания по $\beta'$ в точке $\hat{\beta}$. $\hat{S}$ определена в (6.59). Все оценки ковариационных матриц построены для ошибок, которые являются независимыми и гетероскедастичными, кроме упрощения до гомоскедастичных ошибках для оценки нелинейного двухшагового МНК.} & \bf{Определение и оценка} \\
& \bf{асимптотической ковариационной матрицы} \\
\hline
ОММ & $\mathcal{Q}_{GMM}(\beta)= u' Z W_N Z' u$ \\
(общая $W_N$) & $\widehat{\Var}[\hat{\beta}]= N [\hat{D'} Z W_N Z' \hat{D}]^{-1} [\hat{D'} Z W_N \hat{S} W_N Z' \hat{D}] [\hat{D'} Z W_N Z' \hat{D'}]^{-1}$ \\
Оптимальный ОММ & $\mathcal{Q}_{OGMM}(\beta)= u' Z {\hat{S}}^{-1} Z' u $ \\
$(W_N={\hat{S}}^{-1})$ & $\widehat{\Var}[\hat{\beta}]=N[\hat{D'} Z {\hat{S}}^{-1} Z' \hat{D}]^{-1}$ \\
Нелинейный двухшаговый МНК & $\mathcal{Q}_{NL2SLS}(\beta)=u' Z (Z' Z)^{-1} Z' u $ \\
$(W_N=[N^{-1} Z' Z]^{-1})$ & $\widehat{\Var}[\hat{\beta}]=N[\hat{D'} Z (Z' Z)^{-1} Z' \hat{D}] ^{-1} [\hat{D'} Z (Z' Z)^{-1} \hat{S} (Z' Z)^{-1} Z' \hat{D}]$ \\ 
& $\times [\hat{D'} Z (Z' Z)^{-1} Z' \hat{D}]^{-1} $ \\
& $\widehat{\Var}[\hat{\beta}]=s^2  [\hat{D'} Z (Z' Z)^{-1} Z' \hat{D}]^{-1}$ \\
& \text{если ошибки гомоскедастичны}\\
Нелинейный метод & $\hat{\beta}_{NLIV} \text{ решает} Z' u=0$\\ 
инструментальных переменных & \\
(точно идентифицирована) & $\widehat{\Var}[\hat{\beta}]=N(Z'\hat{D})^{-1} \hat{S} (\hat{D'} Z)^{-1}$\\
\hline
\hline
\end{tabular}
\end{minipage}
\end{center}
\end{table}

\begin{center}
Нелинейный метод инструментальных переменных
\end{center}

В точно идентифицированном случае можно напрямую использовать условия для выборочных моментов, соответствующие (6.55). Это даёт оценку метода моментов в нелинейной
модели метода инструментальных переменных $\hat{\beta}_{NLIV}$, которая является решением:
\begin{equation}
\frac{1}{N} \sum_{i=1}^{N} z_i u_i=0,
\end{equation}
или $Z'u=0$ с оценкой асимптотической ковариационной матрицы, приведённой в таблице 6.3.

Нелинейные оценки часто рассчитываются с использованием итерационных методов, которые находят оптимум целевой функции, а не решают нелинейные системы оценочных уравнений. Для точно идентифицированного случая $\hat{\beta}_{NLIV}$ может быть вычислена как оценка ОММ при минимизации (6.56) при любой матрице весов, наиболее просто случай $W_N=I$, что приводит к
такой же оценке.

\begin{center}
Оптимальный нелинейный ОММ
\end{center}

Для сверх-идентифицированных моделей оценка оптимального ОММ использует матрицу весов $W_N={\hat{S}}^{-1}$. Оценка оптимального ОММ в нелинейной модели метода инструментальных переменных $\hat{\beta}_{OGMM}$ минимизирует
\begin{equation}
\mathcal{Q}_{N}(\beta)= \left( \frac{1}{N} u' Z \right) {\hat{S}}^{-1} \left( \frac{1}{N} Z' u \right).
\end{equation}

Оценка асимптотической ковариационной матрицы, приведённая в таблице 6.3, имеет относительно простую форму, так как (6.57) упрощается, когда $W_N={\hat{S}}^{-1}$.

Как и в линейном случае оценка оптимального ОММ представляет собой двухшаговую оценку, когда ошибки гетероскедастичны. При расчёте оценки ковариационной матрицы можно использовать $\hat{S}$, представленную в таблице 6.3, но более распространено использование вместо этого оценку $\tilde{S}$, которая также рассчитывается с использованием (6.59), но оценивает остатки при оптимальной оценке ОММ, а не оценке первого шага, которая используется для формирования $\hat{S}$ в (6.61).

\begin{center}
Нелинейный двухшаговый МНК
\end{center}

Частный случай оценки ОММ с инструментами использует $W_N=(N^{-1}Z'Z)^{-1}$ в
(6.56). Это даёт оценку нелинейного двухшагового метода наименьших квадратов $\hat{\beta}_{NL2SLS}$, которая минимизирует
\begin{equation}
\mathcal{Q}_{N}(\beta)= \frac{1}{N} u'Z(Z'Z)^{-1}Z'u.
\end{equation}
Данная оценка является оценкой оптимального ОММ, если ошибки гомоскедастичны, поскольку тогда $\hat{S}=s^2 Z'Z /N$, где $s^2$ --- состоятельная оценка константы $\Var[u|z]$, поэтому матрица ${\hat{S}}^{-1}$ пропорциональна $(Z'Z)^{-1}$.

С гомоскедастичными ошибками эта оценка имеет более простую оценку асимптотической ковариационной матрицы, указанную в таблице 6.3. Данный результат  часто приводится в учебниках. Тем не менее, в микроэконометрических приложениях часто допускают гетероскедастичные ошибки и используют более сложные скорректированные оценки, также приведённые в таблице 6.3.

Оценка нелинейного двухшагового МНК, предложенная Амэмия (1974), была важным предшественником оценки ОММ. Идея построения оценки аналогична идее построения линейного двухшагового МНК, приведённого в Разделе 6.4.3. Таким образом, мы умножаем ошибки модели $u$ на инструменты $Z'$ для получения $Z'u$, где $\E[Z'u]=0$, так как $\E[u|Z]=0$. После этого мы оцениваем нелинейную регрессию ОМНК. При гомоскедастичных ошибках она минимизирует 
\[
\mathcal{Q}_{N}(\beta)= u'Z[\sigma^2 Z'Z]^{-1}Z'u,
\]
так из $\Var[u|Z]=\sigma^2 I$ следует $\Var[Z'u|Z]=\sigma^2 Z'Z$. Эта целевая функция пропорциональна выражению (6.62).

Интерпретация Тейла линейного двухшагового метода МНК не всегда может быть перенесена на нелинейные модели (см. Раздел 6.5.4). Кроме того, оценка нелинейного двухшагового МНК является  оценкой одного шага. Амэмия выбрал термин оценка нелинейного двухшагового МНК, потому что, как и в линейном случае, она даёт состоятельные оценки с использованием инструментальных переменных. Название не следует понимать буквально, и более точным термином было бы --- оценка нелинейного метода инструментальных переменных или оценка обобщённого нелинейного метода инструментальных переменных.

\begin{center}
Выбор инструментов в нелинейных моделях
\end{center}

Предыдущие оценки предполагают существование инструментов таких, что $\E[u|z]=0$, и оценивание наиболее эффективно, если оно основано на уравнениях для  безусловного момента $\E[zu]=0$. Рассмотрим нелинейную модель с аддитивными ошибками, где $u=y-g(x,\beta)$. Чтобы быть релевантным, инструмент должен быть коррелирован с регрессорами $x$, тем не менее, чтобы быть годным он не может быть прямой причиной изменений $y$. Из ковариационной матрицы, приведённой в (6.57), имеет значение на самом деле корреляция $z$ с $\partial g/\partial \beta$, а не столько с $x$. Данная корреляция нужна для того, чтобы величина $\hat{D'}Z$ была большой. Сомнения по поводу слабых переменных здесь очень уместны, как и в линейном случае, изученном в Разделе 4.9.

Учитывая очень вероятную гетероскедастичность наилучшее моментное условие, на котором следует строить оценку, при условии что $\E[u|z]=0$, это не $\E[zu]=0$. Из Раздела 6.3.7, однако, оптимальное моментное условие требует дополнительных предположений о моментах, которые трудно сделать. Стандартно используют $\E[zu]=0$, как это было сделали мы.

Альтернативным способом учесть гетероскедастичность будет построение ОММ оценки на ошибке, близкой к гомоскедастичной. Например, со счётными данными, вместо $u=y-\exp(x'\beta)$, лучше использовать стандартизированные ошибки $u^*=u/\sqrt{\exp(x'\beta)}$ (см. Раздел 6.2.2). Однако следует отметить, что $\E[u^*|z]=0$ и $\E[u|z]=0$ --- это разные предположения.

Часто только одна компонента $x$ коррелирует с $u$. Тогда, как и в линейном случае, экзогенные компоненты могут быть использованы в качестве инструментов для самих себя, и задача состоит в том, чтобы найти дополнительный инструмент, который не коррелирует с $u$. Есть некоторые нелинейные приложения, которые возникают из формальных экономических моделей, как в Разделе 6.2.7, и в этом случае многие имеющиеся переменные доступны в качестве инструментов.

\subsection{Пример инструментальных переменных для распределения Пуассона}

Регрессионная модель Пуассона с экзогенными регрессорами специфицирует $\E[y|x]=\exp(x'\beta)$. Это уравнение можно рассматривать как модель с аддитивными ошибками $u=y-\exp(x'\beta)$. Если регрессоры являются эндогенными, то $\E[u|x] \not=0$ и оценки ММП Пуассона тогда будут несостоятельными. Состоятельная оценка предполагает наличие инструментов $z$, которые удовлетворяют условию $\E[u|z]=0$ или, что эквивалентно,
\[
\E[y-\exp(x'\beta)|z]=0.
\]

Предыдущие результаты могут быть применены напрямую. Целевая функция:
\[
\mathcal{Q}_{N}(\beta)= \left[ N^{-1} \sum_i z_i u_i \right]' W_N \left[ N^{-1} \sum_i z_i u_i \right],
\]
где $u_i=y_i-\exp(x'_i\beta)$. Тогда условия первого порядка:
\[
\left[ \sum_i \exp(x'_i \beta) x_i z'_i \right]' W_N \left[ \sum_i z_i (y_i -\exp(x'_i \beta))\right]=0.
\]

Асимптотическое распределение приведено в таблице 6.3, с $\hat{D}'Z = \sum_i \exp(x_i'\hat{\beta}) x_i z_i'$, так как $\partial{g}/\partial{\beta} = \exp(x'\beta)x$, а также $\hat{S}$ определено в (6.39) с $\hat{u}_i = y_i - \exp(x_i'\hat{\beta})$. Оптимальные оценки ОММ и оценки нелинейного двухшагового МНК отличаются матрицами весов $\hat{S}^{-1}$ или $(N^{-1}Z'Z)^{-1}$, где $Z'Z = \sum_i z_iz_i'$.

Другую состоятельную оценку можно получить с помощью подхода Басманна. Во-первых, с помощью МНК необходимо оценить сокращённую форму $x_i = \Pi z_i + v_i$, что даёт $K$ предсказаний $\hat{x}_i = \hat{\Pi}z_i$. Во-вторых, оценим с помощью нелинейного метода инструментальных переменных, как в (6.60) с инструментами $\hat{x}_i$ вместо $z_i$. Учитывая формулу МНК для $\hat{\Pi}$, эта оценка является решением следующего равенства: 

\[
\left[ \sum_i x_iz_i' \right] \left[ \sum_i z_iz_i' \right]^{-1} \left[ \sum_i (y_i - exp(x_i'\beta))z_i \right] = 0.
\]

Эта оценка отличается от оценки нелинейного двухшагового МНК, потому что первое слагаемое в левой части другое. Потенциальные проблемы, связанные с использованием здесь обобщения метода Тейла для линейной модели, подробно описаны в следующем разделе.

Аналогичные вопросы возникают и других в нелинейных моделях, например, в моделях для бинарных данных.

\subsection{Двухшаговое оценивание нелинейных моделей}

Обычная интерпретация линейного двухшагового МНК может не сработать в нелинейных моделях. Итак, пусть у $y$ математическое ожидание равно $g(x,\beta)$ и есть инструменты $z$ для регрессоров $x$. Тогда МНК регрессия $x$ на инструменты $z$, чтобы получить оценённые значения $\hat{x}$ с последующей регрессии НМНК $y$ на $g(\hat{x},\beta)$, может привести к несостоятельным оценкам параметра $\beta$, что мы сейчас покажем. Вместо этого, нужно использовать оценки нелинейного двухшагового МНК, представленного в предыдущем разделе.

Рассмотрим простую модель, основанную на модели, представленной Амэмия (1984), то есть нелинейную по переменным, но линейную по параметрам. Пусть
\begin{equation}
\begin{split}
y=\beta x^2 +u, \\
x=\pi z + \upsilon,\\
\end{split}
\end{equation}
где ошибки $u$ и $\upsilon$ имеют нулевое математическое ожиданием и коррелируют. Регрессоры $x^2$ эндогенны, так как $x$ является функцией от $\upsilon$ и по предположению $u$ и $\upsilon$ коррелируют. В результате оценка МНК $\beta$ несостоятельна. Если $z$ генерируется независимо от других случайных переменных в модели, то он будет годным инструментом, поскольку тогда ясно, что он не зависит от $u$, но коррелирует с $x$. 

Оценка метода инструментальных переменных $\hat{\beta}_{IV}= \left( \sum_i z_i x^2_i  \right)^{-1} \sum_i z_i y_i$. Эту оценку можно реализовать с помощью обычной регрессии метода инструментальных переменных $y$ на $x^2$ с инструментами $z$. Немного алгебраических преобразований, и как и ожидалось, $\hat{\beta}_{IV}$ равна оценке нелинейного метода инструментальных переменных, определённой в (6.60).

Предположим, вместо этого мы выполняем следующее двухшаговое оценивание наименьших квадратов. Во-первых, построим регрессию $x$ на $z$, чтобы получить $\hat{x}=\hat{\pi}z$, а затем построим регрессию $y$ на ${\hat{x}}^2$. Тогда $\hat{\beta}_{2SLS}= \left( \sum_i {\hat{x}_i}^2 {\hat{x}_i}^2 \right)^{-1} \sum_i {\hat{x}_i}^2 y_i$, где ${\hat{x}_i}^2$ --- квадрат прогноза $\hat{x}_i$, полученный из МНК регрессии $x$ на $z$. Мы получаем несостоятельные оценки. Адаптируя доказательство для линейного случая в Разделе 6.4.3, получаем:
\[
y_i=\beta x^2_i + u_i
\]
\[
=\beta {\hat{x_i}}^2 + w_i
\]
где $w_i=\beta({x_i}^2-{\hat{x_i}}^2)+u_i$. МНК регрессия $y_i$ на ${\hat{x_i}}^2$ несостоятельна для $\beta$, потому что регрессоры ${\hat{x_i}}^2$ асимптотически коррелируют с составной ошибкой $w_i$. Из $(x^2_i-{\hat{x_i}}^2)=(\pi z_i + \upsilon_i)^2-(\hat{\pi} z_i )^2=\pi^2 z^2_i + 2 \pi z_i \upsilon_i + \upsilon^2_i - {\hat{\pi}}^2{z_i}^2$, используя тот факт, что $\plim \hat{\pi} =\pi$, после некоторых преобразований мы получаем $\plim N^{-1} \sum_i {\hat{x_i}}^2 (x^2_i-{\hat{x_i}}^2) = \plim N^{-1} \sum_i {\pi}^2 z^2_i \upsilon^2_i \not =0$, даже если $z_i$ и $\upsilon_i$ независимы. Следовательно, $\plim N^{-1} \sum_i {\hat{x_i}}^2 w_i \not=  \plim N^{-1} \sum_i {\hat{x_i}}^2 \beta (x_i-\hat{x_i})^2=0$.

Вариант, который даёт состоятельную оценку, однако, заключается в построении регрессии $x^2$, а не $x$ на $z$ на первом шаге и использовании прогноза $\hat{x}^2 \not = (\hat{x})^2$ на втором этапе. Можно показать, что мы получим $\hat{\beta}_{IV}$. Инструмент для $x^2$ должен быть прогнозом значения $x^2$, а не квадратом прогноза $x$.

Данный пример обобщается на другие нелинейные модели, где нелинейность присутствует только по регрессорам, то есть на
\[
y=g(x)' \beta+u,
\]
где $g(x)$ --- нелинейная функция от $x$. Типичным примером является использование степенной функции и натурального  логарифма. Пусть $\E[u|z]=0$. Несостоятельные оценки получают путём построения регрессии $x$ на $z$, для получения прогнозов $\hat{x}$, а затем построения регрессии $y$ на $g(\hat{x})$. Состоятельные оценки могут быть получены путём регрессии $g(x)$ на $z$, для получения прогнозов $\hat{g}(x)$, а затем регрессии $y$ на $\hat{g}(x)$ на втором этапе. Мы используем $\hat{g}(x)$, а не $g(\hat{x})$ как инструмент для $g(x)$. Даже тогда регрессия на втором этапе даёт неверные стандартные ошибки, поскольку МНК будет использовать остатки $\hat{u}=y-\hat{g}(x)'\hat{\beta}$, а не $\hat{u}=y-g(x)'\hat{\beta}$. Лучше всего сразу использовать ОММ или нелинейный двухшаговый МНК.

В более общем случае модели могут быть нелинейными и по переменным, и по параметрам. Рассмотрим одноиндексную модель с аддитивной ошибкой
\[
y=g(x'\beta)+u.
\]
Несостоятельные оценки могут быть получены с помощью регрессии МНК $x$ на $z$, для получения прогнозов $\hat{x}$, а затем оценивания НМНК регрессии $y$ на $g(\hat{x'}\beta)$. Нужно использовать либо ОММ, либо нелинейный двухшаговый МНК. По сути, для обеспечения состоятельности необходимо использовать $\hat{g}(x'\beta)$, не $g(\hat{x'}\beta)$.

\begin{center}
Пример нелинейного двухшагового МНК
\end{center}

Мы оцениваем с помощью нелинейного двухшагового МНК модель с простой нелинейностью. Квадрат эндогенной переменной выступает в роли регрессора, как и в предыдущем разделе.

Процесс, генерирующий данные, --- (6.63), поэтому $y=\beta x^2 + u$ и $x=\pi z + \upsilon$, где $\beta=1$, $\pi=1$, и $z=1$ для всех наблюдений. Вектор $(u,\upsilon)$ является совместно нормально распределёнными с математическим ожиданием $0$, дисперсией $1$ и корреляцией $0.8$. Берётся выборка размера 200. Результаты приведены в таблице 6.4.

\begin{table}[h]
\begin{center}
\caption{\label{tab:GMMestnl} Пример нелинейного двухступенчатого метода наименьших квадратов}
\begin{minipage}{12.5cm}
\begin{tabular}[t]{cccc}
\hline
\hline
& \multicolumn{3}{c}{\bf{Оценка}} \\
\hline
\bf{Переменная}\footnote{Процесс, генерирующий данные, приведённый в тексте, имеет истинный коэффициент, равный единице. Размер выборки $N=200$.} & \bf{МНК} & \bf{Нелинейного} & \bf{Двушагового} \\
 & & \bf{двухшагового МНК} & \bf{метода} \\
\hline
$x^2$ & 1.189 & 0.960 & 1.642 \\
 & (0.025) & (0.046) & (0.172) \\
$R^2$ & 0.88 & 0.85 & 0.80 \\
\hline
\hline
\end{tabular}
\end{minipage}
\end{center}
\end{table}

Нелинейность здесь довольно мягкая, в роли регрессора выступает квадрат $x$, а не сам $x$. Цель заключается в оценке коэффициента $\beta$ при квадрате $x$. МНК-оценка несостоятельная, в то время как оценка нелинейного двухшагового МНК состоятельна. Двухшаговый метод, где сначала МНК регрессия $x$ на $z$ используется для формирования $\hat{x}$, а затем оценивается МНК-регрессия $y$ на $(\hat{x})^2$, даёт оценку, которая более чем на две стандартных ошибки отличается от истинного значения $\beta=1$. Симуляции также указывают на потерю в качестве подгонки и точности с большими стандартными ошибками и более низким $R^2$, как и в случае линейного метода инструментальных переменных.

\section{Последовательная двухшаговая М-оценка}

В процедуре последовательного двухступенчатого оценивания финальная оценка целевого параметра основана на первоначальной оценке неизвестного параметра. В качестве примера может послужить ДОМНК, где ошибка имеет условную дисперсию $\exp(z'\gamma)$. При наличии оценки $\tilde{\gamma}$ для $\gamma$, оценка ДОМНК $\hat{\beta}$ является решением $\sum_{i=1}^{N}(y_i-x'_i\hat{\beta})^2/\exp(z'_i \tilde{\gamma})$. Вторым примером является двухступенчатый оценка Хекмана, приведённая в Разделе 16.10.2.

Эти оценки привлекательны тем, что они могут обеспечить относительно простой способ получения состоятельных оценок параметров. Однако для правильных статистических выводов может потребоваться изменить оценку асимптотической ковариационной матрицы второго шага для того, чтобы учесть наличие первого шага. Мы представляем результаты для специального случая, когда оценочные уравнения для оценок обоих первого и второго шага приравнивают выборочное среднее к нулю, как это имеет место для М-оценок, оценок метода моментов и оценок методом оценочных уравнений.

Разделим вектор параметров $\theta$ на две части $\theta_1$ и $\theta_2$, основная цель заключается в оценивании $\theta_2$. Модель оценивается последовательно с получением сначала $\hat{\theta}_1$, которая является решением $\sum_{i=1}^{N} h_{1i}(\hat{\theta}_1)=0$, и тогда при заданной $\hat{\theta}_1$ получаем $\hat{\theta}_2$, которая является решением $N^{-1} \sum_{i=1}^{N} h_{2i}(\hat{\theta}_1,\hat{\theta}_2)=0$. В целом распределение $\hat{\theta}_2$, учитывая использование оценки $\hat{\theta}_1$, отличается от распределения $\hat{\theta}_2$ и сложнее, чем распределение $\hat{\theta}_2$, если $\theta_1$ известно. Статистический вывод является неправильным, если он не принимает во внимание это осложнение за исключением некоторых особых случаев, приведённых в конце этого раздела.

Следующий вывод приведён в книге Ньюи (1984) с аналогичными результатами, полученными Мёрфи и Топелем (1985) и Паганом (1986). Двухшаговая оценка может быть переписана в виде одношаговой оценки, где $(\theta_1,\theta_2)$  являются решениями уравнений:
\begin{equation}
\begin{split}
N^{-1} \sum_{i=1}^{N} h_1(w_i,\hat{\theta}_1)=0, \\
N^{-1} \sum_{i=1}^{N} h_2(w_i,\hat{\theta}_1,\hat{\theta}_2)=0.\\
\end{split}
\end{equation}
Определив $\theta=(\begin{matrix} \theta'_1 & \theta'_2 \end{matrix})'$ и $h_i=(\begin{matrix} h'_{1i} & h'_{2i} \end{matrix})'$, можно записать уравнения как:
\[
N^{-1} \sum_{i=1}^{N} h(w_i,\hat{\theta})=0.
\]
В этой постановке предполагается, что $\dim(h_1)=\dim(\theta_1)$ и $\dim(h_2)=\dim(\theta_2)$, чтобы количество оценочных уравнений было равно числу параметров. Тогда (6.64) является оценкой оценочных уравнений или оценкой ММ.

Для состоятельности необходимо условие  $\plim N^{-1} \sum_i h(w_i,\theta_0)=0$, где $\theta_0=[\theta^1_{10},\theta^1_{20}]$. Это условие будет выполнено, если $\hat{\theta}_1$ состоятельна для $\theta_{10}$ на первом шаге, и если оценка $\hat{\theta}_2$ второго шага с известной $\theta_{10}$ (а не оценённой с помощью $\hat{\theta}_1$) приведёт к состоятельной оценке $\theta_{20}$. В рамках метода моментов мы требуем $\E[h_{1i}(\theta_1)]=0$ и $\E[h_{2i}(\theta_1,\theta_2)]=0$. Мы предполагаем, что состоятельность установлена.

Для асимптотического распределения мы применяем общий результат, что 
\[
\sqrt{N} (\hat{\theta}-\theta_0) \xrightarrow{d} \mathcal{N}[0,G^{-1}_0 S_0 (G^{-1}_0)'],
\] 
где $G_0$ и $S_0$ определены в утверждении 6.1. Разбиение $G_0$ и $S_0$ происходит аналогичным образом, как и разбиение $\theta$ и $h_i$. Тогда
\[
G_0= \lim \frac{1}{N} \sum_{i=1}^{N} \E \begin{bmatrix} \partial h_{1i} / \partial \theta'_1 & 0 \\ \partial h_{2i} / \partial \theta'_1 & \partial h_{2i} / \partial \theta'_2
\end{bmatrix} = \begin{bmatrix} G_{11} & 0 \\  G_{21} &  G_{22} \end{bmatrix},
\]
здесь $\partial h_{1i} / \partial \theta'_2 =0$, так как $h_{1i}(\theta)$ не является функцией от $\theta_2$ из (6.64). Поскольку $G_0$, $G_{11}$, $G_{22}$--- квадратные матрицы
\[
G^{-1}_0 = \begin{bmatrix} G^{-1}_{11} & 0 \\  -G^{-1}_{22} G_{21} G^{-1}_{11} &  G^{-1}_{22} \end{bmatrix}.
\]
И ежy понятно,
\[
S_0= \lim \frac{1}{N} \sum_{i=1}^{N} \E \begin{bmatrix} h_{1i} h_{1i}' &  h_{1i} h_{2i}' \\ h_{2i} h_{1i}' &  h_{2i} h_{2i}' \end{bmatrix}= \begin{bmatrix} S_{11} & S_{12} \\ S_{21} & S_{22} \end{bmatrix}.
\]
Асимптотической ковариационной матрицей $\hat{\theta}_2$ является подматрица  ковариационной матрицы $\hat{\theta}$. После некоторых алгебраических преобразований мы получаем:
\begin{equation}
\Var[\hat{\theta}_2]= G^{-1}_{22} \left\{ \begin{matrix} S_{22} + G_{21} [G^{-1}_{11} S_{11} G^{-1}_{11}] G'_{21} \\ -G_{21} G^{-1}_{11} G_{12} - S_{21} G^{-1}_{11} G'_{21} \end{matrix} \right\} G^{-1}_{22}.
\end{equation}

Обычный компьютерный алгоритм выдаёт неправильные стандартные ошибки, которые  занижают истинные стандартные ошибки, так как $\Var[\hat{\theta}_2]$  предполагается равной $G^{-1}_{22} S_{22} G^{-1}_{22}$, и можно показать, что она меньше истинной дисперсии, приведённой в (6.65).

Нет необходимости учитывать дополнительную изменчивости на втором шаге, вызванную использованием оценки на первом этапе в особом случае, если $\E[\partial h_{2i} (\theta) / \partial \theta_1]=0$, поскольку тогда $G_{21}=0$ и $\Var[\hat{\theta}_2]$ в (6.65) сводится к $G^{-1}_{22} S_{22} G^{-1}_{22}$.

Известным примером с $G_{21}=0$ является ДОМНК. Тогда для гетероскедастичных ошибок
\[
h_{2i}(\theta)= \frac{x_{2i}(y_i-x'_i \theta_2)}{\sigma(x_i, \theta_1)},
\]
где $\Var[y_i|x_i]=\sigma^2(x_i, \theta_1)$ и
\[
\E[\partial h_{2i} (\theta) / \partial \theta_1] =\E \left[ -x_{2i} \frac{(y_i-x'_i \theta_2)}{\sigma(x_i, \theta_1)^2} \frac{\partial \sigma(x_i, \theta_1)}{\partial \theta_1} \right],
\]
что равно нулю, поскольку $\E[y_i|x_i]=x'_i \theta_2$. Кроме того, для ДОМНК состоятельность $\hat{\theta}_2$ не требует, чтобы $\hat{\theta}_1$ была состоятельной, так как для $\E[h_{2i}(\theta)]=0$ необходимо, чтобы $\E[y_i|x_i]=x'_i \theta_2$, что не зависит от $\theta_1$.

Второй пример $G_{21}=0$ --- оценка ММП с блочно-диагональной матрицей с $\E[\partial^2 \mathcal{L}(\theta) / \partial \theta_1 \partial \theta'_2]=0$. Например, так будет в регрессии при нормальности ошибок, где $\theta_1$ являются параметрами дисперсии и $\theta_2$ являются параметрами регрессии.

В других примерах, однако, $G_{21} \not = 0$ и должно быть использовано более громоздкое выражение (6.65). Это делается автоматически в компьютерных пакетах для некоторых стандартных двухшаговых оценок, в первую очередь для двухшаговой оценки Хекмана в моделях с самоотбором выборки, представленной в Разделе 16.5.4. В противном случае, $\Var[\hat{\theta}_2]$ должна быть вычислена вручную. Многие компоненты уже посчитаны ранее. В частности $G^{-1}_{11} S_{11} G^{-1}_{11}$ --- скорректированная ковариационная матрица для $\hat{\theta}_1$ и $G^{-1}_{22} S_{22} G^{-1}_{22}$ --- скорректированная ковариационная матрица оценки $\hat{\theta}_2$, которая ошибочно игнорирует оценку $\hat{\theta}_1$. Для данных, независимых по $i$, подкомпоненты подматрицы $S_0$ могут быть состоятельно оценены с помощью $\hat{S}_{jk}= N^{-1} \sum_i \hat{h}_{ji} \hat{h}_{ki}'$. Тогда сложность заключается в вычислении $\hat{G}_{21}= N^{-1} \sum_i \partial h_{2i} / \partial \theta'_1|_{\hat{\theta}}$.

Рекомендуется более простой подход, который заключается в получении бутстрэп стандартных ошибок (см. Раздел 16.2.5), или непосредственно совместно оценить $\theta_1$ и $\theta_2$ в комбинированной модели (6.64), предполагая доступ к обычному ОММ.

Данные более простые подходы также могут быть применены к последовательным оценкам, которые являются оценками ОММ, а не М-оценками. Тогда объединение двух оценок приведёт к набору условий более сложному, чем (6.64), и мы больше не получим (6.65). Тем не менее, можно использовать бутстрэп или оценить совместно, а не последовательно.

\section{Оценивание методом минимального расстояния}

Оценивание методом минимального расстояния --- это способ оценки структурных параметров $\theta$, которые зависят от параметров в приведенной форме $\pi$, при заданной состоятельной оценке $\hat{\pi}$ для $\pi$.

Стандартно обращаются к книге Фергюсона (1958). Ротенберг (1973) применил этот метод к моделям линейных одновременных уравнений, хотя альтернативные методы, приведённые в Разделе 6.9.6, являются стандартными методами, используемыми на практике. Оценка минимального расстояния чаще всего используется в анализе панельных данных. В начальной работе Чемберлен (1982, 1984) (см. Раздел 22.2.7) задаёт $\hat{\pi}$ как оценку МНК из линейной регрессии зависимой переменной текущего периода на регрессоры во всех периодах. Последующие приложения к моделям ковариации (см. Раздел 22.5.4) предполагают, что $\hat{\pi}$ --- оценки дисперсий и автокорреляций в панельных данных. См. также косвенный метод логического вывода (Раздел 12.6).

Предположим, что соотношение между $q$ структурными параметрами и $r > q$ параметрами в приведенной форме --- $\pi_0=g(\theta_0)$. Далее предположим, что у нас есть состоятельная оценка $\hat{\pi}$. Очевидной оценкой является $\hat{\theta}$ такая, что $\hat{\pi}=g(\hat{\theta})$, но это невозможно, так как $q < r$. Вместо этого, оценка минимального расстояния (Minimal distance, MD) $\hat{\theta}_{MD}$ минимизирует целевую функцию по $\theta$:
\begin{equation}
\mathcal{Q}_{N}(\theta)=(\hat{\pi}-g(\theta))' W_N (\hat{\pi}-g(\theta)),
\end{equation}
где $W_N$ --- матрица весов размера $r \times r$.

Если $\hat{\pi} \xrightarrow{p} \pi_0$ и $W_N \xrightarrow{p} W_0$, где $W_0$ конечна и положительно полуопределена, тогда $\mathcal{Q}_{N}(\hat{\theta}) \xrightarrow{p} \mathcal{Q}_{0}(\theta)=(\pi_0-g(\theta))' W_0 (\pi_0-g(\theta))$. Отсюда следует, что $\theta_0$ локально идентифицируема, если 
$ \rank [W_0 \times \partial g(\theta) / \partial \theta']=q$, а для состоятельность в основном необходимо условие $\pi_0=g(\theta_0)$.

Для оценки минимального расстояния $\sqrt{N} (\hat{\theta}_{MD}-\theta_0) \xrightarrow{d} \mathcal{N}[0, \Var[\hat{\theta}_{MD}]]$, где
\begin{equation}
\Var[\hat{\theta}_{MD}]=(G'_0 W_0 {G_0}^{-1})^{-1} (G'_0 W_0 \Var[\hat{\pi}] W_0 G_0) (G'_0 W_0 G_0)^{-1},
\end{equation}
$G_0=\partial g(\theta) / \partial \theta'|_{\theta_0}$, и предполагается, что параметры в приведенной форме $\hat{\pi}$ имеют предельное распределение $\sqrt{N} (\hat{\pi}-\pi_0) \xrightarrow{d} \mathcal{N}[0,\Var[\hat{\theta}]]$. Более эффективные оценки в приведенной форме приводят к более эффективным оценкам минимального расстояния, поскольку меньшее значение $\Var[\hat{\pi}]$ приводит к меньшим значениям $\Var[\hat{\theta}_{MD}]$ в (6.67).

Чтобы получить результат (6.67), начнём со следующего масштабирования условий первого порядка для оценки минимального расстояния:
\begin{equation}
G_N(\hat{\theta})' W_N \sqrt{N} (\hat{\pi}-g(\theta))=0,
\end{equation}
где $G_N(\theta)=\partial g(\theta) / \partial \theta'$. Точное разложение в ряд Тейлора первого порядка в точке $\theta_0$ даёт
\begin{equation}
\sqrt{N} h(\hat{\pi}-g(\hat{\theta}))= \sqrt{N} (\hat{\pi}-\pi_0) - G_N (\theta^+) \sqrt{N} (\hat{\theta}-\theta_0),
\end{equation}
где $\theta^+$ лежит между $\theta$ и $\theta_0$, и мы используем $g(\theta_0)=\pi_0$. Подставляя (6.69) обратно в (6.68) и решая относительно $\sqrt{N} (\hat{\theta}-\theta_0)$ получаем:
\begin{equation}
\sqrt{N} (\hat{\theta}-\theta_0)= [G_N(\hat{\theta})' W_N G_N(\theta^+)]^{-1} G_N(\hat{\theta})' W_N \sqrt{N} (\hat{\pi}-\pi_0),
\end{equation}
что приводит напрямую к (6.67).

Для заданных оценок в приведенной форме $\hat{\pi}$ наиболее эффективные оценки минимального расстояния используют матрицу весов $W_N=\widehat{\Var}[\hat{\pi}]^{-1}$ в (6.66). Эта оценка называется оценкой оптимального метода минимального расстояния, а иногда минимальной оценкой хи-квадрат по Фергюсону (1958).

Распространённым альтернативным частным случаем является оценка равновзвешенного минимального расстояния, у которой $W_N=I$. Она менее эффективна, чем оценка ОМР, но у неё нет проблемы смещения в конечных выборках, аналогичной той, которая обсуждалась в Разделе 6.3.5 и которая возникает при использовании оптимальный матрицы весов. Равновзвешенная оценка может быть получена с помощью регрессии НМНК $\hat{\pi}_j$ на $g_j(\theta), j=1, \dots, r$, так как минимизация $(\hat{\pi}-g(\hat{\theta}))'(\hat{\pi}-g(\hat{\theta}))$ даёт такие же условия первого порядка, как в (6.68) с $W_N=I$.

Максимальное значение целевой функции для оптимального метода наименьшего расстояния распределено по хи-квадрат. В частности,
\begin{equation}
(\hat{\pi}-g(\hat{\theta}_{OMD}))' \Var[\hat{\pi}]^{-1}(\hat{\pi}-g(\hat{\theta}_{OMD})) 
\end{equation}
асимптотически распределена по $\chi^2(r-q)$ при $H_0:g(\theta_0)=\pi_0$. Можно провести тест на спецификацию модели, аналогичный тесту OIR в Разделе 6.3.8.

Оценки минимального расстояния качественно похожи на оценки ОММ. Подход ОММ является более стандартными. Оценка минимального расстояния чаще всего используется в панельных исследованиях ковариационных структур, поскольку $\hat{\pi}$ включает легко оцениваемые выборочные моменты (дисперсии и ковариации) и может быть использована для получения оценки $\hat{\theta}$.

\section{Эмпирический метод правдоподобия}

Подходы ММ и ОММ не требуют полной спецификации условной плотности. Вместо этого, оценка основана на моментных условиях вида $\E[h(y,x,\theta)]=0$. Подход эмпирического метода правдоподобия, по Оуэну (1988), является альтернативной процедурой оценки, основанной на тех же моментных условиях.

Привлекательностью оценки эмпирического метода правдоподобия является то, что, хотя она асимптотически эквивалентна оценке ОММ, у неё другие свойства в конечных выборках, и в некоторых случаях она превосходит оценку ОММ.

\subsection{Оценки математического ожидания генеральной совокупности с помощью эмпирического метода правдоподобия}

Начнём с эмпирического метода правдоподобия в случае скалярных нормальных и независимо распределённых случайных величин $y$ с плотностью $f(y)$ и функцией максимального правдоподобия для выборки $\Pi_i f(y_i)$. Отличие, которое мы рассматриваем, состоит в том, что плотность $f(y)$ не специфицирована, так что обычный подход ММП невозможен.

В полностью непараметрическом подходе оценивают плотность $f(y)$, вычисленную в каждом из выборочных значений $y$. Пусть $\pi_i=f(y_i)$ обозначает вероятность того, что $i$-ое наблюдение $y$ принимает реализовавшееся значение $y_i$. В таких обозначениях цель состоит в максимизации так называемый эмпирической функции правдоподобия $\Pi_i \pi_i$, или что равносильно, максимизации эмпирической логарифмической функции правдоподобия $N^{-1} \sum_i \ln \pi_i$, которая является мультиномиальной моделью без определенной структуры для $\pi_i$. Эта логарифмическая функция правдоподобия неограничена, если не наложить  ограничения на $\pi_i$. Используется нормализация $\sum_i \pi_i=1$. Она приводит к стандартной оценке функции распределения для полностью непараметрического случая, как мы сейчас покажем.

Оценки эмпирического метода правдоподобия максимизируют Лагранжиан по $\pi$ и $\eta$
\begin{equation}
\mathcal{L}_{EL}(\pi,\eta)= \frac{1}{N} \sum_{i=1}^{N} \ln \pi_i - \eta \left( \sum_{i=1}^{N} \pi_i - 1 \right),
\end{equation}
где $\pi=[\pi_1 \dots \pi_N]'$ и $\eta$ --- множитель Лагранжа. Хотя данные $y_i$ не появляются явно в (6.72), они появляются неявно, поскольку $\pi_i=f(y_i)$. Взяв производные по $\pi_i (i=1, \dots, N)$ и приравняв $\eta$ к единице, получаем $\hat{\pi}_i=1/N$ и $\eta=1$. Таким образом, оцененная функция вероятности $\hat{f}(y)$ присваивает вес $1/N$ для каждого реализованного значения $y_i, i=1, \dots, N$. В результате функция распределения --- $\hat{F}(y) = N^{-1} \sum_{i=1}^N {\bf{1}} (y \le y_i)$, где ${\bf{1}}(A)=1$, если событие $A$ происходит, и $0$ в противном случае. Значит $\hat{F}(y)$ --- обычная эмпирическая функция распределения.

Теперь введём параметры. Для простого примера предположим, что мы вводим моментное ограничение, $\E[y-\mu]=0$, где $\mu$ является неизвестным математическим ожиданием генеральной совокупности. В контексте эмпирического метода правдоподобия этот теоретический момент заменяется выборочным моментом, взвешивающим выборочные значения с помощью вероятностей $\pi_i$. Таким образом, мы вводим ограничение, что $\sum_i \pi_i (y_i -\mu)=0$. Функция Лагранжа для оценки эмпирического метода максимального правдоподобия имеет вид
\begin{equation}
\mathcal{L}_{EL}(\pi,\eta,\lambda,\mu) = \frac{1}{N} \sum_{i=1}^{N} \ln \pi_i - \eta \left( \sum_{i=1}^{N} \pi_i -1 \right) - \lambda \sum_{i=1}^{N} \pi_i (y_i -\mu),
\end{equation}
где $\eta$ и $\lambda$ --- множители Лагранжа.

Начнём с дифференцирования функции Лагранжа по $\pi_i (i=1, \dots, N)$, $\eta$ и
$\lambda$, но не $\mu$. Приравняем эти производные к нулю, и получим уравнения, которые являются функциями от $\mu$. Решая их находим $\pi_i=\pi_i(\mu)$ и, следовательно, функция эмпирического правдоподобия $N^{-1} \sum_i \ln \pi_i (\mu)$ максимизируется по $\mu$. Этот метод решения приводит к нелинейным уравнениям, которые должны быть решены численно.

Для этой конкретной проблемы есть более  простой способ найти оценку для $\mu$. Заметим, что максимальное значение $\mathcal{L}(\pi,\eta,\lambda,\mu)$ должно быть меньше или равно $N^{-1} \sum_i \ln N^{-1}$, так как это максимальное значение без последнего ограничения. Однако  $\mathcal{L}(\pi,\eta,\lambda,\mu)$ равно $N^{-1} \sum_i \ln N^{-1}$, если $\pi_i = 1/ N$ и $\hat{\mu}=N^{-1} \sum_i y_i = \bar{y}$. Поэтому оценка математического ожидания генеральной совокупности при использовании эмпирического метода максимального правдоподобия --- это выборочное среднее.

\subsection{Оценки параметров регрессии эмпирического метода правдоподобия}

Теперь рассмотрим данные регрессии, которые являются одинаково и независимо распределёнными по $i$. Структура модели задани  $r$ моментными условиями:
\begin{equation}
\E[h(w_i,\theta)]=0,
\end{equation}
где $h(\cdot)$ и $w_i$ определены в Разделе 6.3.1. Например, $h(w,\theta)=x(y-x'\theta)$ для оценки МНК и $h(y,x,\theta)=(\partial g / \partial \theta)(y-g(x,\theta))$ для оценки НМНК.

Подход эмпирического метода правдоподобия максимизирует эмпирическую функцию правдоподобия $N^{-1} \sum_i \ln \pi_i$ при ограничении $\sum_i \pi_i=1$ (см. (6.72)) и при дополнительном выборочном ограничении, основанном на условии для теоретического момента:
\begin{equation}
\sum_{i=1}^{N} \pi_i h(w_i,\theta)=0.
\end{equation}
Таким образом, мы максимизируем по $\pi,\eta,\lambda$ и $\theta$
\begin{equation}
\mathcal{L}_{EL}(\pi,\eta,\lambda,\theta) = \frac{1}{N} \sum_{i=1}^{N} \ln \pi_i - \eta \left( \sum_{i=1}^{N} \pi_i -1 \right) - \lambda' \sum_{i=1}^{N} \pi_i h(w_i,\theta),
\end{equation}
где множители Лагранжа --- это скаляр $\eta$ и вектор-столбец $\lambda$ той же размерности, что и $h(\cdot)$.

Во-первых, найдем $N$ параметров $\pi_1, \dots, \pi_N$. Продифференцировав $\mathcal{L}(\pi,\eta,\lambda,\theta)$ по $\pi_i$, получим $1/(N \pi_i) -\eta-\lambda'h_i=0$. Тогда получаем $\eta=1$ с помощью умножения на $\pi_i$ и суммируя по $i$ и используя $\sum_i \pi_i h_i=0$. Отсюда следует, что
\begin{equation}
\pi_i(\theta,\lambda)=\frac{1}{N(1+\lambda'h(w_i,\theta))}.
\end{equation}

Теперь задача сведена к максимизации по $(r+q)$ переменных $\lambda$ и $\theta$. Это множители Лагранжа, связанные с $r$ условиями моментов (6.74), и $q$ параметров $\theta$.

Решение на этом этапе требует применения численных методов, даже для точно идентифицированных моделей. Можно максимизировать по $\theta$ и $\lambda$ функцию $N^{-1} \sum_i \ln [1/N(1+\lambda'h(w_i,\theta))]$.

Можно пойти другим путём и сначала выразить $\lambda$. Дифференцирование $\mathcal{L}(\pi(\theta,\lambda),\eta,\lambda)$ по $\lambda$ даёт $\sum_i \pi_i h_i = 0$. Определим $\lambda(\theta)$ как неявное решение системы из $\dim(\lambda)$ уравнений:
\[
\sum_{i=1}^{N} \frac{1}{N(1+\lambda'h(w_i,\theta))} h(w_i,\theta) =0.
\]
При практическом применении нужны численные методы для получения $\lambda(\theta)$. Тогда(6.77) упрощается до:
\begin{equation}
\pi_i(\theta)=\frac{1}{N(1+\lambda(\theta)' h(w_i,\theta))}.
\end{equation} 
Подставляя (6.78) в функцию эмпирического метода правдоподобия $N^{-1} \sum_i \ln \pi_i$, логарифмическая функция эмпирического метода правдоподобия, оценённая в $\theta$, имеет вид,
\[
\mathcal{L}_{EL}(\theta)=-N^{-1} \sum_{i=1}^N \ln [N(1+\lambda(\theta)' h(w_i,\theta))].
\]

Оценка эмпирического метода максимального правдоподобия $\hat{\theta}_{MEL}$ максимизирует эту функцию по $\theta$.

Квин и Лоулесс (1994) показали, что
\[
\sqrt{N} (\hat{\theta}_{MEL}-\theta_0) \xrightarrow{d} \mathcal{N}[0, A(\theta_0)^{-1} B(\theta_0) A(\theta_0)'^{-1}],
\]
где $A(\theta_0)=\plim \E[\partial h(\theta) / \partial \theta'|_{\theta_0}]$ и $B(\theta_0)= \plim \E[h(\theta)h(\theta)'|_{\theta_0}]$. Это то же предельное распределение, как и у метода моментов (см. (6.13)). Однако в конечных выборках $\hat{\theta}_{MEL}$ отличается от $\hat{\theta}_{GMM}$, и статистические выводы строятся на выборочных оценках:
\[
\hat{A}= \sum_{i=1}^N \hat{\pi}_i \left. \frac{\partial h'_i}{\partial \theta} \right|_{\hat{\theta}},
\]
\[
\hat{B}= \sum_{i=1}^N \hat{\pi}_i h_i(\hat{\theta}) h_i(\hat{\theta})'
\]
которые взвешены с помощью оценённых вероятностей $\hat{\pi}_i$, а не с помощью констант $1/N$.

Имбенс (2002) сравнивает эмпирический метод правдоподобия с ОММ в своём обзоре. Различные вариации метода включают, например, замену $N^{-1} \sum_i \ln \pi_i$ в (6.26) на $N^{-1} \sum_i \pi_i \ln \pi_i$. Эмпирический метод правдоподобия  вычислительно более сложен, см. Имбенс (2002) для обсуждения. Преимущество метода в том, что асимтотические результаты дают лучшую аппроксимацию в конечных выборках, чем для оценки ОММ. Это дополнительно рассмотрено в Разделе 11.6.4.

\section{Линейные системы уравнений}

Предыдущая теория оценивания охватывает методы оценки с одним уравнением, которые используются в большинстве прикладных исследований. Рассмотрим теперь одновременную оценку нескольких уравнений. Уравнения, линейные по параметрам с аддитивными ошибками, представлены в этом разделе, с обобщением до нелинейных систем, приведённом в следующем разделе.

Основным преимуществом совместного оценивания является увеличение эффективности,  связанной с учётом корреляции ненаблюдаемых переменных по всем уравнениям для данного индивида. Кроме того, совместная оценка может быть необходима, если есть ограничения на параметры уравнений. Оценка систем с экзогенными регрессорами является небольшим расширением МНК с одним уравнением и оценки НМНК, в то время как с эндогенными регрессорами адаптируется метод инструментальных переменных с одним уравнением.

Один из основных примеров системы уравнений --- уравнения для наблюдаемого спроса на нескольких товаров в определённый момент времени для множества индивидов. Для внешне не связанных уравнений все регрессоры являются экзогенными, тогда как для системы одновременных уравнений некоторые регрессоры являются эндогенными.

Второй распространенный пример --- панельные данные, где одно уравнение наблюдается в нескольких моментах времени для множества индивидов, и каждый период времени рассматривается как отдельное уравнение. Рассматривая модель панельных данных в качестве примера системы, можно повысить эффективность, получить скорректированные стандартные ошибки для панели и получить инструменты, когда некоторые регрессоры являются эндогенными.

Многие эконометрические тексты содержат подробное изложение  систем линейных уравнений. Рассмотрение здесь очень краткое. Оно в основном направлено на обобщение на нелинейные системы (см. Раздел 6.10) и применение на панельных данных (см. Главы 21-23).

\subsection{Системы  линейных уравнений}

Линейная модель с одним уравнением задаётся как $y_i=x'_i\beta+u_i$, где $y_i$ и $x_i$ ---скаляры и $x_i$ и $\beta$ --- вектор-столбцы. Линейная модель с несколькими уравнениями, или многомерная линейная модель, с $G$ зависимыми переменными задаётся как:
\begin{equation}
y_i=X_i \beta + u_i, i=1, \dots, N,
\end{equation}
где $y_i$ и $u_i$ --- векторы размера $G \times 1$, $X_i$ --- матрица размера $G \times K$, и $\beta$ --- вектор-столбец размера $K \times 1$.

В этом разделе мы делаем предположение типичное для пространственных данных, что вектор ошибки $u_i$ независим по $i$, поэтому $\E[u_i u'_j]=0$ для $i \not = j$. Тем не менее, элементы $u_i$ для заданного $i$ могут коррелировать и имеют дисперсию и ковариации, которые изменяются по $i$, что приводит к условной ковариационной матрицы ошибок для $i$-ого индивида равной:
\begin{equation}
\Omega_i=\E[u_i u'_i|X_i].
\end{equation}

Существуют различные причины, по которым может возникнуть модель множественных уравнених. Модель внешне не связанных уравнений содержит $G$ уравнений таких, например, как спросы на различные потребительские товары, где параметры отличаются в разных уравнениях и регрессоры могут варьироваться или не варьироваться в уравнениях. Панельная модель содержит $G$ наблюдений во времени для одного и того же уравнения с параметрами, которые постоянны по периодам и регрессорам, которые варьируются или не варьируются по периодам. Эти два случая подробно изложены в Разделах 6.9.3 и 6.9.4.

Разместив данные (6.79)  по $N$ индивидам в столбец, получаем:
\begin{equation}
\begin{bmatrix} y_1 \\  \vdots \\ y_N \end{bmatrix} = \begin{bmatrix} X_1 \\  \vdots \\ X_N \end{bmatrix} \beta + \begin{bmatrix} u_1 \\  \vdots \\ u_N \end{bmatrix},
\end{equation}
или
\begin{equation}
y=X \beta + u,
\end{equation}
где $y$ и $u$ векторы размера $NG \times 1$ и $X$ представляет собой матрицу $NG \times K$.

Результаты, приведённые далее, могут быть получены путём рассмотрения модели в виде столбцов
(6.82) так же, как и в случае одного уравнения. Таким образом, оценка МНК равна $\hat{\beta}=(X'X)^{-1}X'y$, а в случае точной идентификации с матрицей инструментов $Z$ оценка метода инструментальных переменных равна $\hat{\beta}=(Z'X)^{-1}Z'y$. Единственным существенным изменением является то, что обычное пространственное предположение о диагональной ковариационной матрице ошибок заменяется предположением о блочно-диагональной матрице ошибок. Эта блочная диагональность должна быть принята во внимание при оценке ковариационной матрицы оценок и в формировании оценок ДОМНК и эффективных оценок ОММ.

\subsection{Оценки МНК и ДОМНК для систем уравнений}

МНК оценка системы (6.82) даёт оценку МНК для систем уравнений $(X'X)^{-1}X'y$. Используя (6.81), сразу получаем, что
\begin{equation}
\hat{\beta}_{SOLS}= \left[ \sum_{i=1}^N X'_i X_i \right]^{-1} \sum_{i=1}^N X'_i y_i.
\end{equation}
Оценка является асимптотически нормальной и при предположении, что данные независимы по $i$, обычная робастная сэндвич форма применима и здесь и:
\begin{equation}
\widehat{\Var}[\hat{\beta}_{SOLS}]= \left[ \sum_{i=1}^N X'_i X_i \right]^{-1} \sum_{i=1}^N X'_i \hat{u}_i \hat{u}'_i X_i \left[ \sum_{i=1}^N X'_i X_i \right]^{-1},
\end{equation}
где $\hat{u}_i=y_i-X_i \hat{\beta}$. Эта оценка ковариационной матрицы допускает, что условные дисперсии и ковариации ошибок отличаются по индивидам.

С учётом корреляции элементов вектора ошибок для данного индивида, более эффективная оценка достигается с помощью НМНК или ДОМНК. Если наблюдения независимы по $i$, оценка НМНК системы является оценкой МНК системы, применяемой к преобразованной системе:
\begin{equation}
{\Omega_i}^{-1/2} y_i ={\Omega_i}^{-1/2} X_i \beta + {\Omega_i}^{-1/2}u_i,
\end{equation}
где $\Omega_i$ --- ковариационная матрица ошибок, определённая в (6.80). У преобразованных ошибок ${\Omega_i}^{-1/2}u_i$ нулевое математическое ожидание и следующая ковариационная матрица:
\[
\E \left[ \left( \Omega^{-1/2}_i u_i \right)' \left( \Omega^{-1/2}_i u_i \right) |X_i \right]= \Omega^{-1/2}_i \E[u'_i u_i|X_i] \Omega^{-1/2}_i
\]
\[
= \Omega^{-1/2}_i \Omega_i \Omega^{-1/2}_i
\]
\[
=I_G.
\]
Таким образом, у преобразованной системы ошибки гомоскедастичны и коррелируют в $G$ уравнениях и оценка МНК является эффективной.

Для реализации этой оценки модель для $\Omega_i$ должна быть специфицирована, например, $\Omega_i=\Omega_i(\gamma)$. Затем проводится оценка системы МНК в преобразованной системе, с $\Omega_i$ заменённой на $\Omega_i(\hat{\gamma})$, где $\hat{\gamma}$ --- состоятельная оценка для $\gamma$. Мы получаем оценку ДОМНК системы:
\begin{equation}
\hat{\beta}_{SFGLS} = \left[ \sum_{i=1}^N X'_i {\hat{\Omega}_i}^{-1} X_i \right]^{-1} \sum_{i=1}^N X'_i {\hat{\Omega}_i}^{-1} y_i.
\end{equation}
Эта оценка является асимптотически нормальной и для защиты от возможной неправильной спецификации $\Omega_i(\gamma)$ мы можем использовать робастную сэндвич оценку ковариационной матрицы:
\begin{equation}
\widehat{\Var}[\hat{\beta}_{SFGLS}]= \left[ \sum_{i=1}^N X'_i {\hat{\Omega}_i}^{-1} X_i \right]^{-1} \sum_{i=1}^N X'_i {\hat{\Omega}_i}^{-1} \hat{u}_i \hat{u}'_i  {\hat{\Omega}_i}^{-1} X_i  \left[ \sum_{i=1}^N X'_i {\hat{\Omega}_i}^{-1} X_i \right]^{-1},
\end{equation}
где $\hat{\Omega}_i=\Omega_i(\hat{\gamma})$.

Чаще всего предполагают, что  $\Omega_i$ не зависит от $i$. Тогда $\Omega_i=\Omega$ является матрицей размера $G \times G$, которую можно состоятельно оценить для конечного $G$ и $N \rightarrow \infty$ с помощью:
\begin{equation}
\hat{\Omega}= \sum_{i=1}^N \hat{u}_i \hat{u}'_i 
\end{equation}
где $\hat{u}_i=y_i-X_i \hat{\beta}_{SOLS}$. Тогда оценка системы ДОНМНК совпадает с формулой (6.86) с $\hat{\Omega}$ вместо $\hat{\Omega}_i$, и после некоторых алгебраических преобразований оценка системы ДОНМНК также может быть записана в виде:
\begin{equation}
\hat{\beta}_{SFGLS} = \left[ X' \left( {\hat{\Omega}}^{-1} \otimes I_N \right) X \right]^{-1} X' \left( {\hat{\Omega}}^{-1} \otimes I_N \right) y',
\end{equation}
где $\otimes$ означает Кронекерово произведение. Предположение, что $\Omega_i=\Omega$ исключает, например, гетероскедастичность по $i$. Это сильное предположение, и во многих случаях на практике лучше использовать робастные стандартные ошибки, посчитанные с применением формулы (6.87). При этом будут получаться  правильные стандартные ошибки, даже если $\Omega_i$ зависит от $i$.

\subsection{Внешне несвязанные уравнения}

В модели внешне не связанных уравнений (Seemingly Unrelated Regressions, SUR) $g$-ое из $G$ уравнений
для $i$-го из $N$ индивидов задается формулой:
\begin{equation}
y_{ig}=x'_{ig} \beta_g + u_{ig}, g=1, \dots, G, i=1, \dots, N,
\end{equation}
где $x_{ig}$ --- регрессоры, предполагаемые экзогенными, и $\beta_g$ --- векторы параметров размера $K_g \times 1$. Например, для данных по спросу на $G$ товаров для $N$ индивидов, $y_{ig}$ может быть расходами $i$-ого индивида на товар $g$ или доля бюджета на товар $g$. Далее $G$ предполагается фиксированным и относительно небольшим при $N \rightarrow \infty$. Обратите внимание, что мы индексируем $y$ в порядке $y_{ig}$, поскольку результаты тогда можно легко использовать для панельных данных с переменной $y_{it}$ (см. Раздел 6.9.4). Другие авторы используют обратный порядок $y_{gi}$.

Модель SUR была предложена Зеллнером (1962). Термин внешне не связанные уравнения обманчив, поскольку ясно, что уравнения связаны, если ошибки $u_{ig}$ в разных уравнениях коррелируют. Для модели SUR связь между $y_{ig}$ и $y_{ih}$ является косвенной, она возникает из-за корреляции между ошибками в разных уравнениях.

При оценивании сочетаются наблюдения и по уравнениям, и по индивидам. В микроэконометрических приложениях, где предполагается независимость по $i$, наиболее удобно сначала расположить все уравнения для данного индивида друг под другом. Поставив таким образом $G$ уравнений для $i$-ого индивида получаем:
\begin{equation}
\begin{bmatrix} y_{i1} \\ \vdots \\ y_{iG} \end{bmatrix} = \begin{bmatrix}
x'_{i1} & 0 & 0 \\ 0 & \ddots & 0 \\ 0 & 0 & x'_{iG} \end{bmatrix} \begin{bmatrix} \beta_{1} \\ \vdots \\ \beta_{G} \end{bmatrix} + \begin{bmatrix} u_{i1} \\ \vdots \\ u_{iG} \end{bmatrix}.
\end{equation}
Данное выражение имеет вид $y_i=X_i \beta + u_i $ формулы (6.79), где $y_i$ и $u_i$ --- векторы размера $G \times 1$ с $g$-ыми элементами $y_{ig}$ и $u_{ig}$, $X_i$ --- матрица размера $G \times K$ с $g$-ой строкой $[0 \dots x'_{ig} \dots 0]$, и $\beta=[\beta'_1 \dots \beta'_G]'$ --- вектор размера $K \times 1$, где $K=K_1+ \cdots + K_G$. Некоторые авторы наоборот упорядочивают наблюдения по  всем индивидам для данного уравнения, что приводит к различным алгебраическим выражениям для одинаковых оценок.

Учитывая определения $X_i$ и $y_i$, легко показать, что $\hat{\beta}_{SOLS}$ в (6.83) имеет вид
\[
\begin{bmatrix} \hat{\beta}_1 \\ \vdots \\ \hat{\beta}_G \end{bmatrix} = \begin{bmatrix} \left[ \sum_{i=1}^N x_{i1} x'_{i1} \right]^{-1} \sum_{i=1}^N x_{i1} y_{i1} \\ \vdots \\ \left[ \sum_{i=1}^N x_{iG} x'_{iG} \right]^{-1} \sum_{i=1}^N x_{iG} y_{iG} \end{bmatrix}.
\]
То есть МНК для систем уравнений даёт такие же оценки как МНК применяемый по отдельности к каждому уравнению.  Как и следовало ожидать априори, если единственная связующая между уравнениями --- это ошибка, и ошибки рассматриваются как некоррелированные, то совместная оценка сводится к оценке отдельных уравнения.

Лучшая оценка --- это оценка ДОМНК, определённая в (6.86), с использованием $\hat{\Omega}$ в (6.88) и статистических выводов, которые основаны на асимптотической дисперсии, приведённой в (6.87). Эта оценка, как правило, более эффективна, чем оценка системы МНК, хотя можно показать, что она сходится к оценке МНК, если ошибки в разных  уравнениях не коррелированы или если одни и те же регрессоры используются в каждом уравнении.

В моделях внешне не связанных уравнений могут накладываться ограничения на параметры в разных уравнениях. Например, ограничение симметричности может означать, что коэффициент второго регрессора в первом уравнении равен коэффициенту первого регрессора во втором уравнении. Если такие ограничения --- равенства, можно легко оценить модель с помощью соответствующего переопределения $X_i$ и $\beta$, приведённых в (6.79). Например, если есть два уравнения и ограничение $\beta_2=-\beta_1$, тогда определяем $X_i=[x_{i1} \, -x_{i2}]'$ и $\beta=\beta_1$. Также можно воспользоваться обобщением МНК или НМНК для систем уравнений с линейными ограничениями на параметры.

В системах уравнений возможно, что в результате добавления ограничений ковариационная матрица вектора ошибок $u_i$ становится вырожденной. Например, предположим, что $y_{ig}$ --- $i$-ая доля бюджета и модель имеет вид $y_{ig}= \alpha_g + z'_i \beta_g + u_{ig}$, где во всех уравнениях одинаковые регрессоры. Тогда $\sum_g y_{ig}=1$, так как сумма долей бюджета равна 1, что требует $\sum_g \alpha_{g}=1$, $\sum_g \beta_{g}=0$ и $\sum_g u_{ig}=0$. Последнее ограничение означает что матрица $\Omega_i$ является вырожденной и, следовательно, необратимой. Можно убрать одно уравнение, скажем последнее, и оценить модель для системы оставшихся $G-1$ уравнения. Тогда оценки параметров для $G$-ого уравнения могут быть получены с помощью ограничений. Например, $\hat{\alpha}_G=1-(\hat{\alpha}_1 + \cdots + \hat{\alpha}_{G-1})$. Кроме того, можно ввести ограничения в виде равенств на параметры на этом этапе. В литературе описываются методы гарантирующие, что получаемые оценки инвариантны к выбору удаляемого уравнения; см., например, Берндт и Савин (1975).

\subsection{Панельные данные}

Другим важным применением методов НМНК для систем уравнений являются панельные данные, где скалярная зависимая переменная наблюдается в каждом из $T$ периодов времени для $N$ индивидов. Панельные данные можно рассматривать как систему уравнений, либо $T$ уравнений для $N$ индивидов или $N$ уравнений для $T$ периодов времени. В микроэконометрике мы предполагаем короткую панель с малым $T$ и $N \rightarrow \infty$, поэтому вполне естественно взять скалярную зависимую переменную $y_{it}$, где $g$-ое уравнение в предыдущем обсуждении теперь интерпретируется как $t$-ый период времени и $G=T$.

Простая модель панельных данных имеет вид
\begin{equation}
y_{it}=x'_{it} \beta + u_{it}, t=1, \dots, T, i=1, \dots, N,
\end{equation}
частный случай (6.90) с постоянным вектором $\beta$. Тогда в (6.79) матрица регрессоров приобретает вид $X_i=[x_{i1} \dots x_{iT}]'$. После некоторых алгебраических преобразований оценки МНК для систем уравнений, определённые в (6.83), можно выразить как:
\begin{equation}
\hat{\beta}_{POLS}= \left[ \sum_{i=1}^{N} \sum_{i=1}^{T} x_{it} x'_{it} \right]^{-1} \sum_{i=1}^{N} \sum_{i=1}^{T} y_{it}x'_{it}.
\end{equation}

Эта оценка называется сквозной МНК оценкой, так как она объединяет или сочетает в себе аспекты cross-section данных и временных рядов.

Сквозная МНК оценка может быть получена с помощью МНК регрессии  $y_{it}$ на $x_{it}$. Однако
если $u_{it}$ коррелируют по $t$ для заданного $i$, стандартные ошибки МНК, которые даются по умолчанию и которые предполагают независимость ошибки и по $i$, и по $t$, являются неправильными и могут быть в значительной степени смещены. Вместо этого статистические выводы должны основываться на робастной форме  ковариационной матрицы, приведённой в (6.84). Это подробно описано в Разделе 21.2.3. На практике оцениваются более сложные модели, чем (6.92), которые включают отдельные специфические эффекты (см. Раздел 21.2).

\subsection{Метод инструментальных переменных для систем уравнений}

Оценка одного линейного уравнения с эндогенными регрессорами была представлена в Разделе 6.4. Теперь мы обобщим её на многомерную линейную модель (6.79), для случая $\E[u_i|X_i] \not= 0$. Брунди и Йоргенсон (1971) рассматривали оценку метода инструментальных переменных в применении к системам уравнений для получения состоятельных и эффективных оценок.

Мы предполагаем существование матрицы инструментов $Z_i$ размера $G \times R$, которая удовлетворяет условию $\E[u_i|Z_i] = 0$ и, следовательно,
\begin{equation}
\E[Z'_i(y_i -X_i \beta)]=0.
\end{equation}
Эти инструменты могут быть использованы для получения оценок параметров для каждого уравнения по отдельности, но совместная оценка уравнений может повысить эффективность. Оценка ОММ систем минимизирует:
\begin{equation}
\mathcal{Q}_{N}(\beta)= \left[  \sum_{i=1}^{N} Z'_i(y_i -X_i \beta) \right]' W_N \left[  \sum_{i=1}^{N} Z'_i(y_i -X_i \beta) \right],
\end{equation}
где $W_N$ --- матрица весов размера $r \times r$. Некоторые алгебраические преобразования приводят к
\begin{equation}
\hat{\beta}_{SGMM}=[X' Z W_N Z' X]^{-1} [X' Z W_N Z' y],
\end{equation}
где $X$ представляет собой матрицу размера $NG \times K$, полученную путём постановки друг под другом $X_1, \dots, X_N$ (см. (6.81)), и $Z$ является матрицей размера $NG \times r$, полученной аналогичной постановкой друг под другом $Z_1, \dots, Z_N$. Оценка ОММ систем имеет точно такой же вид, как и (6.37), и асимптотическая ковариационная матрица --- это та, которая дана в (6.39). Отсюда следует, что робастная оценка ковариационной матрицы --- это
\begin{equation}
\widehat{\Var}[\hat{\beta}_{SGMM}]= N \left[ X' Z W_N Z' X \right]^{-1} \left[ X' Z W_N \hat{S} W_N Z' X \right] \left[ X' Z W_N Z' X \right]^{-1}.
\end{equation}
Предполагая независимость по $i$ для систем уравнений мы получаем
\begin{equation}
\hat{S}= \frac{1}{N} \sum_{i=1}^{N} Z'_i \hat{u}_i \hat{u}_i' Z_i.
\end{equation}

Нескольким вариантам матрицы весов уделяют особое внимание.

Во-первых, оценка оптимального ОММ для систем уравнений --- (6.96) с $W_N={\hat{S}}^{-1}$, где $\hat{S}$ определена в (6.98). Тогда ковариационная матрица упрощается до
\[
\widehat{\Var}[\hat{\beta}_{SGMM}]= N \left[ X' Z  {\hat{S}}^{-1} Z' X \right]^{-1}.
\]
Эта оценка является наиболее эффективной оценкой ОММ, основанной на условиях моментов (6.94). Выигрыш в эффективности возникает из двух факторов: (1) оценивание системы уравнений целиком. При этом допускается корреляция ошибок между разными уравнениями, т.е. $\Var[u_i|Z_i]$ необязательно должна быть блочно-диагональной. (2) допущение достаточно общего вида гетероскедастичности и корреляции, так что $\Omega_i$ может меняться по $i$.

Во-вторых, оценка двухшаговым МНК для систем уравнений.  Для неё $W_N=(N^{-1} Z' Z)^{-1}$. Рассмотрим модель внешне не связанных уравнений, определённую в (6.91), с некоторыми эндогенными регрессорами $x_{ig}$. Тогда двухшаговый МНК для систем уравнений сводится к двухшаговому МНК для каждого уравнения по отдельности с инструментами $z_g$ для $g$-ого уравнения и матрицей инструментов
\begin{equation}
Z_i= \begin{bmatrix} z'_{i1} & 0 & 0 \\ 0 & \ddots & 0 \\ 0 & 0 & z'_{iG} \end{bmatrix}.
\end{equation}
Во многих приложениях $z_1=z_2= \cdots = z_g$. В этом случае общий набор инструментов используется во всех уравнениях, но мы не должны ограничивать анализ только этим случаем. Для модели панельных данных (6.92) двухшаговый МНК для систем уравнений сводится к сквозной оценке двухшагового МНК, если мы определим $Z=[z_{i1} \dots z_{iT}]'$.

В-третьих, предположим, что $\Var[u_i|Z_i]$ не меняется по $i$, т.е. $\Var[u_i|Z_i]=\Omega$. Это аналог предположения о гомоскедастичности для системы уравнений. Тогда, как и с (6.88), состоятельная оценка для $\Omega$ --- это $\hat{\Omega}=N^{-1} \sum_i \hat{u}_i \hat{u}'_i$, где $\hat{u}_i$ --- остатки, основанные на состоятельной оценке метода инструментальных переменных, например, подойдёт оценка двухшагового МНК для системы уравнений. Тогда оценка оптимального ОММ --- (6.96) с $W_N = I_N \otimes \hat{\Omega}$. Отметим, что эта оценка отличается от оценки трёхшагового метода наименьших квадратов, представленной в конце следующего равздела.

\subsection{Системы линейных одновременных уравнений}

Модель линейных одновременных уравнений, введённая в Разделе 2.4, очень важна и часто достаточно подробно излагается в вводных курсах эконометрики бакалаврского уровня. В этом разделе мы приводим очень краткое резюме. Обсуждение идентификации перекликается с тем, которое приведено в Главе 2. В связи с наличием эндогенных переменных оценки МНК и для внешне не связанных уравнений несостоятельны. Состоятельные методы оценки лежат в рамках ОММ, хотя стандартные методы были разработаны задолго до ОММ.

Модель одновременных уравнений определяет $g$-ое из $G$ уравнений для $i$-го из $N$ индивидов, как
\begin{equation}
y_{ig}=z'_{ig} \gamma_g + Y'_{ig} \beta_g + u_{ig}, g=1, \dots, G,
\end{equation}
где порядок индексов --- тот, что в Разделе 6.9, а не в Разделе 2.4, $z_g$ --- вектор экзогенных регрессоров, которые, как предполагается, не коррелируют с ошибками $u_g$, и $Y_g$ --- вектор, содержащий подмножество зависимых переменных $y_1, \dots, y_{g-1}, y_{g+1}, \dots, y_G$ других $G-1$ уравнений. Вектор $Y_g$ является эндогенным, так как он коррелирует с ошибками модели. Модель для $i$-го индивида может быть так же записана в виде:
\begin{equation}
y'_i B + z'_i \Gamma =u_i,
\end{equation}
где $y=[y_{i1} \dots y_{iG}]'$ --- вектор эндогенных переменных размера $G \times 1$, $z_i$ --- вектор экзогенных переменных размера $r \times 1$, что является объединением $z_{i1}, \dots, z_{iG}, u_i=[u_{i1} \dots u_{iG}]'$ --- вектор ошибок размера $G \times 1$, $B$ ---матрица параметров размера $G \times G$ с диагональными элементами единицами, $\Gamma$ --- матрица параметров размера $r \times G$, а некоторые из членов $B$ и $\Gamma$ равны единице. Предполагается, что $u_i$ нормально распределены и независимо  по $i$ с математическим ожиданием $0$ и ковариационной матрицей $\Sigma$.

Модель (6.101) называется структурной формой.  Различные ограничения на $B$ и $\Gamma$ задают различные структуры. Если выразить эндогенные переменные через экзогенные, мы получим приведенную форму:
\begin{equation}
\begin{split}
y_i=-z'_i \Gamma B^{-1} + u_i  B^{-1} \\
= z'_i \Pi+ v_i,
\end{split}
\end{equation}
где $\Pi=-\Gamma B^{-1}$ --- матрица параметров в приведенной форме размера $r \times G$ и $ v_i=u_i B^{-1}$ --- приведенная форма вектора ошибок с ковариационной матрицей $\Omega=(B^{-1})' \Sigma B^{-1}$.

Приведённая форма может быть состоятельна оценена с помощью МНК, который даёт оценки $\Pi=-\Gamma B^{-1}$ и $\Omega=(B^{-1})' \Sigma B^{-1}$. Проблема идентификации, см. Раздел 2.5, заключается в получении уникальных оценок структурной формы параметров $B$, $\Gamma$ и $\Sigma$. Это требует некоторых ограничений на параметры, поскольку без ограничений $B$, $\Gamma$, и $\Sigma$ содержат на $G^2$ параметров больше, чем $\Pi$ и $\Omega$. Необходимым условием для идентификации параметров в $g$-ом уравнении является условие порядка для того, чтобы число экзогенных переменных, исключённых из $g$-ого уравнения, было не менее, чем число включённых эндогенных переменных. Это тоже самое, что и условие порядка, указанное в Разделе 6.4.1. Например, если $Y_{ig}$ в (6.100) имеет одну компоненту, то есть одна эндогенная переменная в уравнении, тогда по крайней мере одна из компонент $x_i$ не должна быть включена. Это гарантирует, что существует столько инструментов, сколько регрессоров.
Достаточное условие идентификации --- более сильное условие ранга. Оно даётся во многих книгах таких, как книга Грина (2003), и для краткости оно не приводится здесь. Другие ограничения такие, как ограничения на ковариации, могут также привести к идентификации.

С учётом идентификации структурные параметры модели могут быть состоятельно оценены с помощью отдельной оценки каждого уравнения двухшаговым методом наименьших квадратов, определённым в (6.44). Тот же набор инструментов $z_i$ используется для каждого уравнения. В $g$-ом уравнении подкомпонента $z_{ig}$ используется как инструмент для себя, а остальная часть $z_i$ используется в качестве инструмента для $y_{ig}$.

Более эффективные оценки системы могут быть получены с использованием оценки трёхшагового метода наименьших квадратов Зеллнера и Тейла (1962), которая предполагает, что ошибки гомоскедастичны, но коррелируют в уравнениях. Во-первых, оценим приведенную форму коэффициентов $\Pi$ в (6.102) с помощью МНК регрессии $y$ на $z$. Во-вторых, получим оценки двухшагового метода МНК с помощью регрессии МНК (6.100), где $Y_g$ заменяется на прогнозы для приведенной формы $\hat{Y}_g=z' \hat{\Pi}_g$. Это МНК регрессия $y_g$ на $\hat{Y}_g$ и $z_g$, или, что эквивалентно, $y_g$ на $\hat{x}_g$, где $\hat{x}_g$ --- прогнозы $Y_g$ и $z_g$ из МНК регрессии на $z$. В-третьих, получим оценку трёхшагового метода наименьших квадратов с помощью МНК для систем, построив регрессию  $y_g$ на $\hat{x}_g, g=1, \dots, G$. Тогда из (6.89):
\[
\hat{\theta}_{3SLS} = \left[ \hat{X'} \left( {\hat{\Omega}_1}^{-1} \otimes I_N \right) \hat{X} \right]^{-1} \hat{X'} \left( {\hat{\Omega}_1}^{-1} \otimes I_N \right)y,
\]
где $\hat{X}$ получается так: сначала формируются блочно-диагональные матрицы $\hat{X}_i$ с  блоками $\hat{x}_{i1}, \dots, \hat{x}_{iG}$, а затем $\hat{X}_1, \dots, \hat{X}_N$ ставятся друг на друга, $\hat{\Omega}=N^{-1} \sum_i \hat{u}_i \hat{u}'_i$, $\hat{u}_i$ --- векторы остатков, рассчитанные с использованием оценок двухшагового МНК.

Эта оценка совпадает с оценкой ОММ для систем уравнений с $W_N= I_N \otimes \hat{\Omega}$ в случае, когда оценка ОММ системы использует одинаковые инструменты в каждом уравнении. В противном случае, оценки трёхшагового МНК и оценки ОММ для систем отличаются, хотя оба метода дают состоятельные оценки, если $\E[u_i|z_i]=0$.

\subsection{Оценка ММП для систем уравнений}

Чаще всего для систем уравнений используют оценки МНК или метода инструментальных переменных с выводами, основанными на скорректированных стандартных ошибках. Теперь дополнительно предположим, что ошибки независимы и одинаково распределены, $u_i \sim \mathcal{N}[0,\Omega]$.

Для систем с экзогенными регрессорами оценки ММП асимптотически эквивалентны оценкам НМНК. Однако эти оценки используют разные оценки $\Omega$ и, следовательно, $\beta$, поэтому в небольших выборках появляются различия между оценками ММП и НМНК. Например, см. Главу 21 для модели случайных эффектов по панельным данным.

Для линейной модели одновременных уравнений (6.101) оценка ММП с ограниченной информацией, оценка ММП для отдельных уравнений, асимптотически эквивалентна оценке двухшагового МНК. Оценка ММП с полной информацией, оценка ММП для системы уравнений, асимптотически эквивалентна оценке трёхшагового МНК. См., например, Шмидт (1976) и Грин (2003).

\section{Нелинейные системы уравнений}

Рассмотрим теперь системы уравнений, которые нелинейны по параметрам. Например, система уравнений спроса, полученных по заданной прямой или косвенной функции полезности, может быть нелинейна по параметрам. Вообще, если отдельная переменная описывается нелинейной моделью, например, логит моделью или моделью Пуассона, то любая совместная модель для двух или более таких переменных обязательно будет нелинейной.

Мы начинаем с обсуждения полностью параметрического моделирования систем уравнений до рассмотрения вопроса о частично параметрическом моделировании. Как и в линейном случае мы приводим модели с экзогенными регрессорами, прежде чем рассматривать эндогенные регрессоры.

\subsection{ММП оценка нелинейных систем}

Оценка ММП для одной зависимой переменной была представлена в Разделе 5.6. Эти результаты могут быть применены к совместным моделям нескольких зависимых переменных с очень небольшим изменением, что условная плотность с одной зависимой переменной $f(y_i|x_i,\theta)$ становится $f(y_i|X_i,\theta)$, где $y_i$ обозначает вектор зависимых переменных, $X_i$ обозначает все регрессоры, и $\theta$ обозначает все параметры.

Например, если $y_1 \sim \mathcal{N}[\exp(x'_1 \beta_1),{\sigma_1}^2]$ и $y_2 \sim \mathcal{N}[\exp(x'_2 \beta_2),{\sigma_2}^2]$, тогда подходящая совместная модель заключаться в том, чтобы считать, что $(y_1,y_2)$ имеют двумерное нормальное распределение с математическим ожиданием $\exp(x'_1\beta_1)$ и $\exp(x'_2\beta_2)$, дисперсиями $\sigma^2_1$ и $\sigma^2_2$ и корреляцией $\rho$.

Для данных, которые не нормально распределены, могут быть сложности в спецификации и выборе достаточно гибкого совместного распределения. Например, для одномерных счётных данных стандартная исходная модель --- отрицательная биномиальная (см. главу 20). Для обобщения этой модели на двумерные или многомерные счётные данные существует несколько альтернативных отрицательных биномиальных моделей на выбор. Они могут отличаться, например, тем, какое распределение  предполагается отрицательным биномиальным: одномерное условное  или одномерное предельное. Для многомерного нормального распределения и условное и предельное распределение являются  нормальными. Все эти многомерные отрицательные биномиальные распределения накладывают некоторые ограничения на диапазон корреляции. Например,  ограничение на положительность корреляцию. А для многомерного нормального распределения нет таких ограничений.

К счастью, современные прорывы в вычислительных методах позволяют специфицировать более сложные модели. Например, разумно гибкая модель для коррелированных двумерных счётных данных заключается в предположении, что зависимый от ненаблюдаемых $\varepsilon_1$ и $\varepsilon_2$, $y_1$ распределён по Пуассону с математическим ожиданием $\exp(x'_1 \beta_1 + \varepsilon_1)$ и $y_2$ распределён по Пуассону с математическим ожиданием $\exp(x'_1 \beta_1 + \varepsilon_2)$. Двумерное распределение для исходных данных может быть получено в предположении, что ненаблюдаемые переменные $\varepsilon_1$ и $\varepsilon_2$ являются двумерными нормально распределёнными. Явного решения для этого двумерного распределения не существует, но параметры, тем не менее, можно оценить, используя симуляционный метод максимального правдоподобия, представленный в Разделе 12.4.

Ряд примеров нелинейных совместных моделей приведён в 4-ой части книги. Простейшие совместные модели могут быть негибкими, таким образом, состоятельность может быть основана на предположениях о распределении, которые являются слишком строгими. Тем не менее, как правило, теоретически нет препятствий для спецификации более гибких моделей, которые можно оценить с помощью методов, активно использующих вычисления.

В частности, два основных метода построения сложных многомерных параметрических моделей представлены подробно в Разделе 19.3. Эти методы приведены в контексте модели данных по длительности, но они имеют гораздо более широкое применение. Во-первых, можно ввести коррелированную ненаблюдаемую гетерогенность, как в только что приведённом примере двумерных данных. Во-вторых, можно использовать копулы, обеспечивающие способ генерации совместного распределения при заданных  одномерных предельных распределениях.

Для оценки ММП более простой, хотя и менее эффективной, используется подход квази-ММП, который заключается в спецификации отдельных параметрических моделей для $y_1$ и $y_2$ и получении оценки ММП, предполагая независимость $y_1$ и $y_2$, но при построении статистических выводов учитывается корреляция между $y_1$ и $y_2$. Это подход был  представлен в Разделе 5.7.5. В оставшейся части этого раздела мы рассмотрим аналогичные частично параметрические подходы.

Проблем больше, если есть эндогенность, то есть зависимая переменная в одном уравнении появляется как регрессор в другом уравнении. Существует мало моделей для нелинейных систем одновременных уравнений, помимо нелинейных регрессионных моделей с аддитивными нормальными ошибками.

\subsection{Нелинейные системы уравнений}

Для линейной регрессии переход от одного уравнения к нескольким уравнениям ясен, так как отправной точкой является линейная модель $y=x' \beta +u $ и оценивание ведётся методом наименьших квадратов. Эффективные оценки системы тогда могут быть получены как оценки  НМНК для системы уравнений. Для нелинейных моделей намного больше разнообразия в отправной точке и методе оценки.

Мы определяем многомерную нелинейную модель с $G$ зависимыми переменными следующим образом:
\begin{equation}
r(y_i,X_i,\beta)=u_i,
\end{equation}
где $y_i$ и $u_i$ --- векторы размера $G \times 1$, $r(y_i,X_i,\beta)$ --- векторная функция размера $G \times 1$, $X_i$ --- матрица размера $G \times L$, а $\beta$ --- вектор-столбец размера $K \times 1$. В этом разделе мы делаем обычное  предположение для пространственных данных, что вектор ошибок $u_i$ независим по $i$, но компоненты $u_i$ для заданного $i$ могут коррелировать, а дисперсии и ковариации могут зависеть от $i$.

Один из примеров (6.103) --- нелинейная модель внешне не связанных регрессий. Тогда $g$-ое из $G$ уравнений для $i$-го из $N$ индивидов задаётся таким образом:
\begin{equation}
r_g(y_{ig},x_{ig},\beta_g)=u_{ig}, g=1, \dots, G.
\end{equation}
Например, $u_{ig}=y_{ig}-\exp(x'_{ig} \beta_g)$. Тогда $u_i$ и $r(\cdot)$ в (6.103) --- векторы размера $G \times 1$ с $g$-ыми элементами $u_{ig}$ и $r_g(\cdot)$, $X_i$ такая же блочно-диагональная матрица как в (6.91), и $\beta$ получается путём постановки друг под другом $\beta_1$, $\beta_G$.

Вторым примером является нелинейная модель панельных данных. Тогда для отдельного индивида $i$ в период $t$
\begin{equation}
r(y_{it},x_{it},\beta)=u_{it}, t=1, \dots, T.
\end{equation}
Тогда $u_i$ и $r(\cdot)$ в (6.103) --- векторы размера $T \times 1$, поэтому $G=T$, с $i$-ыми элементами $u_{it}$ и $r(y_{it},x_{it},\beta)$. Модель панельных данных отличается от модели внешне не связанных уравнений тем, что имеет одинаковую функцию $r(\cdot)$ и параметры $\beta$ в каждом периоде.

\subsection{Оценка нелинейных систем}

Когда регрессоры $X_i$ в модели (6.103) являются экзогенными:
\begin{equation}
\E[u_i|X_i]=0,
\end{equation}
где $u_i$ --- ошибки, определенные в (6.103). Мы предполагаем, что ошибки независимы по $i$, а ковариационная матрица выглядит так:
\begin{equation}
\Omega_i = \E[u_iu_i'|X_i].
\end{equation}

\begin{center}
Аддитивнные ошибки
\end{center}

Оценивание нелинейным систем --- это прямое обобщение оценивания систем линейных моделей с помощью МНК и ДОМНК, если ошибки аддитивны. В таком случае можно преобразовать (6.103) в
\begin{equation}
u_i = y_i - g(X_i,\beta).
\end{equation}
В этом случае оценка НМНК для систем минимизирует сумму квадратов остатков $\sum_i u_i'u_i$, в то время как оценка ДОНМНК систем минимизирует
\begin{equation}
Q_N(\beta) = \sum_i u_i'\hat{\Omega}_i^{-1} u_i, 
\end{equation}
где мы задаём модель $\Omega_i(\gamma)$ для $\Omega_i$ и $\hat{\Omega}_i = \Omega_i(\hat{\gamma})$. Чтобы  предотвратить возможную неправильную спецификацию $\Omega_i$, можно использовать скорректированные стандартные ошибки, которые требуют лишь, чтобы $u_i$ были независимы и удовлетворяли (6.106). Тогда оценённая дисперсии оценки ДОНМНК систем такая же, что и для оценки ДОМНК линейных систем из (6.87). Только в данном случае $X_i$ заменён на $\partial{g(y_i,\beta)}/\partial{\beta'}|_{\hat{\beta}}$, и теперь $\hat{u}_i = y_i - g(X_i,\hat{\beta})$. Можно получить более простую оценку дисперсии с помощью НМНК для систем уравнений, заменяя $\hat{\Omega}_i$ на $I_G$.

Основная сложность состоит в том, чтобы специфицировать удачную модель для $\Omega_i$. В качестве примера предположим, что мы хотим хотим совместно моделировать две счетные переменные. В Главе 20 мы покажем, что стандартная модель для счетных данных, немного более общая, чем модель Пуассона, задаёт условное математическое ожидание как $\exp(x'\beta)$ и задаёт условную дисперсию так, чтобы она была кратна $\exp(x'\beta)$. Совместную модель может задать в виде $u = \begin{bmatrix} u_1 & u_2 \end{bmatrix}'$, где $u_1 = y_1 - \exp(x_1'\beta_1)$ и $u_2 = y_2 - \exp(x_2'\beta_2)$. Тогда ковариационная матрица $\Omega_i$ имеет диагональные элементы $\alpha_1 \exp(x_{i1}'\beta_1)$ и $\alpha_2 \exp(x_{i2}'\beta_2)$. В таком случае одной из возможных параметризацией ковариации будет $\alpha_3[\exp(x_{i1}'\beta_1)\exp(x_{i2}'\beta_2)]^{1/2}$. Оценка $\hat{\Omega}_i$ требует наличия оценок $\beta_1, \beta_2, \alpha_1, \alpha_2$ и $\alpha_3$, которые могут быть получены из первого шага оценивания  уравнений по отдельности.

\begin{center}
Неаддитивные ошибки
\end{center}

Регрессия наименьших квадратов с неаддитивными ошибками больше не применима, как было показано в случае одного уравнения в разделе 6.2.2. Вулдридж (2002) представляет состоятельный метод  моментов.

Ограничение на условный момент (6.106) приводит к многим возможным условиям на безусловные моменты, которые можно использовать для оценивания. Очевидная отправная точка --- основывать оценивание на моментных условиях $\E[X_i'u_i] = 0$. Однако можно использовать другие моментные условия. В более общем случае мы рассматриваем оценивание, основанное на $K$ моментных условиях
\begin{equation}
\E[R(X_i,\beta)'u_i] = 0,
\end{equation}
где $R(X_i,\beta)$ --- матрица функций $X_i$ и $\beta$ имеет размер $K \times G$. Спецификация $R(X_i,\beta)$ и возможная зависимость от $\beta$ описаны далее.

По построению существует столько же условий моментов, сколько параметров. Оценка ММ для систем уравнений $\hat{\beta}_{SMM}$ является решением соответствующих выборочных моментных условий 
\begin{equation}
\frac{1}{N} \sum_{i=1}^N R(X_i,\beta)'r(y_i,X_i,\hat{\beta}_{SMM}) = 0,
\end{equation}
где на практике $R(X_i, \beta)$ оценивается в точке $\tilde{\beta}$, являющейся предварительной оценкой. Оценка ММ для систем уравнений асимптотически нормальна с ковариационной матрицей
\begin{equation}
\hat{V}[\hat{\beta}_{SMM}] = \left[ \sum_{i=1}^N \hat{D}_i'\hat{R}_i \right]^{-1} \sum_{i=1}^N \hat{R}_i'\hat{u}_i\hat{u}_i'\hat{R}_i \left[ \sum_{i=1}^N \hat{R}_i'\hat{D}_i \right]^{-1},
\end{equation}
где $\hat{D}_i = \partial{r_i}/\partial{\beta'}|_{\hat{\beta}}$, $\hat{R}_i = R(X_i, \hat{\beta})$ и $\hat{u}_i = r(y_i, X_i, \hat{\beta}_{SMM})$.

Основной вопрос --- спецификация $R(X, \beta)$ из (6.110). Из Раздела 6.3.7 наиболее эффективная оценка, основанная на (6.106), задаёт
\begin{equation}
R^*(X_i, \beta) = \E \left[ \frac{\partial{r(y_i, X_i, \beta)'}}{\partial{\beta}}|X_i \right] \Omega_i^{-1}
\end{equation}

В общем случае первое ожидание правой части требует сильных предположений о распределении, что затрудняет оптимальное оценивание. 

Однако выражение можно упростить, если нелинейная модель содержит аддитивные ошибки из (6.108). Тогда $R^*(X_i, \beta) = \partial{g(X_i,\beta)'}/\partial{\beta} \times \Omega_i^{-1}$, и оцениваемые уравнения (6.110) превращаются в
\[
N^{-1} \sum_{i=1}^N \frac{\partial{g(X_i,\beta)'}}{\partial{\beta}}\Omega_i^{-1}(y_i - X_i'\hat{\beta}_{SMM}) = 0.
\]
Эта оценка асимптотически эквивалентна оценке ДОМНК системы, которая минимизирует (6.109).

\subsection{Оценка метода инструментальных переменных для нелинейных систем}

Когда регрессоры $X_i$ из модели (6.103) эндогенны и $\E[u_i|X_i] \not= 0$, мы предполагаем существование матрицы инструментов $Z_i$ размером $G \times r$ такой, что
\begin{equation}
\E[u_i|Z_i] = 0,
\end{equation}
где $u_i$ --- ошибки, которые определены в (6.103). Мы предполагаем, что ошибки независимы по $i$ и ковариационная матрица имеет вид: $\Omega_i = \E[u_iu_i'|Z_i]$. Для нелинейной  модели внешне не связанных уравнений $Z_i$ определено в (6.99).

Этот подход аналогичен тому, который используется в предыдущем разделе для оценки ММ систем с дополнительной трудностью в том, что теперь может быть избыток инструментов. Это приводит к необходимости оценивания с помощью ОММ, а не просто ММ. Ограничение на условные моменте (6.106) приводит ко многим возможным условиям на безусловные моменте, которые могут использоваться для оценивания. Здесь мы поступаем, как и многие другие, основывая оценивание на моментных  условиях $\E[Z_i'u_i] = 0$. Тогда оценка ОММ систем минимизирует
\begin{equation}
Q_N(\beta) = \left[ \sum_{i=1}^N Z_i'r(y_i,X_i, \beta) \right]'W_N \left[ \sum_{i=1}^N Z_i'r(y_i,X_i, \beta) \right].
\end{equation}
Эта оценка асимптотически нормальна с оценкой ковариационной матрицы
\begin{equation}
\hat{\Var}[\hat{\beta}_{SGMM}] = N\left[ \hat{D}'ZW_N Z'\hat{D} \right]^{-1} \left[ \hat{D}'ZW_N\hat{S}W_NZ'\hat{D} \right] \left[ \hat{D}'ZW_N Z'\hat{D} \right]^{-1},
\end{equation}
где $\hat{D}'Z = \sum_i \partial{r_i'}/\partial{\beta}|_{\hat{\beta}}Z_i$ и $\hat{S} = N^{-1}\sum_i Z_i\hat{u}_i\hat{u}_i'Z_i'$. Мы также предполагаем, что $u_i$ независимы по $i$ с ковариационной матрицей $V[u_i|X_i] = \Omega_i$.

Выбор $W_N = [N^{-1}\sum_i Z_iZ_i']^{-1}$ соответствует нелинейному двухшаговому МНК в случае, если $r(y_i,X_i, \beta)$ получено из нелинейной  модели внешне не связанных уравнений. Выбор $W_N = [N^{-1}\sum_i Z_i\hat{\Omega}Z_i']^{-1}$, где $\hat{\Omega} = N^{-1}\sum_i \hat{u}_i\hat{u}_i'$, называется оценкой нелинейного трёхшагового МНК. Эта оценка является наиболее эффективной оценкой, основанной на моментных условиях $\E[Z_i'u_i] = 0$, если $\Omega_i = \Omega$. Выбор $W_N = \hat{S}^{-1}$ даёт наиболее эффективную оценку при более общем предположении, что $\Omega_i$ может меняться в зависимости от $i$. Однако, как правило, моментные условия, отличные от $\E[Z_i'u_i] = 0$, могут приводить к более эффективным оценкам.

\subsection{Нелинейные системы одновременных уравнений}

Модель нелинейных одновременных уравнений задаёт $g$-ое из $G$ уравнений для $i$-ого из $N$ индивидов следующим образом:
\begin{equation}
u_{ig} = r_g(y_i,x_{ig},\beta_g), g = 1, \dots, G.
\end{equation}
Это похоже на нелинейная модель внешне не связанных уравнений, но сейчас среди регрессоров могут быть зависимые переменные из других уравнений. В отличие от модели линейных одновременных уравнений существует мало полезных с практической точки зрения результатов, которые гарантируют, что модель нелинейных одновременных уравнений идентифицируема.

Если модель идентифицируема, то можно получить состоятельные оценки с использованием оценок ОММ, которые описаны в предыдущем разделе. С другой стороны, мы можем предположить, что $u_i \sim \mathcal{N}[0, \Omega]$, и получить нелинейную оценку ММП с полной информацией. В отличие от модели линейных одновременных уравнений, нелинейная оценка ММП с полной информацией в целом имеет асимптотическое распределение, которое отличается от распределения оценки нелинейного трёхшагового МНК. Также состоятельность нелинейной оценки ММП с полной информацией требует, чтобы ошибки были нормально распределены. Дополнительные подробности можно посмотреть у Амэмия (1985).

Бороться с эндогенностью в нелинейных моделях может быть сложно. Раздел 16.8 рассматривает одновременность в моделях тобит, где анализ проще, когда модель является линейной по латентным переменным. Раздел 20.6.2 представляет пример более сложной нелинейности и эндогенные регрессоры в моделях счётных данных.

\section{Практические соображения}

В идеальном случае можно найти оценки ОММ с помощью эконометрических пакетов, при этом требуются не сильно более глубокия знания, чем для оценивания при  помощи нелинейного метода наименьших квадратов с гетероскедастичными ошибками. Однако, не все широко применяемые эконометрические пакеты могут реализовывать оценивание с помощью ОММ. В зависимости от конкретного применения ОММ оценивание может подразумевать использование более подходящего пакета или использование матричного языка программирования и знание алгебры ОММ.

Распространённое применение ОММ --- оценивание с помощью метода инструментальных переменных. Большинство эконометрических пакетов включает линейный метод инструментальных переменных, но не включают нелинейные методы инструментальных переменных. По умолчанию стандартные ошибки могут предполагаться гомоскедастичнымы, а не скорректированными на гетероскедастичность. Как уже подчёркивалось в Главе 4, может быть трудно получить инструменты, которые не коррелируют с ошибкой, но в то же время достаточно коррелирует с регрессором, или в нелинейном случае соответствующие производные ошибки по параметрам.

Эконометрические пакеты обычно включают методы для  линейных систем, но не для  нелинейных. Опять же по умолчанию стандартные ошибки могут не иметь поправки на гетероскедастичность.

\section{Библиографические заметки}

Описание ОММ представлено в книгах Дэвидсона и МакКиннона (1993, 2004), Гамильтона (1994) и Грина (2003). Более свежие книги Хаяши (2000) и Вулдриджа (2002) делают упор на оценивание с помощью ОММ. Бера и Билиас (2002) описывают связь и историю многих оценок, представленных в главах 5 и 6.

\begin{itemize}
\item [$6.3$] Изначальное описание представлено у Хансена (1982). Хорошее объяснение оптимальных моментных условий для ОММ приведено в приложении у Арельяно (2003). Октябрьский выпуск 2002 года Журнала экономической и бизнес статистики (Journal of Business and Economic Statistics) посвящён оцениванию с помощью ОММ.
\item [$6.4$] Использование линейной оценки инструментальных переменных, описаное Сарганом (1958), является одним из основных предшественником ОММ.
\item [$6.5$] Оценка нелинейного ДМНК, которая были введены Амэмия (1974), легко обобщается до ОММ оценки.
\item [$6.6$] При описании последовательного двухшагового оценивания ссылаются на Ньюи (1984), Мерфи и
Топела (1985), и Пагана (1986).
\item [$6.7$] Описывая оценивание с помощью минимального расстояния, ссылаются на Чемберлена (1982).
\item [$6.8$] Хороший обзор эмпирического метода максимального правдоподобия приведён у Миттелхаммера, Джаджа и Миллера (2000). Ключевые работы, связанные с этой темой, --- работы Оуэна (1988, 2001) и Кьюина и Лоулесса (1994). Имбенс (2002) приводит обзор и применение этого сравнительно нового метода.
\item [$6.9$] Такие работы, как работа Грина (2003), содержат более детальное описание оценивания систем, чем  представленное здесь. Это особенно касается линейных моделей внешне не связанных уравнений и моделей линейных одновременных уравнений.
\item [$6.10$] Амэмия (1985) подробно описывает нелинейные системы уравнений.
\end{itemize}

\section{Упражнения}

\begin{enumerate}
\item [$6 - 1$] Для гамма-регрессионной модели из упражнения 5.2 $\E[y|x] = \exp(x'\beta)$ и $\Var[y|x] = (\exp(x'\beta))^2/2$.
\begin{enumerate}
\item Покажите, что из этих условий следует $\E[x\{(y-x'\beta)^2 - (\exp(x'\beta))^2/2\}] = 0$.
\item Используя моментное условие из пункта (а), выведите оценку ММ $\hat{\beta}_{MM}$.
\item Приведите асимптотическое распределение $\hat{\beta}_{MM}$, используя результат (6.13).
\item Предположим, что мы используем моментное условие  $\E[x(y-\exp(x'\beta))]$ в дополнение к тому, которое было
в пункте (а). Приведите целевую функцию для ОММ-оценки $\beta$.
\end{enumerate}
\item [$6 - 2$] Рассмотрим линейную регрессионную модель для независимых по $i$ данных с $y_i = x_i'\beta + u_i$. Пусть $\E[u_i|x_i] \not= 0$, но есть доступные инструменты $z_i$ с $\E[u_i|z_i] = 0$ и $\Var[u_i|z_i] = \sigma_i^2$, где $\dim(z) > \dim(x)$. Рассмотрим ОММ-оценку $\hat{\beta}$, которая минимизирует
\[
Q_N(\beta) = \left[ N^{-1} \sum_i z_i(y_i - x_i'\beta) \right]'W_N \left[  N^{-1} \sum_i z_i(y_i - x_i'\beta) \right].
\]
\begin{enumerate}
\item Выведите предельное распределение $\sqrt{N}(\hat{\beta} - \beta_0)$, используя общий ОММ результат из (6.11).
\item Укажите, как получить состоятельную оценку асимптотической дисперсии $\hat{\beta}$.
\item Если ошибки гомоскедастичны, то какое $W_N$ Вы бы использовали? Ваш ответ обоснуйте.
\item Если ошибки гетероскедастичны, то какое $W_N$ Вы бы использовали? Ваш ответ обоснуйте.
\end{enumerate}
\item [$6 - 3$] Рассмотрим пример Лапласа только с константой, который приведён в конце раздела 6.3.6. Таким образом, $y = \mu + u$. Тогда оценивание ОММ основывается на $\E[h(\mu)] = 0$, где $h(\mu) = [(y-\mu), (y-\mu)^3]'$.
\begin{enumerate}
\item Используя знания о центральных моментах $y$, приведённых в Разделе 6.3.6, покажите, что $G_0 = \E[\partial{h}/\partial{\mu}] = [-1,-6]'$ и что $S_0 = \E[hh']$ имеет диагональные элементы 2 и 720, а также внедиагональные элементы равные 24.
\item Покажите, что $G_0'S_0^{-1}G_0 = 252/432$.
\item Покажите, что $\hat{\mu}_{OGMM}$ имеет асимптотическую дисперсию $1.7143/N$.
\item Покажите, что ОММ-оценка $\mu$ с $W = I_2$ имеет асимптотическую дисперсию $19.14/N$.
\end{enumerate}
\item [$6 - 4$] Этот вопрос использует пробит-модель, но требует мало знаний о модели. Пусть $y$ --- бинарная переменная, которая принимает значение 0 или 1 в соответствии с тем, происходит ли событие или нет. Пусть $y$ ---вектор регрессоров, и предположим, что наблюдения независимы.
\begin{enumerate}
\item Предположим, что $\E[y|x] = \Phi(x'\beta)$, где $\Phi(\cdot)$ --- функция стандартного нормального распределения. Покажите, что $\E[(y-\Phi(x'\beta))x] = 0$. Приведите оцениваемые уравнения для ОММ-оценки $\beta$.
\item Даст ли эта оценка дают те же самые оценки, что и оценки ММП для пробит-модели? [Только для этой части Вам необходимо прочитать раздел 14.3.]
\item Приведите целевую функцию ОММ, которая соответствует оценке из пункта (а). То есть приведите целевую функцию, которая приводит к тем же самым условиям первого порядка, вплоть до преобразования  с помощью матрицы полного ранга, что и условия, полученные в пункте (а).
\item Теперь предположим, что из-за эндогенности в некоторых компонентах $\E[y|x] \not= \Phi(x'\beta)$. Предположим, существует вектор $z$ с $\dim[z] > \dim[x]$, такой что $\E[y - \Phi(x'\beta)|z] = 0$. Приведите целевую функцию для состоятельной оценки $\beta$. Оценка необязательно должна быть полностью эффективной.
\item Для Вашей оценки из пункта (г) приведите асимптотическое распределение оценки. Чётко обозначьте любые предположения о процессе, порождающем данные, чтобы получить этот результат.
\item Приведите матрицу весов и способ её получить для оценки оптимальным ОММ из пункта (г).
\item Приведите  пример из жизни для пункта (г). То есть, приведите смысловой пример пробит-модели с эндогенным(и) регрессором(ами) и подходящими инструментом(ами). Обозначьте зависимую переменную, эндогенный(ые) регрессор(ы) и инструменты(ы), которые используются для получения состоятельной оценки. [Эта часть на удивление сложная.]
\end{enumerate}
\item [$6 - 5$] Предположим, что мы накладываем ограничение, что $\E[w_i] = g(\theta)$, где $\dim[w] > \dim[\theta]$. 
\begin{enumerate}
\item Получите целевую функцию для ОММ оценки.
\item Получите целевую функцию для оценки минимального расстояния (см. раздел 6.7) с $\pi = \E[w_i]$ и $\hat{\pi} = \bar{w}$.
\item Покажите, что метод минимального расстояния и ОММ эквивалентны для этого примера.
\end{enumerate}
\item [$6 - 6$] Оценка минимального расстояния (см. раздел 6.7) использует ограничение $\pi-g(\theta)=0$. Предположим, в более общем случае, что ограничение имеет вид $h(\theta, \pi)=0$ и мы оцениваем с помощью оценки обобщённого минимального расстояния, которая минимизирует $\mathcal{Q}_{N}(\theta)=h(\theta, \hat{\pi})' W_N h(\theta, \hat{\pi})$. Измените (6.68) - (6.70) таким образом, чтобы показать, что (6.67) выполняется с $G_0= \partial h(\theta, \pi) / \partial \theta|_{\theta_0,\pi_0}$ и $\Var[\hat{\pi}]$, заменённой на $H'_0 \Var[\hat{\pi}] H_0$, где $H_0= \partial (\theta,\pi) / \partial \pi\theta|_{\theta_0,\pi_0}$.
\item [$6 - 7$] Для данных, порожденных процесса из раздела 6.6.4 с $N = 1 000$, получите оценки нелинейного двухшагового МНК и сравните их с оценками двухшагового метода.
\end{enumerate}
