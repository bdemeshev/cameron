

\chapter{Пропущенные данные и пополнение}
\section{Введение}
Проблема {\bf пропусков в данных}, по которым проводится исследование, возникла давно из-за отсутствия ответов или частичных ответов на вопросы обследований. Причины того, что респонденты не отвечают, включают нежелание раскрывать информацию, о которой их спрашивают, сложность вспомнить события, произошедшие в прошлом, а также незнание правильного ответа на вопрос. {\bf Восполнение} \emph{(imputation)} представляет собой процесс оценки или предсказания пропущенных наблюдений.

В этой Главе мы имеем дело с регрессией, построенной на основе векторов данных $(y_i, \, \mathbf{x}_i), \, i=1,\dots, N$. Для некоторых наблюдений часть элементов $\mathbf{x}_i$ или обоих векторов $(y_i, \, \mathbf{x}_i)$ пропущены. Возникает ряд вопросов. Когда мы можем проводить анализ только по полной выборке, и когда мы должны попробовать заполнить пропуски в наблюдениях? Какие методы восполнения пропусков могут быть применены? Когда получены значения, которыми могут быть заполнены пропуски, какими должны будут стать процесс оценивания и выводы?

Если набор данных содержит пропущенные наблюдения, и если эти пропуски могут быть заполнены с помощью некоторой статистической процедуры,  то есть возможность воспользоваться преимуществом большей по объему и более репрезентативной выборки, и в идеальных условиях получить более точные выводы. Природа издержек оценивания пропущенных наблюдений заключается в необходимости введения предпосылок (зачастую несоответствующих действительности) для возможности использования процедуры генерирования прокси для отсутствующих значений, а также в наличии ошибок аппроксимации, свойственных таким процедурам. Далее, статистические выводы, которые следуют за расширением базы наблюдений после восполнения пропусков, усложняются, так как должны быть учтены ошибки аппроксимации, возникающие при восполнении. 

Пропуски в данных, как результат отсутствия ответов на вопросы обследований, а также выбытия из панели, возникают часто. Восполнение пропусков может быть осуществлено как агентствами, которые создают и обеспечивают доступность баз данных обследований, так и теми, кто использует данные для моделирования. В первом случае может обладать более обширной информацией, включая конфиденциальную, которая может быть полезной при заполнении пропусков. Во втором случае строящий модели может иметь специальную конструкцию модели, которая может быть использована при восполнении. В обоих случаях основанное на некоторой модели замещение пропущенных наблюдений целесообразно.

…

Интересный пример пропущенных наблюдений возникает в контексте Исследования Потребительских Финансов (the Survey of Consumer Finances; Kennickell, 1998). Из-за деликатности вопроса потребительских финансов, обследование содержит большое количество пропусков в вопросах о доходах и благосостоянии. Аналитики Федеральной резервной системы США (U.S. Federal reserve) занимались развитием и внедрением комплекса алгоритмов заплнения пропусков в данных для случаев непрерывных и дискретных переменных, используя как открытую и доступную информацию обследований дохода и благосостояния, так и конфиденциальную информацию из переписи.

Рисунок 27.1 отражает несколько возможных примеров пропусков значений регрессоров. База данных содержит скалярную зависимую переменную $y$ и три регрессора: $x_1$, $x_2$ и $x_3$ для каждого наблюдения, которые могут быть записаны как $(\mathbf{y, \, x_1, \, x_2, \, x_3})$. В панели A есть полная информация по показателям $(\mathbf{y, \, x_2, \, x_3})$, тогда так часть данных по $\mathbf{x_1}$ отсутствует. В панели B – полная информация есть по $(\mathbf{y, \, x_3})$, но нет части данных по $(\mathbf{x_1, \, x_2})$, так что значения по ним никогда не наблюдаются одновременно. В панели C представлен пример пропусков в данных общего вида по всем трём  регрессорам, но без какого-то определённого порядка этих пропусков.  

Наиболее простая форма работы с пропусками в переменных --– удалить их и анализировать урезанную выборку, состоящую из «полных» \emph{(«complete»)} данных. Например, в случае панели A, полной будет являться выборка из переменных $(\mathbf{y, \, x_1, \, x_2, \, x_3})$, сформированная по всем доступным значениях $\mathbf{x_1}$ и соответствующим им значениям $(\mathbf{y, \, x_2, \, x_3})$. В случае панели B, тем не менее, согласно такому подходу, не останется ни одного доступного наблюдения, если не исключать $(\mathbf{x_1, \, x_2})$ из рассмотрения. В панели C полная выборка будет сформирована при удалении всех наблюдений, для которых хотя бы по одной трёх независимых переменных есть пропуски.
Описанная выше процедура носит название !!!!!!!!! \emph{listwise deletion}. К ней часто прибегают и она, как правило, является опцией «по умолчанию» в статистических пакетах. Это не всегда безобидно, последствия зависят от устройства пропусков, и сделанные на основе этих исследований выводы могут быть серьезно искажены. Конечно, в общем, выкидывание данных означает потерю информации, что снижает эффективность оценивание. Таким образом, при условии, что пропуски в данных могут быть заполнены без внесения искажений, !!!!!!!!! \emph{listwise deletion} представляется неверным путём. В этой Главе будут рассмотрены альтернативные подходы и рамки их применимости.

В целом, есть два подхода к вопросу восполнения пропусков в данных, первые {\bf основан на моделях} \emph{(model-based)}, второй – нет. Современная точка зрения склоняется к подходу, основанному на моделях. В нём используется построение моделей для заполнения пропусков, а затем в анализе участвует вся выборка с целью получения лучших оценок параметров модели.  Это процесс носит итерационный характер. Доступно как единичное, так и множественное заполнение. Основная идея современного подхода –-- рассмотрение пропущенных значений как случайные переменные, а затем замещение их многомерными значениями, полученными из распределения, лежащего в основе предположений об этих значениях; процесс этот называется множественное восстановление \emph{(multiple imputation)}. Симуляционные методы могут использоваться для аппроксимации такого распределения.

Эта тема оправдывает отдельную короткую вводную главу, так заполнение пропусков в данных является важным моментом микроэконометрических исследований. Анализируемые данные неизбежно содержат пропуски, и наиболее распространённой практикой восполнения пропущенных значений является метод !!!!!!!!! \emph{listwise deletion}. Но доступны и более совершенные методы. Но важным предостережением является тот факт, что, тем не менее, любые методы работы с пропусками в данных основываются на предпосылках, которые в ряде случаев могут быть слишком жесткими.

Большая часть этой Главы посвящена основанным на моделях подходам. Секция 27.2 представляет собой введение в терминологию и предпосылки, которые, как правило, имеют место в литературе на тему пропущенных данных и их восполнению. Секция 27.3 содержит общее рассмотрение методов работы с пропусками в данных, которые не предполагают построения моделей. Секция 27.4 начинается с первого подхода, основанного на моделях, метода максимального правдоподобия. Секция 27.5 посвящен регрессионному подходу и методам EM-типа. В Секциях 27.6 и 27.7 Представлены подходы восполнения пропущенных данных на основе концепции Байеса и MCMC. Секция 27.8 иллюстрирует описанное выше примером. В Секциях 27.6--27.8 приводится любопытное применение Байесовских методов из Главы 13.

\section{Предположения при работе с пропущенными данными !!!!\emph{(Missing data assumptions)}}

Некоторая базовая терминология и формальные определения, широко используемые в литературе о проблеме восполнения пропущенных значений переменных, принадлежат Рубину (Rubin, 1976), который ввёл два основных механизма пропусков: случайная потеря и !!!!!! {\bf полностью случайная потеря} \emph{(completely at random)}, которые служат базовыми. 

Постановка Рубина включает $\mathbf{Y}$, матрицу размеров $N \times p$, состоящую из полного набора данных, часть их которых может не наблюдаться. Обозначим $\mathbf{Y}_{obs}$ наблюдаемую часть и $\mathbf{Y}_{mis}$ --- ненаблюдаемую (пропущенную) часть наблюдений. В контексте регрессионной модели $\mathbf{Y}$ относится и к регрессорам, и к зависимым переменным. Таким образом, этот анализ подразумевает работу с пропусками общего вида. Обозначим $\mathbf{R}$ матрицу размерности $N \times p$, состоящую из переменных-индикаторов, элементы которой равны нулю или единице в зависимости от того, наблюдаемо или нет соответствующее значение в матрице $\mathbf{Y}$.

Для регрессии с одной зависимой переменной $\mathbf{Y}$ включает наблюдения по регрессанту $\mathbf{y}$ и $(p-1)$ регрессору $\mathbf{Y}$. Вероятность того, что $x_{ki}$, наблюдение $i$ переменной $x_k$, пропущено, может быть (i) независимо от её реального значения, (ii) зависимо от реального значения, (iii) зависимо от $x_{kj}, \, j \neq i$, или (iv) зависимо от $x_{lj}, \, j \neq i, \, l \neq k$.

Далее более подробно рассматриваются предпосылки относительно структуры пропусков.

\subsection{Случайные пропуски} 
Предположим, что $x_i \, (i = 1, \dots ,N)$  есть наблюдений одной переменной в исследуемой базе данных. Предпосылка !!!!! {\bf случайности пропуска} \emph{(missing at random --- MAR)} означает, что отсутствующие значения $x_i$ не зависят от самих значений, но могут зависеть от других значений $x_j \, (j \neq i)$. Формально,
\begin{align}
x_i \, \text{удовлетворяет MAR} & \Prob[x_i \text{пропущено} \, | \, x_i, \, x_j \, \forall \, j \neq i] \\
&= \Prob[x_i \text{пропущено} \, | \, x_j \, \forall \, j \neq i]. \notag
\end{align}
После контроля на остальные значения $x$ вероятность пропуска $x_i$ не связана со значением $x_i$.

Даже более формальное определение Рубина (Rubin, 1979) означает, что MAR-предположение подразумевает, что вероятностная модель для индикаторной переменной $\mathbf{R}$ не зависит от $\mathbf{Y}_{mis}$, то есть,
\begin{align*}
\Prob[\mathbf{R} \, | \, \mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}, \, \mathbf{\psi}]=\Prob[\mathbf{R} \, | \, \mathbf{Y}_{obs}, \, \mathbf{\psi}],
\end{align*}
где $\mathbf{\psi}$ --- предполагаемый вектор параметров, которые определяют механизм пропуска.

В качестве MAR как результат, основанный на методе максимального правдоподобия, который игнорирует механизм пропуска данных, вводится несмещенность при наличии пропусков, хотя оценки могут быть неэффективными. Если MAR-предпосылка не выполняется, то, тем не менее, вероятность пропусков зависит от ненаблюдаемых значений переменных. MAR-ограничение не тестируется, так как пропущенные наблюдения неизвестны. В силу того, что MAR является сильной предпосылкой, хотелось бы иметь анализ чувствительности, основанный на разных предпосылках о пропусках.

Отдельной проблемой является то, действительно ли характер пропусков в чистом виде случайным. В практике мы должны ожидать, что наблюдения, пропущенные в разных кластерах данных, согласно описанному в Главе 24, должны бать коррелированы. Тем не менее, этот вопрос не относится к смещенности от наличия пропусков, связанных со значениями переменной.

\subsection{Полностью случайные пропуски} 

!!!! {\bf Полностью случайная потеря данных} \emph{(Missing completely at random --- MCAR)} представляет собой особый случай MAR. Это означает, что $\mathbf{Y}_{obs}$ является просто случайной выборкой всех потенциально наблюдаемых значений переменных (Schafer, 1997).

Снова положим $x_i$ наблюдением переменной в исслеуемой базе данных. Тогда данные имеют характер пропуска MCAR, если вероятность пропуска в $x_i$ не зависит ни от собственного значения, ни от других значений переменных в выборке. Формально,
\begin{align}
x_i \, \text{удовлетворяет MCAR} & \Prob[x_i \text{пропущено} \, | \, x_i, \, x_j \, \forall \, j \neq i] \\
&= \Prob[x_i \text{пропущено}]. \notag
\end{align}
Например, MCAR нарушается, если (a) те, кто не сообщает доход, в среднем старше тех, кто сообщает, или (b) потеряны, как правило, меньшие значения.

Для случаев (i)--(iv), упомянутых в начале этой Секции, случай (i) удовлетворяет обоим условиям MAR и MCAR, случаи (iii) и (iv) удовлетворяют MAR, случай (ii) не удовлетворяет MAR.

MCAR подразумевает, что наблюдаемые значения являются случайной подвыборкой потенциальной полной выборки. Если предпосылки справедливы, то игнорирование неполноты выборки (наличия пропусков в данных) не приведёт к смещённым результатам.

Подводя итоги, отметим, что нарушение MCAR подразумевает смещение от формирования выборки \emph{(sample selection type of bias)}. MAR является более слабой предпосылкой, которая также помогает для восполнения пропусков в данных, так как она предполагает, что механизм потери данных зависит только от наблюдаемых значений.

\subsection{Игнорируемые и неигнорируемые пропуски} 

Механизм потери данных называют {\bf игнорируемым} \emph{(ignorable)}, если (a) база данных удовлетворяет MAR и (b) параметры процесса генерации пропущенных значений, $\mathbf{\psi}$, не связаны с параметрами $\mathbf{\theta}$, которые мы хотим оценивать.

Это условие, которое схоже со {\bf слабой экзогенность} \emph{(weak exogeneity)}, которая обсуждалась в Главе 2, подразумевает, что параметры $\mathbf{\theta}$ модели далеки от параметров $\mathbf{\psi}$ механизма пропусков. Таким образом, если пропущенные данные игнорируемы, то нет необходимости в моделировании механизма пропущенных данных, как обязательной части исследовательского процесса. MAR и «игнорируемость» часто рассматриваются как эквивалентные при выполнении предпосылки о соблюдении условия (b) для игнорируемости, что часто оправдано (Allison, 2002).

{\bf Неигнорируемый} механизм появления пропущенных значений наблюдений возникает, если MAR-предпосылка нарушается для $(y, \, x)$, но случай, когда MAR нарушен только для $x$ не подходит. В этом случае !!!!!!!dgp для пропущенных значений должен моделироваться вместе с построением основной модели для получения состоятельных оценок параметров $\mathbf{\theta}$. Чтобы избежать возможного смещения выборки, должно быть использовано оценивание, такое как двухшаговая процедура Хекмана (см. Главу 16).

Литература по восполнению пропусков в данных акцентируется на игнорируемых пропусках. Если кроме этого данные удовлетворяют MCAR, то пропуски не вызывают проблем, кроме потери эффективности оценок, которая может возникнуть в процессе восполнения. Если ситуация иная, и база данных удовлетворяет только MAR, то методы восполнения пропущенных значений переменных должны быть проверены на предмет получения состоятельных оценок и большей их эффективности.

\section{Работа с пропусками в данных без применения моделей} 

Если никакие модели не могут быть использованы, то можно просто анализировать доступные данные или прибегнуть к анализу после восполнения пропусков без применения моделей.

\subsection{Использование только доступных данных}
!!!!!!!! {\bf Исключение списком} \emph{Listwise deletion} или анализ полной выборки \emph{(complete case analysis)} означает исключение наблюдений (cases), которые имеют пропущенные значения по одной или нескольким переменным из исследуемого набора. Согласно MCAR-предпосылке, выборка, оставшаяся после такого исключения, является по-прежнему случайной выборкой из генеральной совокупности, так что полученные при этом оценки будут состоятельными. Тем не менее, стандартные ошибки будут выше, так как использовано меньше информации. Если набор регрессоров велик, то общий эффект от !!!!!!!! исключения списком listwise deletion может привести к существенным потерям в общем числе наблюдений. Это может побудить к исключению из анализа переменных с высокой долей пропущенных значений, но результаты, полученные после такой операции, вероятно, будут неверными. 

Если MCAR-предпосылка не выполняется и данные удовлетворяют только MAR, то оценки будут смещёнными. Таким образом, !!!!!!!listwise deletion не робастно к отклонениям от MCAR. Однако !!!! исключение списком listwise delition робастно к отклонениям от MAR относительно независимых переменных в регрессионном анализе, так что, когда вероятность пропуска значений любого из регрессоров не зависит от значений зависимой переменной. Обобщая сказанное, можно сказать, что !!!!!исключение списком listwise deletion допустимо, если случаи неполноты данных из-за пропущенных значений составляют небольшой процент, например, около 5\% или меньше, от общего числа (Schafer, 1996). Важно отметить, что выборка после !!!! исключения списком listwise deletion в этом случае репрезентативна. 



!!!!! {\bf Парное исключение} \emph{(pairwise deletion)} или анализ доступных данных \emph{(available-case analysis)} часто полагается более предпочтительным методом, чем!!!! исключение списком listwise deletion. Идея этого метода состоит в использовании всех доступных пар наблюдений $(x_{1i}, \, x_{2i})$ в оценивании выборочных моментов пары $(x_1, \, x_2)$ и в использовании всех наблюдений индивидуально каждой переменной  в оценивании предельных моментов. Таким образом, в линейной регрессии, под парным исключением мы будем понимать оценивание $(\mathbf{X’X})$ и $(\mathbf{X’y})$ с использованием всех возможных пар регрессоров, тогда как под !!!!исключением списком listwise deletion мы понимаем оценивание того же, но после удаления всех случаев отсутствия любых значений. Понятно, что мы теряем меньше информации при парном исключении. Здесь предлагается использовать максимам информации для оценивания индивидуальных описательных статистик, таких как средние и ковариации, а затем для использования этих статистик для вычисления оценок параметров регрессии.

Есть два важных ограничения применения парного исключения: (1) Вычисленные обычным методом стандартные ошибки и тестовые статистики будут смещенными и (2) полученная в результате матрица ковариаций регрессоров $(\mathbf{X’X})$ может оказаться не положительно определённой.

\subsection{Восстановление данных без использования моделей} 

Существует ряд !!!!!!! ad hoc и слабо определённых (weakly justified) процедур, которые часто зашиты в статистических пакетах.

!!!! {\bf Восполнение среднего} \emph{(mean imputation)} или !!!!! {\bf замена среднего} \emph{(mean substitution)} включает в себя замещение пропуска средним из доступных значений. Эта процедура не меняет среднего значения \emph{(mean-preserving)}, но оказывает влияние на распределение данных. Очевидно, что вероятностная масса в середине предельного распределения будет увеличена. Также она будет влиять на ковариации и корреляции с другими переменными.

!!!!!!!Simple hot deck восполнение данных подразумевает замену пропущенных значений случайно полученным из доступных наблюдаемых значений этой переменной, он схож с процедурой бутстрапа \emph{(bootstrap)}. Этот метод сохраняет предельное распределение переменной, но искажает ковариации и корреляции между переменными. 

В построении регрессии ни один из этих двух весьма распространённых подходов не является привлекательным, кроме как из-за своей простоты.

\section{Правдоподобие наблюдаемым данным} 

Современный подход к восполнению пропусков в данных состоит в подстановке вместо пропусков единичных или множественных значений, полученных из оценки распределения, основанной на заданной модели наблюдаемых данных и модели механизма появления пропусков в данных. Байесовские варианты этой процедуры дают эти значения из апостериорной вероятности, которая задействует и метод максимального правдоподобия, и априорное распределение параметров.

Первый важный вопрос --– роль, которую играет механизм проявления пропусков в процедуре их восстановления, и особенно, можно ли его игнорировать.

Обозначим $\mathbf{\theta}$ параметры процесса для $\mathbf{Y}=(\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis})$ и $\mathbf{\psi}$ --- как параметры механизма пропусков. Для подсчета ковариаций предположим, что $(\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis})$ --– непрерывные переменные. Тогда совместное распределение $(\mathbf{R}, \, \mathbf{Y}_{mis})$ выражается как 
\begin{align}
\Prob[\mathbf{R}, \, \mathbf{Y}_{obs} | \mathbf{\theta}, \, \mathbf{\psi}] &= \int \Prob[\mathbf{R}, \, \mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}| \, \mathbf{\theta}, \, \mathbf{\psi}]\mathbf{dY}_{mis} \\
&= \int \Prob[\mathbf{R}| \, \mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}, \, \mathbf{\psi}] \Prob[\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}| \, \mathbf{\theta}]\mathbf{dY}_{mis} \notag \\
&= \Prob[\mathbf{R}| \, \mathbf{Y}_{obs}, \, \mathbf{\psi}] \int \Prob [\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}| \, \mathbf{\theta}]\mathbf{dY}_{mis} \notag \\
&=\Prob[\mathbf{R}| \, \mathbf{Y}_{obs}, \, \mathbf{\psi}] \Prob[\mathbf{Y}_{obs}| \, \mathbf{\theta}]. \notag
\end{align}
Первое равенство означает, что совместная вероятность $(\mathbf{R}, \, \mathbf{Y}_{obs})$ для среднего $\mathbf{Y}_{mis}$ \emph{(averaging over)} получена из совместное вероятности всех данных и $\mathbf{R}$. Вторая строка определяет совместную вероятность в условных и предельных компонентах при условии $\mathbf{Y}_{obs}$ и $\mathbf{Y}_{mis}$. Третья строка иллюстрирует разделение механизма  появления пропусков и механизма генерации наблюдаемых данных, что возможно при выполнении предпосылки MAR. Последняя строка означает, то $\mathbf{\theta}$ и $\mathbf{\psi}$ являются несвязанными параметрами, и, таким образом, выводы относительно $\mathbf{\theta}$ могут быть сделаны баз учета механизма появления пропусков и только на основе $\mathbf{Y}_{obs}$. 

!!!!{\bf Правдоподобие наблюдаемым данным} \emph{(observed-data likelihood)} пропорционально последнему фактору в четвёртой строке:
\begin{equation}
\L[\mathbf{\theta}|\mathbf{Y}_{obs}] \propto \Prob[\mathbf{Y}_{obs}|\mathbf{\theta}]
\end{equation}
Это условие включает в себя только наблюдаемые значения $\mathbf{Y}_{obs}$, даже если параметры $\mathbf{\theta}$ появляется в !!!!!dgp для всех наблюдений (наблюдаемых и ненаблюдаемых). Как в Главе 13, коэффициент пропорциональности не учитывается в (27.4).

В рамках MAR-предпосылки {\bf совместная апостериорная вероятность} \emph{(joint posterior probability)} параметров $(\mathbf{\theta}, \, \mathbf{\psi})$ представляется через $\Prob[\mathbf{R}, \, \mathbf{Y}_{obs}|\mathbf{\theta}, \, \mathbf{\psi}]$ и совместную априорную вероятность $\pi (\mathbf{\theta}, \, \mathbf{\psi})$ как
\begin{align}
\Prob[\mathbf{\theta}, \, \mathbf{\psi}|\mathbf{Y}_{obs}, \, \mathbf{R}] &= k \Prob[\mathbf{R}, \, \mathbf{Y}_{obs}|\mathbf{\theta}, \, \mathbf{\psi}] \pi (\mathbf{\theta}, \, \mathbf{\psi}) \\
&\propto \Prob[\mathbf{R}|\mathbf{Y}_{obs}, \, \mathbf{\psi}] \Prob[\mathbf{Y}_{obs}|\mathbf{\theta}]\pi (\mathbf{\theta}, \, \mathbf{\psi}) \notag \\
&\propto \Prob[\mathbf{R}|\mathbf{Y}_{obs}, \, \mathbf{\psi}] \Prob[\mathbf{Y}_{obs}|\mathbf{\theta}]\pi_{\theta} (\mathbf{\theta})\pi_{\psi} (\mathbf{\psi}), \notag
\end{align}
где $k$ в первой строке есть коэффициент пропорциональности, не зависящий от $(\mathbf{\theta}, \, \mathbf{\psi})$. Во второй строке задействована факторизация из (27.3), а в третьей строке использована предпосылка о независимости априорных $\mathbf{\theta}$ и $\mathbf{\psi}$.

Так как основной интерес для нас представляют параметры $\mathbf{\theta}$, выразим предельную апостериорную вероятность для $\mathbf{\theta}$, вынося из-под интеграла $\mathbf{\psi}$ для совместной апостериорной вероятности. Это даёт {\bf апостериорную вероятность по наблюдаемым данным} \emph{(observed-data posterior)}
\begin{align}
\Prob[\mathbf{\theta}|\mathbf{Y}_{obs}, \, \mathbf{R}] &= \int \Prob[\mathbf{\theta}, \, \mathbf{\psi}|\mathbf{Y}_{obs}, \, \mathbf{R}]d\mathbf{\psi} \\
&\propto \Prob[\mathbf{Y}_{obs}|\mathbf{\theta}]\pi_{\theta} (\mathbf{\theta}) \int \Prob[\mathbf{R}|\mathbf{Y}_{obs}, \, \mathbf{\psi}]\pi_{\psi} (\mathbf{\psi}) \notag \\
&\propto \L [\mathbf{\theta}|\mathbf{Y}_{obs}]\pi_{\theta} (\mathbf{\theta}), \notag 
\end{align}
где во второй строке разделяются $\mathbf{\theta}$ и $\mathbf{\psi}$, а последняя строка не включает $\mathbf{\psi}$ и независима от механизма появления пропусков в $\mathbf{R}$.

\section{Восполнение пропусков на основе регрессий} 

В этой Секции мы рассмотрим восполнение пропусков на основе метода наименьших квадратов. Основной компонент этого подхода – использование EM-алгоритма, ранее введённого и описанного в Секции 10.3.7. 

EM-алгоритм состоит из двух шагов: формирования ожидания и максимизации. Структура EM-алгоритма тесно связана с Байесовским подходом MCMC и методами расширения базы данных. Таким образом, вместо представления полностью операционного метода для работы с пропущенными значениями переменных, мы приведём пример, иллюстрирующий мотивацию использования современных техник множественного восполнения пропусков в данных и показывающий основные свойства такого подхода.

\subsection{Пример линейной регрессии с пропущенными значениями зависимой переменной} 

На практике встречаются пропуски значений как зависимой (эндогенной) переменной, так и объясняющих переменных. Мы рассмотрим пример регрессии, в котором пропущены значения зависимой переменной,
\begin{align}
\begin{bmatrix}
\mathbf{y}_1 \\ \mathbf{y}_{mis} 
\end{bmatrix} 
= 
\begin{bmatrix}
\mathbf{X}_1 \\ \mathbf{X}_2 
\end{bmatrix} 
\mathbf{\beta}
+
\begin{bmatrix}
\mathbf{u}_1 \\ \mathbf{u}_2 
\end{bmatrix}
,
\end{align}
где $\Expect[\mathbf{u|X}]= \mathbf{0}$ и $\Expect[\mathbf{uu'|X}]= \sigma^2\mathbf{I_N}$. Сложность заключается в том, что блок наблюдений зависимой переменной $\mathbf{y}$, обозначенный как $\mathbf{y}_{mis}$, пропущен. Мы предполагаем. Что доступные полные наблюдения являются случайной выборкой из генеральной совокупности, так что пропущенные данные предполагаются удовлетворяющими MAR, но неудовлетворяющие MCAR.

При MAR-предпосылках $N_1>K$, первый блок из $N_1$ наблюдений может быть использован для получения состоятельных оценок  параметра $\mathbf{\beta}$ размерности $K$ и $\sigma^2$. Оценки метода максимального правдоподобия $(\mathbf{\beta}, \, \sigma^2$  при Гауссовском распределении ошибки буду равны $\widehat{\mathbf{\beta}}=[\mathbf{X'_1 X_1}]^{-1}\mathbf{X'_1 y_1}$ и $s^2=(\mathbf{y_1-X_1}\widehat{\mathbf{\beta}})'(\mathbf{y_1-X_1}\widehat{\mathbf{\beta}})/N_1$. Согласно стандартной теории и при предположениях о нормальности, $\widehat{\mathbf{\beta}}|\text{data} \sim \mathcal{N}[\mathbf{\beta}, \sigma^2[\mathbf{X'_1 X_1}]^{-1}]$ и $s^2 / \sigma^2 | \widehat{\mathbf{\beta}} \sim (N_1-K)\chi^2_{N_1-K}$.

В качестве первого шага, рассмотрим процедуру наивного единичного восполнения для генерации значений вместо пропущенных. Предсказанные значения $\mathbf{y}_{mis}$ при условии $\mathbf{X_2}$, обозначенные как $\widehat{\mathbf{y}}_{mis}$, получены как , где $\mathbf{X_2} \widehat{\mathbf{\beta}}$ --- описанная выше оценка, полученная на основе лишь первых $N_1$ наблюдений. Тогда
\begin{align}
\widehat{\Expect}[\mathbf{y}_{mis}|\mathbf{X_2}&=\widehat{\mathbf{y}}_{mis}=\mathbf{X_2} \widehat{\mathbf{\beta}}, \\
\widehat{\Var}[\mathbf{y}_{mis}]&\equiv \widehat{\Var}[\widehat{\mathbf{y}}|\mathbf{X_2}]=s^2(\mathbf{I}_{N_2}+\mathbf{X_2}[\mathbf{X'_1 X_1}]^{-1}]\mathbf{X'_2}). \notag
\end{align}
где $s^2\mathbf{I}_{N_2}$ является оценкой $\Var[\mathbf{u}_2]$.

В наивном методе генерируется $N_2$ предсказанных значений $\mathbf{y}_{mis}$, а затем применяются стандартные регрессионные методы для всей выборки из $N=N_1+N_2$ наблюдений.

Два шага в наивном методе соответствуют двум шагам в {\bf EM-алгоритме}. Шаг, на котором предсказываются значения, соответствует {\bf E-шагу}, второй этап применения метода наименьших квадратов к расширенной выборке --- это {\bf M-шаг}.



Тем не менее, у этого решения есть недостатки. Во-первых, рассмотрим шаг раасширения выборки. Так как сгенерированные значения $\mathbf{y}_{mis}$ \emph{в точности} следуют из МНК-подгонки, то дополнение выборки значениями $(\widehat{\mathbf{y}}_{mis}, \, \mathbf{X_2})$ для получения новых оценок $\mathbf{\widehat{\beta}_A}$ не изменяют прошлого значения $\widehat{\mathbf{\beta}}$:
\begin{align*}
\mathbf{\widehat{\beta}_A}&= [\mathbf{X'_1 X_1}+\mathbf{X'_2 X_2}]^{-1}[\mathbf{X'_1 y_1}+\mathbf{X'}_2 \widehat{\mathbf{y}}_{mis}] \\
&=[\mathbf{X'_1 X_1}+\mathbf{X'_2 X_2}]^{-1}[\mathbf{X'_1 X_1}\widehat{\mathbf{\beta}}+\mathbf{X'_2 X_2}\widehat{\mathbf{\beta}}] \\
&= \widehat{\mathbf{\beta}}.
\end{align*}

В качестве второго шага имеем, что оценка $\sigma^2$, полученная по стандартной формуле для остатков регрессии по расширенной выборке даёт значение, которое слишком мало, так как дополнительные $N_2$ остатка равны нуля по построению,
\begin{align}
s^2_A &= (\mathbf{y-X}\widehat{\mathbf{\beta}}_A)'(\mathbf{y-X}\widehat{\mathbf{\beta}}_A)/N \\
&= (\mathbf{y_1-X_1}\widehat{\mathbf{\beta}})'(\mathbf{y_1-X_1}\widehat{\mathbf{\beta}})/N<s^2 \notag
\end{align}
где $s^2$ отражает различия для $N_1$ и $N$.

Наконец, как можно заметить из выражения для выборочной дисперсии $\widehat{\mathbf{y}}_{mis}$, сгенерированные предсказания гетероскедастичны, в отличие от $\mathbf{y}_1$, и, таким образом дисперсия $\widehat{\mathbf{\beta}}_A$ не может быть оценена по формуле метода наименьших квадратов, как обычно. Наблюдения $\widehat{\mathbf{y}}_{mis}$ получены из распределения с другой дисперсией. В наивном методе не принимается во внимание неопределённость относительно оценок $\widehat{\mathbf{y}}_{mis}$.

Чтобы решить эти проблемы, необходимо модифицировать метод. Во-первых, при оценивании $\widehat{\mathbf{y}}_{mis}$ должна учитываться неопределённость относительно $\widehat{\mathbf{\beta}}$. Это может быть сделано путём корректировки $\widehat{\mathbf{y}}_{mis}$ и добавки некоторого «шума» в генерируемые предсказания, так, чтобы оценки пропущенных значений больше походили на полученные из (оценённого или условного) распределения $\mathbf{y}_1$. На !!! этапе (шаге) стандартизации может быть использован факт того, что оценка $\Var[\widehat{\mathbf{y}}_{mis}]$, $\widehat{\mathbf{\Var}}$, доступна из (27.8). Следовательно, компоненты преобразованной переменной $\widehat{\mathbf{\Var}}^{-1/2}\widehat{\mathbf{y}}_{mis}$ будут иметь единичную дисперсию. Чтобы приблизиться к распределению y1, мы можем воспользоваться результатами метода Монте-Карло для распределения $\mathcal{N}[0, \, s^2]$ и умножить их на $\widehat{\mathbf{\Var}}^{-1/2}\widehat{\mathbf{y}}_{mis}$.

Обобщим алгоритм:
\begin{enumerate}
\item	Оценить $\widehat{\mathbf{\beta}}$, используя $N_1$ полных наблюдений, как и раньше.
\item	Сгенерировать значения $\widehat{\mathbf{y}}_{mis}=\mathbf{X}_2\widehat{\mathbf{\beta}}$.
\item	Сгенерировать скорректированные значения $\widehat{\mathbf{y}}_{mis}^a=(\widehat{\mathbf{\Var}}^{-1/2}\widehat{\mathbf{y}}_{mis}) \odot \mathbf{u}_m$, где $\mathbf{u}_m$ получены методом Монте-Карло для распределения $\mathcal{N}[0, \, s^2]$ и $\odot$ отражает поэлементное перемножение.
\item	Используя расширенную выборке, получить исправленное (revised) значение оценки $\widehat{\mathbf{\beta}}$.
\item	Повторить этапы 1--4, где для этапа 1 используется исправленное значение $\widehat{\mathbf{\beta}}$.
\end{enumerate}

!!!Алгоритм исправления \emph{(revised algorithm)}, а также EM-алгоритм, повторяются, пока не сойдутся в том смысле, что изменение в коэффициентах или изменения в сумме остатков МНК-регрессии будут сколь угодно малы.

Чтобы связать эту тему с дальнейшим обсуждением, мы дадим алгоритму другую интерпретацию. Этап 3 даёт результат, взятый из условного распределения $\mathbf{y}$ при заданном $\mathbf{\beta}$, а этап 4 даёт результат из условного распределения $\mathbf{\beta}$ при заданных $s^2$ и $\mathbf{X}$. Этот подход может быть далее улучшен путём добавления ещё одного этапа, на котором получается результат из распределения $s^2$. Мы не будет проходить по всем этапам этого подхода, так как они становятся более понятными в ходе дальнейшего рассмотрения восполнения пропусков в данных.

Альтернативные модели пропущенных значений зависимой переменной были рассмотрены в Главе 16. Они свободны от MAR-предпосылки и специфицируют неигнорируемые пропуски. Так, рассмотренный выше EM-алгоритм ведёт к несостоятельным оценкам. Цензурированная Тобит-модель специфицирует пропуски для наблюдений с $\mathbf{x'\beta}+u \leqslant 0$, и состоятельными являются Тобит-оценки метода максимального правдоподобия (см. Секцию 16.3). Amemiya (1985, pp. 376-376) подробно рассматривает EM-алгоритм для Тобит-модели.

\section{!!!Расширение выборки и MCMC (Markov Chain Monte-Carlo)}
Общая структура Байесовского подхода к пропущенным данным состоит в использовании следующего типа итерационного алгоритма, который включает этапы восполнения пропусков и предсказания. 
{\bf Этап восполнения пропусков} \emph{(imputation step, I-step)} даёт разультат из условного предсказанного распределения $\mathbf{Y}_{mis}$. Оценка на r-ом шаге будет следующей:
\begin{equation}
\mathbf{Y}_{mis}^{(r+1)} \sim \Prob[\mathbf{Y}_{mis}|\mathbf{Y}_{obs}, \, \mathbf{\theta}^{(r)}].
\end{equation}
Это выражение определяет случайное значение $\mathbf{Y}_{mis}^{(r+1)}$ на основе предсказанного условного распределения $\mathbf{Y}_{mis}$ при заданном текущем значении оценки $\mathbf{\theta}^{(r)}$ и наблюдаемых значениях $\mathbf{Y}_{obs}$. Стоит отметить, что $\mathbf{Y}_{mis}$ в общем представляет собой матрицу, так что описанное выше относится  (в общем) к ряду значений.

{\bf Этап предсказания} \emph{(prediction step, P-step)} осуществляется путём получения значений из апостериорного распределения полной выборки
\begin{equation}
\mathbf{\theta}^{(r+1)} \sim \Prob[\mathbf{\theta}|\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}^{(r+1)}].
\end{equation}
Таким образом, $\mathbf{Y}_{obs}$ расширяется путём добавления значения $\mathbf{Y}_{mis}^{(r+1)}$, полученного из предсказанного распределения $\mathbf{Y}_{mis}$, а также значения, полученного из апостериорного распределения $\mathbf{\theta}$. Шаги (27.10) и (27.11) затем могут быть повторены.

Последовательное формирование выборки согласно двум описанным шагам образует Марковскую цепь. Этот процесс, который очень похож на EM-алгоритм, по сути является моделью Гиббса \emph{(Gibbs sampler)} из Секции 13.5.2, но в литературе о пропусках в данных его относят к {\bf методам расширения выборки} \emph{data augmentation}. При выполнении соответствующих условий и согласно Теореме из Секции 13.5.1, последующие результаты будут сходиться к стационарному распределению для достаточно большого значения $r$, которое отражает длину цепи. Когда цепь заканчивается, мы имеем единственную замену $\mathbf{Y}_{mis}$. После этого мы можем посчитать $\mathbf{\theta}^{(r)}$ как приблизительный результат $\Prob[\mathbf{\theta}|\mathbf{Y}_{obs}]$ и $\mathbf{Y}_{mis}^{(r+1)}$ как приблизительное значение $\Prob[\mathbf{Y}_{mis}|\mathbf{Y}_{obs}]$. В любом случае применения MCMC цепь должна иметь значительную длину для обеспечения уверенности в том, что последующее замещение свободно от статистической зависимости. Эта проблема была предметом рассмотрения Главы 13.

После того, как цепь сойдётся, мы получим конечные показатели для восполнения пропущенных значений, основанные на модели, специфицированной для данных, а также оценивание модели с использованием как наблюдаемых, так и восполненных значений переменных. По результатам сходимости (postconvergence) мы будем иметь данные для расчетов апостериорных моментов $\mathbf{\theta}$ и любых интересующих функций от $\mathbf{\theta}$, а также $\mathbf{Y}$, используя идеи из Главы 13.

Для особенной итерации рассматриваемой процедуры мы снова приведём пример регрессии для случая пропущенных значений из предыдущей Секции. Шаги MCMC-алгоритма следующие:
\begin{enumerate}
\item	На основе наблюдаемых данных вычислить $\widehat{\mathbf{\beta}}=[\mathbf{X'_1 X_1}]^{-1}\mathbf{X'_1 y_1}$ и $\widehat{\mathbf{u}}=(\mathbf{y_1-X_1}\widehat{\mathbf{\beta}})$.
\item	Сгенерировать $\sigma^2$ как $\widehat{\mathbf{u}}'\widehat{\mathbf{u}}$, делённое на значение, полученное из распределения $\chi^2_{N_1-K}$.
\item	Получить $\mathbf{\beta}|\sigma^2 \sim \mathcal{N}[\widehat{\mathbf{\beta}}, \sigma^2[\mathbf{X'_1 X_1}]^{-1}]$.
\item	Получить $\mathbf{y}_{mis} \sim \mathcal{N}[\mathbf{X}_2\widehat{\mathbf{\beta}}, \sigma^2]$.
\item	Используя $\mathbf{y}$ вместо $\mathbf{y}_1$ и $\mathbf{X}$ вместо $\mathbf{X}_1$, повторить шаги с 1 по 4 после соответствующих корректировок.
\end{enumerate}

Основания для шага 2 таковы, что при неинформативных \emph{(uninformative)} априорных сведениях о $(\mathbf{\beta}, \sigma^2)$, условное апостериорное распределение $\widehat{\mathbf{u}}'\widehat{\mathbf{u}}/ \sigma^2$ будет $\chi^2_{N_1-K}$, только если использованы наблюдаемые значения. После расширения выборки оно изменяется до $\chi^2_{N-K}$. Основания для шага 3 таковы, что при неинформативных априорных сведениях, условное апостериорное распределение будет $\mathcal{N}[\widehat{\mathbf{\beta}}, \sigma^2[\mathbf{X'_1 X_1}]^{-1}]$. После расширения выборки оно меняется до $\mathcal{N}[\widehat{\mathbf{\beta}}, \sigma^2[\mathbf{X' X}]^{-1}]$. Шаг 4 является шагом, на котором добавляются данные с использованием условной предсказанной плотности $\mathcal{N}[\mathbf{X}_2\widehat{\mathbf{\beta}}, \sigma^2]$. Эти шаги могут быть при необходимости модифицированы, если мы используем, например информативное \emph{(informative)} нормальное-гамма априорное распределение $(\mathbf{\beta}, \sigma^2)$. Условные апостериорные законы распределения для этого случая приведены в Секции 13.3.



\section{!!!!Множественное восполнение пропусков}

Анализ, проведённый в предыдущей Секции, объясняет, как полный цикл MCMC помогает сгенерировать единичную замену пропуску. Тем не менее, единичная замена не позволяет адекватно отразить неопределённость, возникающую в случае потери данных. Это ключевой аргумент в пользу применения процедуры множественного восполнения данных. Условное предсказанное распределение $\mathbf{Y}_{mis}|\mathbf{Y}_{obs}, \, \mathbf{\theta}$ получено путём усреднения по апостериорной наблюдаемой информацией о $\mathbf{\theta}$:
\[
\Prob[\mathbf{Y}_{mis}|\mathbf{Y}_{obs}] = \int \Prob[\mathbf{Y}_{mis}|\mathbf{Y}_{obs}, \, \mathbf{\theta}] \Prob[\mathbf{\theta}|\mathbf{Y}_{obs}]\mathbf{d\theta}.
\]
Правильные множественные замещения с точки зрения Байесовского подхода отражают неопределённость относительно $\mathbf{Y}_{mis}$, что порождает неопределённость в коэффициентах модели.

После {\bf множественного восстановления} пропущенные значения $\mathbf{Y}_{mis}$ заменяются симулированными/восстановленными значениями $\mathbf{Y}_{mis}^{(1)}, \, \mathbf{Y}_{mis}^{(2)}, \, \mathbf{Y}_{mis}^{(3)}\, \dots, \, \mathbf{Y}_{mis}^{(m)}$. Каждый из полных наборов данных затем анализируется как изначально бывший полным. Результаты $m$ раз проведённого анализа будут показывать вариацию, которая возникает вследствие неопределённости, возникшей из-за пропусков в данных. С $m$ различными наборами данных возникает вопрос о том, как определять нужное значение $m$ и как объединить $m$ наборов оценок параметров и ковариационных матриц. Мы освещаем оба вопроса, используя результаты обзора литературы, но без подробных обоснований.

Рассматривая, как объединить результаты, полученные по множественному восполнению пропусков, основным для выражения произвольной статистики $Q$ является
\begin{equation}
\Prob[Q|\mathbf{Y}_{obs}] = 
\int \Prob[Q|\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}] 
\Prob[\mathbf{Y}_{mis}|\mathbf{Y}_{obs}]\mathbf{d}\mathbf{Y}_{mis}
\end{equation}
что отражает действительное апостериорное распределение $Q$ получено путём усреднения по апостериорному распределению $Q$ на основе полных данных. Это означает усреднение  по результатам множественного восстановления пропущенных значений (Rubin, 1996).

Уравнение (27.12) подразумевает, что конечная оценка Q определяется, согласно закону итерационных ожиданий, как
\begin{equation}
\Expect[Q|\mathbf{Y}_{obs}]=\Expect[\Expect[Q|\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}]|\mathbf{Y}_{obs}].
\end{equation}
Усреднённое $Q_r$ есть апостериорное среднее значение $Q$ на основе полного набора данных после повторенного восстановления.

Конечная дисперсия $Q$ представляется формулой
\begin{equation}
\Var[Q|\mathbf{Y}_{obs}]=\Expect[\Var[Q|\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}]|\mathbf{Y}_{obs}]+ \Var[\Expect[Q|\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}]|\mathbf{Y}_{obs}],
\end{equation}
с использованием декомпозиции вариации, приведённой в Секции А.8.
Рубин (Rubin, 1996) также дал следующие правила для комбинирования информации о моментах, сформулированные в терминах скалярных параметров. Для любого скалярного параметра предположим, что $\widehat{Q}_r$ --- точечная оценка rого восстановления и $\widehat{U}_r$ --- оценка дисперсии. Тогда определим средние значения точечных оценок и оценок дисперсии соответственно как 
\begin{align}
\bar{Q}&=m^{-1} \sum \limits^{m}_{r=1}\widehat{Q}_r, \\
\bar{Q}&=m^{-1} \sum \limits^{m}_{r=1}\widehat{Q}_r, \\
\end{align}
и ковариацию между оценками разных восстановлений как
\begin{equation}
B=(m-1)^{-1} \sum \limits^{m}_{r=1}(\widehat{Q}_r - \bar{Q})^2
\end{equation}
а также общую дисперсию как
\begin{equation}
T=\bar{U}+(1+m^{-1})B.
\end{equation}

Результаты (27.15 и 27.16) следуют из (27.13), уравнение (27.18) следует из (27.14). Шафер (Schafer, 1997) привёл результаты для комбинирования Р-значений и статистик отношения правдоподобия и дал дополнительные ссылки.

Выводы относительно отдельных коэффициентов или групп коэффициентов, сделанные после восстановления пропусков, могут быть сделаны на основе конечных оценок, так как стандартная центральная предельная теорема и связанные с ней результаты для больших выборок \emph{(large-sample results)} могут быть распространены и на этот случай.

Следующая формула отражает меру относительной эффективности $m$ множественных восстановлений:
\begin{equation}
reff=(1+(\lambda/m))^{-1},
\end{equation}
где $\lambda$ --- часть пропущенных наблюдений. Эффективность измеряется относительно ситуации отсутствия пропусков. Арифметические вычисления в Таблице 27.1 отражают, что при трёх восстановлениях эффективность может быть равна 97\% при 10\% пропущенных значений и 86\% при 50\% пропущенных значений. 10 и более раз восстановленные данные дают относительную эффективность, превосходящую 95\% при 50\% пропусков. Таким образом, как было отмечено Шафером (Schafer, 1997), количество восстановлений не должно быть очень высоким.

\section{Пример восполнения пропусков в данных методом MCMC} 

В этой Секции в качестве иллюстраций приведены два варианта приложения методов восполнения пропусков в данных: методы !!!!! удаления списком listwise deletion и восстановления средним, которые не требуют построения моделей, (см. Секцию 27.2), а также основанный на модели метод расширения выборки с использованием MCMC алгоритма (см. Секцию 27.6). Пропущенными являются значения наблюдений только регрессоров, при этом механизм появления пропусков удовлетворяет MAR.

Первый случай применения включает в себя простую множественную регрессию, второй случай включает логит-регрессию. Для ясности и простоты мы используем сгенерированные данные с известным !!!!dgp.

\subsection{Линейная регрессия с пропущенными значениями регрессоров} 



В примете линейной регрессии данные будут устроены как
\begin{equation}
y_i=\beta_0+\beta_1 x_{1i}+\beta_2 x_{2i}+u_i, \, i=1, \, 2, \dots , \, N,
\end{equation}
с $u_i|x_{1i}, \, x_{2i} \sim \mathcal{N}[0, \, \sigma^2]$ и $(x_{1i}, \, x_{2i})$ имеющими двумерное нормальное распределение
\begin{equation}
\begin{bmatrix}
x_{1i} \\ x_{2i}
\end{bmatrix}
\sim \mathcal{N}
\begin{bmatrix}
\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix}
\end{bmatrix},
\end{equation}
так что $x_{2i}|x_{1i} \sim \mathcal{N}[\rho x_{1i}, \, 1-\rho^2]$. Также мы определим $\mathbf{\beta'}=\begin{bmatrix} 1 & 1 & 1 \end{bmatrix}, \, n=1000$, а также пропорция случайно пропущенных значений x1 и x2 будут 10\% или 25\%. Для любого i, для любого из $x_1$ и $x_2$ или для обоих могут быть пропуски. Мы также используем два разных значения $\rho$, 0.36 и 0.64.

Для Марковской цепи выбрано 500 итераций для !!!!! \emph{burn-in phase}. Вычисления для Марковской цепи были проведены с помощью алгоритма SAS MI Proc, в котором используется неинформативная априорная информация. Только для демонстрации процесса число восстановлений было взято равным 10, но длина цепи после !!!! \emph{burn-in phase} варьировалась от 10 до 10000. Proc MI комбинирует результаты множественных восстановлений, используя Уравнения (27.15)—(27.18).

Таблицы 27.2 и 27.3 содержат результаты для высоких $\rho$, а также низких и высоких долей пропусков. Между методами не наблюдается большой разницы. Так как выполнена MAR-предпосылка, то точечные оценки после !!!!!! удаления списком listwise deletion и для всей выборки остались близкими, но, как и ожидалось, стандартные ошибки после !!!!!! удаления списком listwise deletion выше. При замещении средним точечные оценки $\beta_2$ расходятся больше, но наблюдаемая вариация находится в пределах ошибки выборки \emph{(sampling error)}. Оказалось, что в обоих случаях Марковская цепь достигает стационарности довольно быстро, между результатами для 10 и 10000 итераций разница невелика. Вероятно, это происходит из-за того, что количество !!!! \emph{burn-in} итераций 500, что может быть больше, чем нужно для этого относительно простого случая.

В Таблице 27.4 симуляционная процедура повторена для «наихудшего» \emph{(«worst-case»)} сценария: низкого значения $\rho$ и 25\% пропусков. Расхождение между точечными оценками по всей выборке, после !!!!!! удаления списком listwise deletion и после замещения средним оказалось в общем относительно больше, чем для случаев MCMС. Тем не менее, даже в этом случае отличия от оценок по всей выборке на самом деле не очень велико. Ещё раз мы увидели, что особые преимущества от использования длинной Марковской цепи в этом примере выявлены не были.



\subsection{Логит-регрессия с пропущенными значениями регрессоров} 
Далее мы рассмотрим пример нелинейной модели с пропущенными значениями регрессоров, используя симулированные данные. В этом симуляционном примере мы продолжим работать с данными, устройство которых было описано ранее, но заменим зависимую переменную на дискретную дихотомическую (бинарную). Сначала изменим интерпретацию механизма симуляции, предложенного для примера линейной регрессии, так что $y=y^*$ --- латентная переменная. Данные устроены как
\begin{equation}
y_i=\beta_0+\beta_1 x_{1i}+\beta_2 x_{2i}+u_i, \, i=1, \, 2, \dots , \, N.
\end{equation}
Тогда дихотомическая переменная $y_i$ формируется согласно следующему правилу:



\begin{equation}
y_i=
\begin{cases}
1, \text{если} y_i^*>0, \\
0, \text{если} y_i^*\leqslant 0
\end{cases}
\end{equation}



Мы будем моделировать вероятность того, что $y_i=0$, используя логит-модель, хоть такой !!!!dgp и для пробит-модели. Как было рассмотрено в Секции 14.4.1, в логит-модели идентифицируется вектор параметров $\mathbf{\beta}/\sigma$, где дисперсия равна $\sigma^2=\pi^2/3$. Если все компоненты $\mathbf{\beta}$ равны единице, логит-модель даёт оценки истинных значений параметров, равные приблизительно -0.551. Оценивание с помощью MCMC осуществляется, как и раньше, с использованием неинформативной априорной информации.

В Таблице 27.5 представлен «благоприятный» случай с 10\% пропусков в данных и высокой корреляции между $x_1$ и $x_2$, а в Таблице 27.6 представлен «менее удачный» случай с 25\% пропусков и низкой корреляцией между $x_1$ и $x_2$.

В первом случае даже при отсутствии пропусков оценка $\widehat{\beta}_2$ значительно отличается от своего ожидаемого значения. Оценки методом MCMC несколько изменяются, когда длина Марковской цепи возрастает с 10 до 1000. Тем не менее, при симуляционном восполнении пропусков имеет место лишь небольшое изменение в точечных оценках, что можно проинтерпретировать как индикатор сходимости цепик стационарному распределению.

Для второго примера, включающего рассмотрение менее удачного механизма симуляции, результаты представлены в Таблице 27.6. Основная разница состоит в том, что расхождение ожидаемых точечных оценок и оценённых значений несколько больше в предыдущем случае. Тем не менее, говоря в общем, результаты множественного восстановления пропусков в случае логистической регрессии схожи с результатами для линейной регрессии.



\section{Практические рекомендации}

Основное практическое приложение методов анализа, о которых шла речь в этой Главе, --- анализ скорее множественного, чем единичного, восстановления пропусков в данных, имеет теоретические преимущества. Более того, подходы, основанные на моделях, менее ad hoc, чем механические подходы, такие как замена средним или !!! \emph{hot deck}. Тем не менее, во многих случаях практического применения процедур восстановления данных MCMC типа могут возникнуть сложности на фоне относительной простоты примеров, рассмотренных в последней Секции.

Различия могут быть отмечены между множественным восстановлением пропусков, где конечным результатом являются данные, и подходом, в котором конечный результат состоит из оценок коэффициентов для получения выводов. Хотя обе процедуры могут быть основаны на моделях, вторая может включать в себя более сложные эконометрические модели. Примеры этого можно найти в работах Brownstone и Valetta (1996), Stinebrinkner (1999), Kennicknell (1998), а также Davey, Shanaham и Schafer (2001).

Даже когда первостепенно восстановление пропущенных значений, без подробного моделирования проблема может быть далеко не простой. Например, в своём исследовании 1995 года «Исследование потребительских финансов» (Survey of Consumer Finances) Kennickell (1998, p. 5) отмечает, что:
!!!!!!!!
[Когда] исследование содержит большое количество переменных, имеет место значительная или частичная потеря (области) данных, характер пропущенной информации гетерогенен, законы распределения многих переменных искажены, данные имеют сложную структуру, [тогда] анализ в условиях отсутствия восполнения пропусков будет очень трудной задачей. Более того, любой, кто использует опубликованные базы данных, будет испытывать недостаток в ключевой информации, которая может оказаться важной для понимания закона распределения пропущенных значений. Так, даже при полной эффективности есть случаи, в которых стоит восстанавливать пропущенные данные.

Несмотря на сложность проблемы, Kennickell мог пользоваться процедурами восполнения пропусков, сходными с рассмотренными в этой Главе.

Stinebrinkner (1999), также столкнувшийся с проблемой пропусков в данных, в которой исключение списком !!!!! listwise deletion «оставляет эконометриста со слишком маленьким набором данных, чтобы оценивать интересующую модель», развил двухшаговую симуляционную процедуру, основанную на максимальном правдоподобии, для оценивания совместного распределения пропущенных значений и для оценивания моделей длительности для продолжительности первого обучения \emph{(first teaching spell)}.

Для относительно простых случаев может быть использовано такое программное обеспечение, как пакет SAS Proc MI. S-Plus и SOLAS также помогают. Полезное пособие и исследование компьютерных программных пакетов предложили Horton и Lipsitz (2001). За дополнительной информацией стоит смотреть Интернет сайты с соответствующей тематикой.

Большинство методов анализа, на которых строится эта Глава, основаны на предпосылке об игнорируемости механизма появления пропусков в данных. С эконометрической точки зрения, это может служить значительным упрощением. Например, смотрите работу Lillard, Smith и Welch (1986), которые критиковали метод !!!!!!\emph{Census hot deck} для восстановления данных о зарплатах. Как же себя вести, если механизм не игнорируем? Как отмечено в Секции 27.4, неигнорируемый механизм появления пропусков в данных подразумевает, что параметры $\mathbf{\beta}$ и $\mathbf{\psi}$ взаимосвязаны. Тогда нужно специфицировать в явном виде этот механизм, как в случае моделей отбора \emph{(selection models)} и моделей систематической ошибки, связанной с выбыванием наблюдений из выборки \emph{(models of attrition bias)} (см. Главу 16 и Секцию 23.5.2). Schafer (1997, p. 28) привёл ссылки на литературу по этой теме.

\section{Библиографические заметки}
Важные ранние ссылки включают работы Little и Rubin (1987) и Rubin (1987). Allison (2002) предложил относительно нетехническое, но понятное введение в проблему пропущенных значений переменных и соответствующую литературу. Rubin (1996) предложил исследование с историческим аспектом. Schafer (1997) предложил более полный анализ, который включает в себя случаи категориальных переменных, смешенных дискретно-непрерывных данных и данных комплексных обследований.
\begin{enumerate}
\item Meng (2000) предложил историческую справку по механизмам появления пропусков.
\item Little (1988, 1992) привёл хороший обзор литературы о линейных регрессиях с пропусками в регрессорах, затрагивающей как случаи подходов, основанных на моделях, так и не требующие их.
\end{enumerate}

\section*{Упражнения} 

\begin{enumerate}
\item Пусть есть некоторая регрессионная модель, линейная или нелинейная, с зависимой переменной $y$ и экзогенными переменными $\mathbf{x}$, а также независимыми одинаково распределёнными ошибками $\mathbf{\epsilon}$. Покажите, что, если вероятность пропущенных значений $\mathbf{x}$ независима от $y$, то регрессия, основанная на listwise deletion обеспечивает состоятельную оценку условной функции среднего. [Подсказка: покажите, что условное распределение $y$ при заданном $\mathbf{x}$ не искажается при наличии пропусков.]

\item (Gourieroux and Marfont, 1981. Пусть есть регрессионная модель $\mathbf{y}=\beta_1\mathbf{x}+\mathbf{Z\beta_2} + \mathbf{u}$, где $\mathbf{y}$ --– вектор размерности $N \times 1$, $\mathbf{Z}$ --– матрица размерности $N \times K$, а $\mathbf{x}$ --– скалярный регрессор, вектор размерности $N \times 1$, некоторые компоненты которого пропущены. Предположим, что наблюдения пропущены случайно и $\Expect[\mathbf{u}|\mathbf{x, \, Z}]=\mathbf{0}$ и $\Expect[\mathbf{uu'}|\mathbf{x, \, Z}]=\sigma^2\mathbf{I}_N$. И $\mathbf{Z}$, и $\mathbf{y}$ полностью наблюдаемы. Следующий механизм предложен для работы с пропущенными значениями. Предположим линейную регрессионную модель, связывающую $\mathbf{x}$ и $\mathbf{Z}$, $\mathbf{x}=\mathbf{Z\gamma}+ \mathbf{\epsilon}$, где $\Expect[\mathbf{\epsilon}| \, \mathbf{Z}]=\mathbf{0}$ и $\Expect[\mathbf{\epsilon \epsilon'}| \, \mathbf{Z}]=\sigma^2_{\epsilon}\mathbf{I}_N$. Тогда пусть $\widehat{\mathbf{\gamma}}=[\mathbf{Z'_c Z_c}]^{-1}\mathbf{Z'_c x_c}$, где индекс c относится к «полным данным» \emph{(complete data)}. Восстановим значения согласно $\widehat{\mathbf{x}}_m=\mathbf{Z_m}[\mathbf{Z'_c Z_c}]^{-1}\mathbf{Z'_c x_c}$, где $\mathbf{x}_m$ относится к пропущенным наблюдениям и $\mathbf{Z_m}$ –-- к соответствующим значениям $\mathbf{Z}$. Исходная регрессия тогда переоценивается с использованием полного набора из $N$ наблюдений после восполнения пропусков в $\mathbf{x}$ с помощью восстановленных значений.
\begin{enumerate}
\item	Объясните, почему МНК-оценки, основанные на полных и восстановленных данных могут быть смещенными в случае конечной выборки.
\item	Какие дополнительные предпосылки нужны для того, чтобы доказать состоятельность МНЕ-оценок, основанных на полных и восстановленных данных?
\item	Будет ли МНК-оценка эффективной?
\end{enumerate}

\item Пусть, когда оценивается модель после восполнения пропусков в данных, точное значение оценок скорее всего будут завышены, если не будет сделано никакой корректировки на шаге восстановления. Другими словами, восстановленные данные могут быть рассмотрены как сгенерированные переменные и, таким образом, будут источником проблем для последующего двухшагового оценивания, о котором шла речь в Секции 6.6. Объясните, будет ли корректировка, применённая к восстановлению пропущенных данных, необходима в асимптотике.

\end{enumerate}



