

\chapter{Пропущенные данные и восстановление данных}
\section{Введение}
Проблема {\bf пропусков в данных}, по которым проводится исследование известна давно и возникает из-за отсутствия ответов или частичных ответов на вопросы обследований. Причины того, что респонденты не отвечают, включают нежелание раскрывать информацию, о которой их спрашивают, сложность вспомнить события, произошедшие в прошлом, а также незнание правильного ответа на вопрос. {\bf Восстановления} \emph{(imputation)} представляет собой процесс оценки или предсказания пропущенных наблюдений.

В этой Главе мы имеем дело с регрессией, построенной на основе  данных $(y_i, \, \mathbf{x}_i), \, i=1,\dots, N$. Для некоторых наблюдений часть элементов $\mathbf{x}_i$ или  $(y_i, \, \mathbf{x}_i)$ пропущены. Возникает ряд вопросов. Когда мы можем проводить анализ только по полной выборке, и когда мы должны попробовать заполнить пропуски в наблюдениях? Какие методы восстановления пропусков могут быть применены? Когда получены значения, которыми могут быть заполнены пропуски, какими должны будут стать процесс оценивания и выводы?

Если набор данных содержит пропущенные наблюдения, и если эти пропуски могут быть заполнены с помощью некоторой статистической процедуры,  то есть возможность воспользоваться преимуществом большей по объему и более репрезентативной выборки, и в идеальных условиях получить более точные выводы. Природа издержек оценивания пропущенных наблюдений заключается в необходимости введения предпосылок (зачастую несоответствующих действительности) для возможности использования прокси для отсутствующих значений, а также в наличии ошибок аппроксимации, свойственных таким процедурам. Далее, статистические выводы, которые делаются после восстановления пропусков, усложняются, так как должны быть учтены ошибки аппроксимации, возникающие при восстановлении. 

Пропуски в данных, как результат отсутствия ответов на вопросы обследований, а также истощения панели, возникают часто. Пополнение пропусков может быть осуществлено как агентствами, которые создают и обеспечивают доступность баз данных обследований, так и теми, кто использует данные для моделирования. В первом случае агентство может обладать более обширной информацией, включая конфиденциальную, которая может быть полезной при заполнении пропусков. Во втором случае исследователь, строящий модели, может иметь специальную  модель, которая может быть использована при восстановлении. В обоих случаях возможно восстановление пропущенных наблюдений, основанное на некоторой модели.

\vspace{3cm}
Рисунок 27.1. Пропуски в данных. Примеры пропусков в регрессорах.

A: Univariate missing data pattern --- Пропуски в одном регрессоре

B: Special Pattern of missing data on x1 and x2 --- Частный случай пропусков в $x_1$ и $x_2$

C: General pattern of missing data --- Общий случай пропусков в регрессорах



Интересный пример пропущенных наблюдений возникает в контексте Обследование Финансового Положения Потребителей (Survey of Consumer Finances) см. работу Кенникелла (1998). Из-за деликатности самого сюжета финансового положения потребителя, обследование содержит большое количество пропусков в вопросах о доходах и благосостоянии. Аналитики Федеральной резервной системы США (U.S. Federal reserve) занимались развитием и внедрением комплекса алгоритмов заполнения пропусков в данных для случаев непрерывных и дискретных переменных, используя как открытую и доступную информацию обследований дохода и благосостояния, так и конфиденциальную информацию из переписи.

Рисунок 27.1 отражает несколько возможных примеров пропусков значений регрессоров. База данных содержит скалярную зависимую переменную $y$ и три регрессора: $x_1$, $x_2$ и $x_3$ для каждого наблюдения, которые могут быть записаны как $(\mathbf{y, \, x_1, \, x_2, \, x_3})$. В панели A есть полная информация по показателям $(\mathbf{y, \, x_2, \, x_3})$, тогда так часть данных по $\mathbf{x_1}$ отсутствует. В панели B полная информация есть по $(\mathbf{y, \, x_3})$, но нет части данных по $(\mathbf{x_1, \, x_2})$, так что значения по ним никогда не наблюдаются одновременно. В панели C представлен пример пропусков в данных общего вида по всем трём  регрессорам, но без какого-то определённого порядка этих пропусков.  

Наиболее простая форма работы с пропусками в переменных --– удалить их и анализировать урезанную выборку, состоящую из <<полных>> \emph{(<<complete>>)} данных. Например, в случае панели A, полной будет являться выборка из переменных $(\mathbf{y, \, x_1, \, x_2, \, x_3})$, сформированная по всем доступным значениях $\mathbf{x_1}$ и соответствующим им значениям $(\mathbf{y, \, x_2, \, x_3})$. В случае панели B, тем не менее, согласно такому подходу, не останется ни одного доступного наблюдения, если не исключать $(\mathbf{x_1, \, x_2})$ из рассмотрения. В панели C полная выборка будет сформирована при удалении всех наблюдений, для которых хотя бы по одной трёх независимых переменных есть пропуски.
Описанная выше процедура носит название полного удаления наблюдений с пропусками (\emph{listwise deletion}). К ней часто прибегают и она, как правило, является опцией <<по умолчанию>> в статистических пакетах. Это не всегда безобидно, последствия зависят от устройства пропусков, и сделанные на основе этих исследований выводы могут быть серьезно искажены. Конечно, в общем, выкидывание данных означает потерю информации, что снижает эффективность оценивание. Таким образом, при условии, что пропуски в данных могут быть заполнены без внесения искажений, полное удаление наблюдений с пропусками представляется неверным путём. В этой Главе будут рассмотрены альтернативные подходы и рамки их применимости.

В целом, есть два подхода к вопросу восстановления пропусков в данных, первый {\bf основан на моделях} \emph{(model-based)}, второй --- нет. Современная точка зрения склоняется к подходу, основанному на моделях. В нём используется построение моделей для заполнения пропусков, а затем в анализе участвует вся выборка с целью получения лучших оценок параметров модели.  Это процесс носит итерационный характер. Возможно восстановление как пропусков по одной переменной, так и по многим. Основная идея современного подхода --- рассмотрение пропущенных значений как случайных переменных, а затем замещение их многократными случайными выборками, полученными из рассматриваемого распределения. Процесс этот называется множественным восстановлением данных \emph{(multiple imputation)}. Симуляционные методы могут использоваться для аппроксимации такого распределения.

Эта тема оправдывает отдельную короткую вводную главу, так как заполнение пропусков в данных является важным моментом микроэконометрических исследований. Анализируемые данные неизбежно содержат пропуски, и наиболее распространённой практикой восстановления пропущенных значений является метод полное удаление наблюдений с пропусками. Но доступны и более совершенные методы. Важным предостережением является тот факт, что, тем не менее, любые методы работы с пропусками в данных основываются на предпосылках, которые в ряде случаев могут быть слишком жесткими.

Большая часть этой Главы посвящена основанным на моделях подходам. Раздел 27.2 представляет собой введение в терминологию и предпосылки, которые, как правило, имеют место в литературе на тему пропущенных данных и их восстановления. Раздел 27.3 содержит общее рассмотрение методов работы с пропусками в данных, которые не предполагают построения моделей. Раздел 27.4 начинается с первого подхода, основанного на моделях, метода максимального правдоподобия. Раздел 27.5 посвящен регрессионному подходу и методам EM-типа. В Разделах 27.6 и 27.7 представлены подходы восстановления пропущенных данных на основе байесовского подхода и MCMC. Раздел 27.8 иллюстрирует описанное выше примером. В Разделах 27.6--27.8 приводится удачное  применение байесовских методов из Главы 13.

\section{Предположения при работе с пропущенными данными}

Некоторая базовая терминология и формальные определения, широко используемые в литературе о проблеме восстановления пропущенных значений переменных, принадлежат Рубину (1976), который ввёл два основных механизма пропусков: случайный пропуск (missing at random) и {\bf полностью случайный пропуск} \emph{(missing completely at random)}, которые служат базовыми. 

Постановка Рубина включает $\mathbf{Y}$, матрицу размеров $N \times p$, состоящую из полного набора данных, часть их которых может не наблюдаться. Обозначим $\mathbf{Y}_{obs}$ наблюдаемую часть и $\mathbf{Y}_{mis}$ --- ненаблюдаемую (пропущенную) часть наблюдений. В контексте регрессионной модели матрица $\mathbf{Y}$ содержит и регрессоры, и зависимые переменные. Таким образом, этот анализ подразумевает работу с пропусками общего вида. Обозначим $\mathbf{R}$ матрицу размерности $N \times p$, состоящую из переменных-индикаторов, элементы которой равны нулю или единице в зависимости от того, наблюдаемо или нет соответствующее значение в матрице $\mathbf{Y}$.

Для регрессии с одной зависимой переменной $\mathbf{Y}$ включает наблюдения по зависимой переменной  $\mathbf{y}$ и $(p-1)$-му регрессору $\mathbf{X}$. Вероятность того, что $x_{ki}$, наблюдение $i$ переменной $x_k$, пропущено, может быть (i) независима от её фактического  значения, (ii) зависима от фактического значения, (iii) зависима от $x_{kj}, \, j \neq i$, или (iv) зависима от $x_{lj}, \, j \neq i, \, l \neq k$.

Далее более подробно рассматриваются предпосылки относительно структуры пропусков.

\subsection{Случайные пропуски} 
Предположим, что $x_i \, (i = 1, \dots ,N)$  есть наблюдений одной переменной в исследуемой базе данных. Предпосылка {\bf случайности пропуска} \emph{(missing at random --- MAR)} означает, что наличие пропуска значения $x_i$ не зависит от самого значения, но может зависеть от других значений $x_j \, (j \neq i)$. Формально,
\begin{align}
x_i \, \text{ удовлетворяет MAR} & \Rightarrow \Prob[x_i \text{ пропущено } \, | \, x_i, \, x_j \, \forall \, j \neq i] \\
&= \Prob[x_i \text{ пропущено } \, | \, x_j \, \forall \, j \neq i]. \notag
\end{align}
После учета остальных значений $x$ вероятность пропуска $x_i$ не связана со значением $x_i$.

Согласно более формальному определению Рубина (1979), предпосылка MAR  подразумевает, что вероятностная модель для индикаторной переменной $\mathbf{R}$ не зависит от $\mathbf{Y}_{mis}$, то есть,
\begin{align*}
\Prob[\mathbf{R} \, | \, \mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}, \, \mathbf{\psi}]=\Prob[\mathbf{R} \, | \, \mathbf{Y}_{obs}, \, \mathbf{\psi}],
\end{align*}
где $\mathbf{\psi}$ --- вектор параметров, которые определяют механизм пропуска.

Если предпосылка MAR выполнена, то при использовании метода максимального правдоподобия по полным наблюдениям не возникает смещения из-за пропущенных значений, хотя оценки могут быть неэффективными. Если MAR-предпосылка не выполняется, то, вероятность пропусков зависит от ненаблюдаемых значений переменных. MAR-ограничение не тестируемо, так как пропущенные наблюдения неизвестны. В силу того, что MAR является сильной предпосылкой, желательным является  анализ чувствительности, основанный на разных предпосылках о пропусках.

Отдельным вопросом является вопрос о том, действительно ли характер пропусков в чистом виде случаен. На практике мы должны ожидать, что наблюдения, пропущенные в кластерах данных, в смысле Главы 24, могут быть коррелированы. Тем не менее, этот вопрос не относится к смещенности от наличия пропусков, связанных со значениями переменной.

\subsection{Полностью случайные пропуски} 

{\bf Полностью случайные пропуски} \emph{(Missing completely at random --- MCAR)} представляет собой частный случай MAR. Это означает, что $\mathbf{Y}_{obs}$ является просто случайной выборкой всех потенциально наблюдаемых значений переменных Шефер (1997).

Снова обозначим с помощью $x_i$ наблюдение за переменной в исследуемом наборе данных. Тогда данные имеют MCAR-пропуски, если вероятность пропуска в $x_i$ не зависит ни от собственного значения, ни от других значений переменных в выборке. Формально,
\begin{align}
x_i \, \text{ удовлетворяет MCAR } & \Rightarrow \Prob[x_i \text{ пропущено } \, | \, x_i, \, x_j \, \forall \, j \neq i] \\
&= \Prob[x_i \text{ пропущено }]. \notag
\end{align}
Например, MCAR нарушается, если (a) те, кто не сообщает доход, в среднем старше тех, кто сообщает, или (b) пропущены, как правило, меньшие значения.

Для случаев (i)--(iv), упомянутых в начале этого Раздела, случай (i) удовлетворяет обоим условиям MAR и MCAR, случаи (iii) и (iv) удовлетворяют MAR, случай (ii) не удовлетворяет MAR.

MCAR подразумевает, что наблюдаемые значения являются случайной подвыборкой потенциальной полной выборки. Если предпосылки справедливы, то игнорирование неполноты выборки (наличия пропусков в данных) не приведёт к смещённым результатам.

Подводя итоги, отметим, что нарушение MCAR приводит к смещению самоотбора. MAR является более слабой предпосылкой, которая также помогает при заполнении пропусков в данных, так как она предполагает, что механизм потери данных зависит только от наблюдаемых значений.

\subsection{Игнорируемые и неигнорируемые пропуски} 

Механизм потери данных называют {\bf игнорируемым}, если (a) набор данных удовлетворяет предпосылке MAR и (b) параметры процесса порождающего пропуски, $\mathbf{\psi}$, не связаны с параметрами $\mathbf{\theta}$, которые мы хотим оценивать.

Это условие, схожее со {\bf слабой экзогенностью}, которая обсуждалась в Главе 2, подразумевает, что параметры $\mathbf{\theta}$ модели отличаются от параметров $\mathbf{\psi}$ механизма пропусков. Таким образом, если пропущенные данные игнорируемы, то нет необходимости в моделировании механизма пропущенных данных, как обязательной части исследовательского процесса. MAR и <<игнорируемость>> часто рассматриваются как эквивалентные при выполнении предпосылки о соблюдении условия (b) игнорируемости, что часто оправдано, см. работу Эллисона (2002).

{\bf Неигнорируемый} механизм появления пропущенных значений наблюдений возникает, если MAR-предпосылка нарушается для $(y, \, x)$. Случай, когда предпосылка MAR нарушена только для $x$ сюда не относится. В случае неигнорируемых пропусков необходимо моделировать процесс порождающий пропуски вместе с построением основной модели для получения состоятельных оценок параметров $\mathbf{\theta}$. Чтобы избежать возможного смещения выборки, должно быть использовано соответствующее оценивание, такое как, например, двухшаговая процедура Хекмана (см. Главу 16).

Литература по восстановлению пропусков в данных акцентируется на игнорируемых пропусках. Если  данные удовлетворяют предпосылке MCAR, то пропуски не вызывают проблем, кроме потери эффективности оценок, которая может быть уменьшена с помощью восстановления пропусков. Если ситуация иная, и данные удовлетворяет только предпосылке MAR, то методы восстановления пропущенных значений  должны быть использованы для  получения состоятельных оценок, а не только для увеличения  эффективности.

\section{Работа с пропусками  без применения моделей} 

Если никакие модели не могут быть использованы, то можно просто анализировать доступные данные или прибегнуть к анализу после восстановления пропусков без применения моделей.

\subsection{Использование только доступных данных}
Полное удаление наблюдений с пропусками или анализ полной выборки  означает исключение наблюдений, которые имеют пропущенные значения по одной или нескольким переменным из исследуемого набора. При MCAR-предпосылке, выборка, оставшаяся после такого исключения, является по-прежнему случайной выборкой из генеральной совокупности, так что полученные при этом оценки будут состоятельными. Тем не менее, стандартные ошибки будут выше, так как использовано меньше информации. Если набор регрессоров велик, то общий эффект от полного удаления наблюдений с пропусками может привести к существенным потерям в общем числе наблюдений. Это может подтолкнуть к  исключению из анализа переменных с высокой долей пропущенных значений, но результаты, полученные после такой операции, вероятно, будут неверными. 

% опечатка в английском тексте в следующем абзаце. MCAR вместо MAR верно.
Если MCAR-предпосылка не выполняется и данные удовлетворяют только MAR, то оценки будут смещёнными. Таким образом, полное удаление наблюдений с пропусками не робастно к отклонениям от MCAR. Однако полное удаление робастно к отклонениям от MСAR относительно независимых 
переменных в регрессионном анализе, если вероятность пропуска значений любого из регрессоров не зависит от значений зависимой переменной. Обобщая сказанное, можно сказать, что полное удаление наблюдений с пропусками допустимо, если случаи неполноты данных из-за пропущенных значений составляют небольшой процент, например, около 5\% или меньше, от общего числа (Шефер, 1996). Важно отметить, что выборка после полного удаления наблюдений с пропусками в этом случае репрезентативна. 


{\bf Попарное удаление} наблюдений  или анализ доступных данных \emph{(available-case analysis)} часто полагается более предпочтительным методом, чем полное удаление наблюдений с пропусками. Идея этого метода состоит в использовании всех доступных пар наблюдений $(x_{1i}, \, x_{2i})$ при оценивании выборочных моментов пары $(x_1, \, x_2)$ и в использовании всех наблюдений по отдельной переменной  при оценивании её моментов. Таким образом, в линейной регрессии, под попарным удалением мы будем понимать оценивание $(\mathbf{X’X})$ и $(\mathbf{X’y})$ с использованием всех возможных пар регрессоров, тогда как под полным удалением мы понимаем оценивание тех же матриц, но после удаления всех всех наблюдений с пропусками. Понятно, что мы теряем меньше информации при попарном исключении. Идея состоит в том, чтобы использовать максимум информации для оценивания индивидуальных описательных статистик, таких как средние и ковариации, а затем в использовании этих статистик для вычисления оценок параметров регрессии.

Есть два важных ограничения применения попарного исключения: (1) Вычисленные обычным методом стандартные ошибки и тестовые статистики будут смещенными и (2) полученная в результате матрица ковариаций регрессоров $(\mathbf{X’X})$ может не оказаться  положительно определённой.

\subsection{Восстановление данных без использования моделей} 

Существует ряд  ad hoc или слабо аргументированных процедур, которые часто реализованы в статистических пакетах.

{\bf Заполнение средним} \emph{(mean imputation)} подразумевает замещение пропуска средним из доступных значений. Эта процедура не меняет среднего значения, но оказывает влияние на распределение данных. Очевидно, плотность в середине распределения будет увеличена. Также она будет влиять на ковариации и корреляции с другими переменными.

{\bf Метод карточной колоды} подразумевает замену пропущенных значений случайно выбранными из доступных наблюдаемых значений этой переменной, он схож с процедурой бутстрапа. Этот метод сохраняет частное распределение переменной, но искажает ковариации и корреляции между переменными. 

При построении регрессии ни один из этих двух весьма распространённых подходов не является привлекательным несмотря на их простоту.

\section{Функция правдоподобия по наблюдаемым данным} 

Современный подход  состоит в восстановлении пропущенных значений с помощью однократной или нескольких повторных выборок из  оцененного распределения, основанного на заданной модели наблюдаемых данных и модели механизма появления пропусков в данных. Байесовские варианты этой процедуры используют выборку из апостериорного распределения, при построении которого учитываются и метод максимального правдоподобия, и априорное распределение параметров.

Первый важный вопрос --- роль, которую играет механизм проявления пропусков в процедуре их восстановления, и особенно, можно ли его игнорировать.

Обозначим $\mathbf{\theta}$ параметры процесса порождающего $\mathbf{Y}=(\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis})$ и с помощью $\mathbf{\psi}$ --- параметры механизма пропусков. Для удобства записи предположим, что $(\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis})$ --– непрерывные переменные. Тогда совместное распределение $(\mathbf{R}, \, \mathbf{Y}_{mis})$ имеет вид 
\begin{align}
\Prob[\mathbf{R}, \, \mathbf{Y}_{obs} | \mathbf{\theta}, \, \mathbf{\psi}] &= \int \Prob[\mathbf{R}, \, \mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}| \, \mathbf{\theta}, \, \mathbf{\psi}]\mathbf{dY}_{mis} \\
&= \int \Prob[\mathbf{R}| \, \mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}, \, \mathbf{\psi}] \Prob[\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}| \, \mathbf{\theta}]\mathbf{dY}_{mis} \notag \\
&= \Prob[\mathbf{R}| \, \mathbf{Y}_{obs}, \, \mathbf{\psi}] \int \Prob [\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}| \, \mathbf{\theta}]\mathbf{dY}_{mis} \notag \\
&=\Prob[\mathbf{R}| \, \mathbf{Y}_{obs}, \, \mathbf{\psi}] \Prob[\mathbf{Y}_{obs}| \, \mathbf{\theta}]. \notag
\end{align}
В первом равенство  формула совместной плотности $(\mathbf{R}, \, \mathbf{Y}_{obs})$ получается путем интегрирования (усреднения) по $\mathbf{Y}_{mis}$ совместной плотности для всех данных и $\mathbf{R}$. Вторая строка разлагает совместную плотность в произведение условных плотностей, условие берется по $\mathbf{Y}_{obs}$ и $\mathbf{Y}_{mis}$. В третьей строке разделяются механизм появления пропусков и механизм порождающий наблюдаемые данные, что возможно при выполнении предпосылки MAR. Последняя строка означает, то $\mathbf{\theta}$ и $\mathbf{\psi}$ являются несвязанными параметрами, и, таким образом, выводы относительно $\mathbf{\theta}$ могут быть сделаны баз учета механизма появления пропусков и только на основе $\mathbf{Y}_{obs}$. 

{\bf Функция правдоподобия по наблюдаемым данным} \emph{(observed-data likelihood)} пропорциональна последнему сомножителю в четвёртой строке:
\begin{equation}
\L[\mathbf{\theta}|\mathbf{Y}_{obs}] \propto \Prob[\mathbf{Y}_{obs}|\mathbf{\theta}]
\end{equation}
В ней задействованы только наблюдаемые значения $\mathbf{Y}_{obs}$, хотя параметры $\mathbf{\theta}$ появляются в процессе порождающем  все наблюдения (наблюдаемые и ненаблюдаемые). Как в Главе 13, коэффициент пропорциональности опущен в формуле (27.4).

В рамках MAR-предпосылки {\bf совместная апостериорная плотность}  параметров $(\mathbf{\theta}, \, \mathbf{\psi})$ представляется как произведение $\Prob[\mathbf{R}, \, \mathbf{Y}_{obs}|\mathbf{\theta}, \, \mathbf{\psi}]$ и совместной априорной плотности $\pi (\mathbf{\theta}, \, \mathbf{\psi})$:
\begin{align}
\Prob[\mathbf{\theta}, \, \mathbf{\psi}|\mathbf{Y}_{obs}, \, \mathbf{R}] &= k \Prob[\mathbf{R}, \, \mathbf{Y}_{obs}|\mathbf{\theta}, \, \mathbf{\psi}] \pi (\mathbf{\theta}, \, \mathbf{\psi}) \\
&\propto \Prob[\mathbf{R}|\mathbf{Y}_{obs}, \, \mathbf{\psi}] \Prob[\mathbf{Y}_{obs}|\mathbf{\theta}]\pi (\mathbf{\theta}, \, \mathbf{\psi}) \notag \\
&\propto \Prob[\mathbf{R}|\mathbf{Y}_{obs}, \, \mathbf{\psi}] \Prob[\mathbf{Y}_{obs}|\mathbf{\theta}]\pi_{\theta} (\mathbf{\theta})\pi_{\psi} (\mathbf{\psi}), \notag
\end{align}
где $k$ в первой строке есть коэффициент пропорциональности, не зависящий от $(\mathbf{\theta}, \, \mathbf{\psi})$. Во второй строке используется разложение на сомножители из (27.3), а в третьей строке использована предпосылка о независимости априорных распределений $\mathbf{\theta}$ и $\mathbf{\psi}$.

Так как основной интерес для нас представляют параметры $\mathbf{\theta}$, выразим частную апостериорную плотность для $\mathbf{\theta}$, проинтегрировав по $\mathbf{\psi}$  совместную апостериорную плотность. Мы получаем {\bf апостериорную плотность по наблюдаемым данным} \emph{(observed-data posterior)}
\begin{align}
\Prob[\mathbf{\theta}|\mathbf{Y}_{obs}, \, \mathbf{R}] &= \int \Prob[\mathbf{\theta}, \, \mathbf{\psi}|\mathbf{Y}_{obs}, \, \mathbf{R}]d\mathbf{\psi} \\
&\propto \Prob[\mathbf{Y}_{obs}|\mathbf{\theta}]\pi_{\theta} (\mathbf{\theta}) \int \Prob[\mathbf{R}|\mathbf{Y}_{obs}, \, \mathbf{\psi}]\pi_{\psi} (\mathbf{\psi}) \notag \\
&\propto \L [\mathbf{\theta}|\mathbf{Y}_{obs}]\pi_{\theta} (\mathbf{\theta}), \notag 
\end{align}
где во второй строке разделяются $\mathbf{\theta}$ и $\mathbf{\psi}$, а в последней строке интеграл включен в константу пропорциональности. Таким образом, выражение в последней строке не включает $\mathbf{\psi}$ и независимо от механизма появления пропусков в $\mathbf{R}$.

\section{Восстановление пропусков на основе регрессии} 

В этом Разделе мы рассмотрим восстановление пропусков на основе метода наименьших квадратов. Основной компонент этого подхода --- использование EM-алгоритма, ранее введённого и описанного в Разделе 10.3.7. 

EM-алгоритм состоит из двух шагов: подсчета ожидания и максимизации. Структура EM-алгоритма тесно связана с байесовским подходом MCMC и методами восстановления данных. Таким образом, вместо представления полностью готового к применению метода для работы с пропущенными значениями переменных, мы приведём пример, иллюстрирующий использования современных техник множественного восстановления пропусков в данных и показывающий основные свойства такого подхода.

\subsection{Пример линейной регрессии с пропущенными значениями зависимой переменной} 

На практике встречаются пропуски значений как зависимой (эндогенной) переменной, так и объясняющих переменных. Мы рассмотрим пример регрессии, в котором пропущены значения зависимой переменной,
\begin{align}
\begin{bmatrix}
\mathbf{y}_1 \\ \mathbf{y}_{mis} 
\end{bmatrix} 
= 
\begin{bmatrix}
\mathbf{X}_1 \\ \mathbf{X}_2 
\end{bmatrix} 
\mathbf{\beta}
+
\begin{bmatrix}
\mathbf{u}_1 \\ \mathbf{u}_2 
\end{bmatrix}
,
\end{align}
где $\Expect[\mathbf{u|X}]= \mathbf{0}$ и $\Expect[\mathbf{uu'|X}]= \sigma^2\mathbf{I_N}$. Сложность заключается в том, что блок наблюдений зависимой переменной $\mathbf{y}$, обозначенный как $\mathbf{y}_{mis}$, пропущен. Мы предполагаем, что доступные полные наблюдения являются случайной выборкой из генеральной совокупности, так что пропущенные данные удовлетворяют предпосылке MAR, но не MCAR.

При предпосылке MAR и $N_1>K$, первый блок из $N_1$ наблюдений может быть использован для получения состоятельных оценок вектора параметров размерности $K$, состоящего из $\mathbf{\beta}$  и $\sigma^2$. Оценки метода максимального правдоподобия $(\mathbf{\beta}, \, \sigma^2$  при гауссовском распределении ошибки буду равны $\widehat{\mathbf{\beta}}=[\mathbf{X'_1 X_1}]^{-1}\mathbf{X'_1 y_1}$ и $s^2=(\mathbf{y_1-X_1}\widehat{\mathbf{\beta}})'(\mathbf{y_1-X_1}\widehat{\mathbf{\beta}})/N_1$. Согласно стандартной теории и при предположениях о нормальности, $\widehat{\mathbf{\beta}}|\text{ данные } \sim \mathcal{N}[\mathbf{\beta}, \sigma^2[\mathbf{X'_1 X_1}]^{-1}]$ и $s^2 / \sigma^2 | \widehat{\mathbf{\beta}} \sim (N_1-K)\chi^2_{N_1-K}$.

Для начала рассмотрим процедуру наивного единичного восстановления пропущенных значений. Прогноз для $\mathbf{y}_{mis}$ при известном $\mathbf{X_2}$, обозначаемый как $\widehat{\mathbf{y}}_{mis}$, равен $\mathbf{X_2} \widehat{\mathbf{\beta}}$, где $\widehat{\mathbf{\beta}}$ --- описанная выше оценка, полученная на основе лишь первых $N_1$ наблюдений. Тогда
\begin{align}
\widehat{\Expect}[\mathbf{y}_{mis}|\mathbf{X_2}&=\widehat{\mathbf{y}}_{mis}=\mathbf{X_2} \widehat{\mathbf{\beta}}, \\
\widehat{\Var}[\mathbf{y}_{mis}]&\equiv \widehat{\Var}[\widehat{\mathbf{y}}|\mathbf{X_2}]=s^2(\mathbf{I}_{N_2}+\mathbf{X_2}[\mathbf{X'_1 X_1}]^{-1}]\mathbf{X'_2}). \notag
\end{align}
где $s^2\mathbf{I}_{N_2}$ является оценкой $\Var[\mathbf{u}_2]$.

В наивном методе генерируется $N_2$ предсказанных значений $\mathbf{y}_{mis}$, а затем применяются стандартные регрессионные методы для полной восстановленной выборки из $N=N_1+N_2$ наблюдений.

Два шага в наивном методе соответствуют двум шагам в {\bf EM-алгоритме}. Шаг, на котором предсказываются значения, соответствует {\bf E-шагу}, второй этап применения метода наименьших квадратов к расширенной выборке --- это {\bf M-шаг}.



Тем не менее, у этого решения есть недостатки. Во-первых, рассмотрим шаг пополнения выборки. Так как сгенерированные значения $\mathbf{y}_{mis}$ \emph{в точности} в плоскости МНК-прогнозов, то дополнение выборки значениями $(\widehat{\mathbf{y}}_{mis}, \, \mathbf{X_2})$ для получения новой оценки $\mathbf{\widehat{\beta}_A}$ не изменяют прошлого значения $\widehat{\mathbf{\beta}}$:
\begin{align*}
\mathbf{\widehat{\beta}_A}&= [\mathbf{X'_1 X_1}+\mathbf{X'_2 X_2}]^{-1}[\mathbf{X'_1 y_1}+\mathbf{X'}_2 \widehat{\mathbf{y}}_{mis}] \\
&=[\mathbf{X'_1 X_1}+\mathbf{X'_2 X_2}]^{-1}[\mathbf{X'_1 X_1}\widehat{\mathbf{\beta}}+\mathbf{X'_2 X_2}\widehat{\mathbf{\beta}}] \\
&= \widehat{\mathbf{\beta}}.
\end{align*}

Во-вторых, оценка $\sigma^2$, полученная по стандартной формуле из остатков регрессии по расширенной выборке даёт значение, которое слишком мало, так как дополнительные $N_2$ остатка равны нуля по построению,
\begin{align}
s^2_A &= (\mathbf{y-X}\widehat{\mathbf{\beta}}_A)'(\mathbf{y-X}\widehat{\mathbf{\beta}}_A)/N \\
&= (\mathbf{y_1-X_1}\widehat{\mathbf{\beta}})'(\mathbf{y_1-X_1}\widehat{\mathbf{\beta}})/N<s^2 \notag
\end{align}
где в $s^2$ используется верное деление на $N_1$, а не на $N$.

Наконец, из выражения для выборочной дисперсии $\widehat{\mathbf{y}}_{mis}$ можно сделать вывод, что сгенерированные предсказания гетероскедастичны, в отличие от $\mathbf{y}_1$, и, таким образом дисперсия $\widehat{\mathbf{\beta}}_A$ не может быть оценена по обычной формуле метода наименьших квадратов. Наблюдения $\widehat{\mathbf{y}}_{mis}$ получены из распределения с другой дисперсией. В наивном методе не принимается во внимание неопределённость относительно оценок $\widehat{\mathbf{y}}_{mis}$.

Чтобы решить эти проблемы, необходимо модифицировать метод. Во-первых, при оценивании $\widehat{\mathbf{y}}_{mis}$ должна учитываться неопределённость относительно $\widehat{\mathbf{\beta}}$. Это может быть сделано путём корректировки $\widehat{\mathbf{y}}_{mis}$ и добавки некоторого <<шума>> в генерируемые предсказания, так, чтобы оценки пропущенных значений больше походили на полученные из (оценённого или условного) распределения $\mathbf{y}_1$. На шаге стандартизации может быть использован факт того, что оценка $\Var[\widehat{\mathbf{y}}_{mis}]$, $\widehat{\mathbf{\Var}}$, доступна из (27.8). Следовательно, компоненты преобразованной переменной $\widehat{\mathbf{\Var}}^{-1/2}\widehat{\mathbf{y}}_{mis}$ будут иметь единичную дисперсию. Чтобы сымитировать распределение $y_1$, мы можем сгенерировать выборку Монте-Карло из распределения $\mathcal{N}[0, \, s^2]$ и умножить их на $\widehat{\mathbf{\Var}}^{-1/2}\widehat{\mathbf{y}}_{mis}$.

Пересмотренный алгоритм:
\begin{enumerate}
\item	Оценить $\widehat{\mathbf{\beta}}$, используя $N_1$ полных наблюдений, как и раньше.
\item	Сгенерировать значения $\widehat{\mathbf{y}}_{mis}=\mathbf{X}_2\widehat{\mathbf{\beta}}$.
\item	Сгенерировать скорректированные значения $\widehat{\mathbf{y}}_{mis}^a=(\widehat{\mathbf{\Var}}^{-1/2}\widehat{\mathbf{y}}_{mis}) \odot \mathbf{u}_m$, где $\mathbf{u}_m$ сгенерированы  методом Монте-Карло из распределения $\mathcal{N}[0, \, s^2]$ и $\odot$ обозначает поэлементное перемножение.
\item	Используя расширенную выборку, получить исправленное значение оценки $\widehat{\mathbf{\beta}}$.
\item	Повторить этапы 1--4, где для этапа 1 используется исправленное значение оценки $\widehat{\mathbf{\beta}}$.
\end{enumerate}

Шаги данного алгоритма,  также являющегося EM-алгоритмом, повторяются, пока не будет достигнута сходимость, а именно пока изменение в коэффициентах или изменение в сумме квадратов остатков МНК-регрессии не станут меньше заданного порога.

Чтобы связать эту тему с дальнейшим обсуждением, мы дадим алгоритму другую интерпретацию. На шаге 3 генерируется случайная величина из условного распределения $\mathbf{y}$ при заданном $\mathbf{\beta}$, а на шаге 4 генерируется случайная величина из условного распределения $\mathbf{\beta}$ при заданных $s^2$ и $\mathbf{X}$. Этот подход может быть развит далее  путём добавления ещё одного шага, на котором генерируется случайная величина из распределения $s^2$. Мы не будет проходить по всем этапам этого подхода, так как они становятся более понятными в ходе дальнейшего рассмотрения восстановления пропусков в данных.

Альтернативные модели пропущенных значений зависимой переменной были рассмотрены в Главе 16. Они не используют MAR-предпосылку и специфицируют неигнорируемые пропуски. В том случае  рассмотренный выше EM-алгоритм ведёт к несостоятельным оценкам. Цензурированная тобит-модель специфицирует, что пропуски имеют место  для наблюдений с $\mathbf{x'\beta}+u \leqslant 0$, и состоятельными являются тобит-оценки метода максимального правдоподобия (см. Раздел 16.3). Амэмия (1985, стр. 376-376) подробно рассматривает EM-алгоритм для тобит-модели.

\section{Пополнение данных и алгоритм MCMC}
Общая структура байесовского подхода к  данным с пропусками состоит в использовании следующего типа итерационного алгоритма, который включает шаги восстановления пропусков и предсказания. 
{\bf Шаг восстановления пропусков} \emph{(imputation step, I-step)} состоит в генерировании случайного значения из условного предсказанного распределения $\mathbf{Y}_{mis}$. Имея оценки $r$-ого шага,
\begin{equation}
\mathbf{Y}_{mis}^{(r+1)} \sim \Prob[\mathbf{Y}_{mis}|\mathbf{Y}_{obs}, \, \mathbf{\theta}^{(r)}].
\end{equation}
Данное выражение означает, что  случайно генерируется значение $\mathbf{Y}_{mis}^{(r+1)}$ на основе прогнозного условного распределения $\mathbf{Y}_{mis}$ при известном текущем значении оценки $\mathbf{\theta}^{(r)}$ и наблюдаемых значениях $\mathbf{Y}_{obs}$. Стоит отметить, что $\mathbf{Y}_{mis}$ в общем случае представляет собой матрицу, так что описанное выше относится  в принципе к генерации нескольких случайных значений.

{\bf Шаг предсказания} \emph{(prediction step, P-step)} осуществляется путём генерирования значений из апостериорного распределения для полной выборки
\begin{equation}
\mathbf{\theta}^{(r+1)} \sim \Prob[\mathbf{\theta}|\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}^{(r+1)}].
\end{equation}
Таким образом, $\mathbf{Y}_{obs}$ пополняются восстановленными значениями $\mathbf{Y}_{mis}^{(r+1)}$, сгенерированными из прогнозного распределения $\mathbf{Y}_{mis}$, а случайное значение генерируется из апостериорного распределения $\mathbf{\theta}$. Шаги (27.10) и (27.11) затем могут быть повторены.

Последовательное генерирование случайных значений согласно двум описанным шагам образует марковскую цепь. Этот процесс, который очень похож на EM-алгоритм, по сути является сэмплированием Гиббса из Раздела 13.5.2, но в литературе о пропусках в данных его относят к {\bf методам пополнения выборки}. При выполнении соответствующих условий и согласно теореме из Раздела 13.5.1, случайные значения будут сходиться к стационарному распределению при достаточно большой длине цепи $r$. Последний член цепи дает нам единственное восстановленное значение $\mathbf{Y}_{mis}$. Таким образом мы можем считать, что  $\mathbf{\theta}^{(r)}$ сгенерировано примерно согласно распределения $\Prob[\mathbf{\theta}|\mathbf{Y}_{obs}]$ и $\mathbf{Y}_{mis}^{(r+1)}$ --- примерно согласно распределению $\Prob[\mathbf{Y}_{mis}|\mathbf{Y}_{obs}]$. В любом случае MCMC цепь должна иметь значительную длину для обеспечения уверенности в том, что сходимость действительно имеет место для восстанавливаемых значений. Эта проблема была предметом рассмотрения Главы 13.

После того, как цепь сойдётся, мы получим решение одновременно двух задач --- восстановления пропущенных значений, основанного на модели, специфицированной для данных, а также оценивания модели с использованием как наблюдаемых, так и восстановленных значений переменных. После достижения сходимости мы будем иметь все необходимые данные для расчетов апостериорных моментов $\mathbf{\theta}$ и любых интересующих нас функций от $\mathbf{\theta}$, а также $\mathbf{Y}$, используя идеи из Главы 13.

В качестве примера рассматриваемой процедуры мы снова приведём  регрессию для случая пропущенных значений из предыдущей Раздела. Шаги MCMC-алгоритма следующие:

\begin{enumerate}
\item	На основе наблюдаемых данных вычислить $\widehat{\mathbf{\beta}}=[\mathbf{X'_1 X_1}]^{-1}\mathbf{X'_1 y_1}$ и $\widehat{\mathbf{u}}=(\mathbf{y_1-X_1}\widehat{\mathbf{\beta}})$.
\item	Сгенерировать $\sigma^2$ как $\widehat{\mathbf{u}}'\widehat{\mathbf{u}}$, делённое на случайное значение, полученное из распределения $\chi^2_{N_1-K}$.
\item	Сгенерировать $\mathbf{\beta}|\sigma^2 \sim \mathcal{N}[\widehat{\mathbf{\beta}}, \sigma^2[\mathbf{X'_1 X_1}]^{-1}]$.
\item	Сгенерировать $\mathbf{y}_{mis} \sim \mathcal{N}[\mathbf{X}_2\widehat{\mathbf{\beta}}, \sigma^2]$.
\item	Используя $\mathbf{y}$ вместо $\mathbf{y}_1$ и $\mathbf{X}$ вместо $\mathbf{X}_1$, повторить шаги с 1 по 4 после соответствующих корректировок.
\end{enumerate}

Обоснуем шаг 2. При неинформативном априорном распределении $(\mathbf{\beta}, \sigma^2)$, условное апостериорное распределение $\widehat{\mathbf{u}}'\widehat{\mathbf{u}}/ \sigma^2$ --- это  $\chi^2_{N_1-K}$-распределение, если использованы только наблюдаемые значения. После пополнения выборки используется $\chi^2_{N-K}$-распределение. Для обоснования шага 4 заметим,  что при неинформативном априорном распределении, условное апостериорное распределение $\beta$ --- это $\mathcal{N}[\widehat{\mathbf{\beta}}, \sigma^2[\mathbf{X'_1 X_1}]^{-1}]$. После пополнения выборки оно меняется до $\mathcal{N}[\widehat{\mathbf{\beta}}, \sigma^2[\mathbf{X' X}]^{-1}]$. На шаге 4 восстанавливаются пропущенные значения с использованием условной прогнозной плотности $\mathcal{N}[\mathbf{X}_2\widehat{\mathbf{\beta}}, \sigma^2]$. Эти шаги можно модифицировать, если мы используем, например информативное нормальное-гамма априорное распределение для $(\mathbf{\beta}, \sigma^2)$. Условные апостериорные законы распределения для этого случая приведены в Разделе 13.3.



\section{Множественное восстановление пропусков}

Анализ, проведённый в предыдущем Разделе, показывает, как  MCMC цепь помогает получить однократное восстановление пропуска. Тем не менее, однократное восстановление не позволяет адекватно учесть неопределённость, возникающую в случае пропущенных данных. Это ключевой аргумент в пользу применения процедуры множественного восстановления данных. Условное прогнозное распределение $\mathbf{Y}_{mis}|\mathbf{Y}_{obs}, \, \mathbf{\theta}$ получается путём усреднения по наблюдаемым данным апостериорного распределения $\mathbf{\theta}$:
\[
\Prob[\mathbf{Y}_{mis}|\mathbf{Y}_{obs}] = \int \Prob[\mathbf{Y}_{mis}|\mathbf{Y}_{obs}, \, \mathbf{\theta}] \Prob[\mathbf{\theta}|\mathbf{Y}_{obs}]\mathbf{d\theta}.
\]
Правильное множественное восстановление с точки зрения байесовского подхода отражает неопределённость относительно $\mathbf{Y}_{mis}$, при имеющейся неопределённости относительно  коэффициентов модели.

После {\bf множественного восстановления} пропущенные значения $\mathbf{Y}_{mis}$ заменяются восстановленными значениями $\mathbf{Y}_{mis}^{(1)}, \, \mathbf{Y}_{mis}^{(2)}, \, \mathbf{Y}_{mis}^{(3)}\, \dots, \, \mathbf{Y}_{mis}^{(m)}$. Каждый из восстановленных полных наборов данных затем анализируется как изначально бывший полным. Результаты $m$ раз проведённого анализа будут показывать изменчивость, отражающую неопределённость, возникшую из-за пропусков в данных. С $m$ различными наборами данных возникает вопрос о том, как определять нужное значение $m$ и как объединить $m$ наборов оценок параметров и ковариационных матриц. Мы освещаем оба вопроса, используя существующие результаты, но без подробных обоснований.

При объединении результатов, полученных с помощью множественного восстановления пропусков, основным является следующий результат, верный для произвольной статистики $Q$:
\begin{equation}
\Prob[Q|\mathbf{Y}_{obs}] = 
\int \Prob[Q|\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}] 
\Prob[\mathbf{Y}_{mis}|\mathbf{Y}_{obs}]\mathbf{d}\mathbf{Y}_{mis}
\end{equation}
Это результат говорит, что настоящее апостериорное распределение $Q$ можно получить путём усреднения апостериорного распределения $Q$ по полным данным. Это означает усреднение  по результатам множественного восстановления пропущенных значений (Рубин, 1996).

Из уравнения (27.12) следует, что финальная оценка $Q$ определяется с помощью закона повторных ожиданий
\begin{equation}
\Expect[Q|\mathbf{Y}_{obs}]=\Expect[\Expect[Q|\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}]|\mathbf{Y}_{obs}].
\end{equation}
Взяв среднее арифметическое всех $Q_r$, считаемых по полным восстановленным многократно выборкам, мы получим апостериорное среднее значение $Q$.

Финальная дисперсия $Q$ задается формулой
\begin{equation}
\Var[Q|\mathbf{Y}_{obs}]=\Expect[\Var[Q|\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}]|\mathbf{Y}_{obs}]+ \Var[\Expect[Q|\mathbf{Y}_{obs}, \, \mathbf{Y}_{mis}]|\mathbf{Y}_{obs}],
\end{equation}
здесь используется разложение дисперсии, приведённое в Разделе А.8.

Рубин (1996) также предложил следующие правила для учета информации о моментах, сформулированной в терминах скалярных параметров. Допустим, что для произвольного скалярного параметра, $\widehat{Q}_r$ --- точечная оценка $r$-ого из всех восстановления и $\widehat{U}_r$ --- оценка дисперсии. Тогда определим средние значения точечных оценок и оценок дисперсии соответственно как 
\begin{align}
\bar{Q}&=m^{-1} \sum \limits^{m}_{r=1}\widehat{Q}_r, \\
\bar{Q}&=m^{-1} \sum \limits^{m}_{r=1}\widehat{Q}_r, \\
\end{align}
и ковариацию между оценками разных восстановлений как
\begin{equation}
B=(m-1)^{-1} \sum \limits^{m}_{r=1}(\widehat{Q}_r - \bar{Q})^2
\end{equation}
тогда общая дисперсия равна
\begin{equation}
T=\bar{U}+(1+m^{-1})B.
\end{equation}


\begin{table}[h]
\begin{center}
\caption{\label{tab:27.1} Относительная эффективность множественного восстановления}
\begin{tabular}[t]{lccc}
\hline
\hline
Число & \multicolumn{2}{c}{Пропущено наблюдений ($\lambda$)} \\
восстановлений ($m$) & 10\% & 30\% & 50\% \\
\hline
3  &  0.967 & 0.909 & 0.857 \\
10 &  0.990 & 0.970 & 0.952 \\
20 &  0.995 & 0.985 & 0.975 \\
\hline
\hline
\end{tabular}
\end{center}
\end{table}




Результаты (27.15 и 27.16) следуют из (27.13), уравнение (27.18) следует из (27.14). Шефер (1997) приводит результаты для комбинирования $Р$-значений и статистик отношения правдоподобия и дает дополнительные ссылки.

После восстановления пропусков выводы относительно отдельных коэффициентов или групп коэффициентов можно строить на основе финальных оценок, так как стандартная центральная предельная теорема и связанные с ней результаты для больших выборок  могут быть распространены и на этот случай.

Следующая формула отражает меру относительной эффективности $m$ множественных восстановлений:
\begin{equation}
reff=(1+(\lambda/m))^{-1},
\end{equation}
где $\lambda$ --- доля пропущенных наблюдений. Эффективность измеряется относительно ситуации отсутствия пропусков. Арифметические вычисления в Таблице 27.1 показывают, что уже при трёх восстановлениях эффективность может быть равна 97\% при 10\% пропущенных значений и 86\% при 50\% пропущенных значений. Десять и более раз восстановленные данные дают относительную эффективность, превосходящую 95\% при 50\% пропусков. Таким образом, как было отмечено Шефером (1997), количество восстановлений не обязательно должно быть высоким.

\section{Пример восстановления пропусков с помощью MCMC} 

В этом Разделе приведено две иллюстрации  методов восстановления пропусков в данных: метод полного удаления наблюдений с пропусками и метод восстановления пропусков с помощью среднего значения, которые не требуют построения моделей, (см. Раздел 27.2), а также основанный на модели метод пополнения выборки с использованием MCMC алгоритма (см. Раздел 27.6). Пропущенными являются только значения  регрессоров, при этом механизм появления пропусков удовлетворяет предпосылке MAR.

Первый пример --- это  простая множественная регрессия, второй --- логит-регрессию. Для ясности и простоты мы используем искусственно сгенерированные данные с известным процессом, порождающи данные.

\subsection{Линейная регрессия с пропущенными значениями регрессоров} 



В нашем примере линейной регрессии данные порождаются процессом:
\begin{equation}
y_i=\beta_0+\beta_1 x_{1i}+\beta_2 x_{2i}+u_i, \, i=1, \, 2, \dots , \, N,
\end{equation}
с $u_i|x_{1i}, \, x_{2i} \sim \mathcal{N}[0, \, \sigma^2]$ и $(x_{1i}, \, x_{2i})$ имеющими двумерное нормальное распределение
\begin{equation}
\begin{bmatrix}
x_{1i} \\ x_{2i}
\end{bmatrix}
\sim \mathcal{N}
\begin{bmatrix}
\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix}
\end{bmatrix},
\end{equation}
так что $x_{2i}|x_{1i} \sim \mathcal{N}[\rho x_{1i}, \, 1-\rho^2]$. Также мы зададим $\mathbf{\beta'}=\begin{bmatrix} 1 & 1 & 1 \end{bmatrix}, \, n=1000$, и рассмотрим две доли случайно пропущенных значений $x_1$ и $x_2$ --- 10\% или 25\%. Для любого $i$, могут быть пропущены $x_1$ или $x_2$ или сразу оба регрессора. Мы также используем два разных значения $\rho$, 0.36 и 0.64.

Для марковской цепи 500 первых итераций используются в качестве прожига. Вычисления для марковской цепи были проведены с помощью алгоритма SAS MI Proc, в котором используется неинформативное априорное распределение. Для демонстрационных целей число восстановлений было взято равным 10, но длина цепи после прожига варьировалась от 10 до 10000. Proc MI комбинирует результаты множественных восстановлений, используя Уравнения (27.15)—(27.18).



\begin{table}[h]
\begin{center}
\caption{\label{tab:27.2} Восстановление пропущенных данных: линейная регрессия с 10\% пропущенных данных и высокой корреляций, MCMC алгоритм}
\begin{tabular}[t]{lccccccc}
\hline
\hline
 & Нет пропущенных & Полное & Замещение & \multicolumn{4}{c}{Длина марковской цепи} \\
 & данных & удаление & средним  & 10 & 1000 & 5000 & 10000 \\
\hline
$\hat{\beta}_0$  &  0.919 & 0.913 & 0.899 & 0.910 & 0.911 & 0.909 & 0.903 \\
& (0.104) & (0.113)& (0.105) & (0.102) & (0.101) & (0.103) & (0.101) \\
$\hat{\beta}_1$  &  1.097 & 1.067 & 1.053 & 1.196 & 1.205 & 1.199 & 1.199 \\
& (0.138) & (0.151) & (0.141) & (0.148) & (0.155) & (0.144) & (0.147) \\
$\hat{\beta}_2$  &  1.000 & 1.072 & 1.112 & 1.042 & 1.051 & 1.041 & 1.055 \\
& (0.132) & (0.145) & (0.135) & (0.140) & (0.146) & (0.143) & (0.146) \\
$R^2$ & 0.240 & 0.254 & 0.226 & & & & \\
\hline
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[h]
\begin{center}
\caption{\label{tab:27.3} Восстановление пропущенных данных: линейная регрессия с 25\% пропущенных данных и высокой корреляций, MCMC алгоритм}
\begin{tabular}[t]{lccccccc}
\hline
\hline
 & Нет пропущенных & Полное & Замещение & \multicolumn{4}{c}{Длина марковской цепи} \\
 & данных & удаление & средним  & 10 & 1000 & 5000 & 10000 \\
\hline
$\hat{\beta}_0$  &  0.919 & 0.863 & 0.984 & 0.899 & 0.898 & 0.925 &  0.900 \\
& (0.104) & (0.167) & (0.108) & (0.108) & (0.105) & (0.111) & (0.110) \\
$\hat{\beta}_1$  &  1.097 &  1.048 & 1.062 & 1.028 & 1.047 & 1.082 & 0.987 \\
&  (0.138) & (0.167) & (0.150) & (0.152) & (0.166) & (0.161) & (0.155) \\
$\hat{\beta}_2$  &  1.000 & 1.129 &  1.156 & 1.071 & 1.085 &  1.024 & 1.124 \\
&  (0.132) & (0.161) & (0.148) & (0.152) & (0.144) & (0.172) & (0.152)  \\
$R^2$ & 0.240 & 0.268 & 0.203 & & & & \\
\hline
\hline
\end{tabular}
\end{center}
\end{table}

              
      
      




Таблицы 27.2 и 27.3 содержат результаты для высоких $\rho$, а также низких и высоких долей пропусков. Между методами не наблюдается большой разницы. Так как выполнена MAR-предпосылка, то точечные оценки после полного удаления наблюдений с пропусками близки к оценкам по всей выборки.  Но, как и ожидалось, стандартные ошибки после полного удаления выше. При замещении средним точечные оценки $\beta_2$ отличаются больше, но наблюдаемые отличия находится в пределах выборочной ошибки. Оказалось, что в обоих случаях марковская цепь достигает стационарности довольно быстро, между результатами для 10 и 10000 итераций разница невелика. Вероятно, это происходит из-за того, что длина прожига составляет 500 итераций, что может быть больше, чем нужно для этого относительно простого случая.

В Таблице 27.4 симуляционная процедура повторена для <<наихудшего>> сценария: низкого значения $\rho$ и 25\% пропусков. Расхождение между точечными оценками по всей выборке, после полного удаления наблюдений с пропусками и после замещения средним оказалось в общем относительно больше, чем для случаев MCMС. Тем не менее, даже в этом случае отличия от оценок по всей выборке на самом деле не очень велико. Ещё раз мы увидели, что особые преимущества от использования длинной марковской цепи в этом примере выявлены не были.



\begin{table}[h]
\begin{center}
\caption{\label{tab:27.4} Восстановление пропущенных данных: линейная регрессия с 10\% пропущенных данных и низкой корреляций, MCMC алгоритм}
\begin{tabular}[t]{lccccccc}
\hline
\hline
 & Нет пропущенных & Полное & Замещение & \multicolumn{4}{c}{Длина марковской цепи} \\
 & данных & удаление & средним  & 10 & 1000 & 5000 & 10000 \\
\hline
$\hat{\beta}_0$  &  1.121 & 1.162 & 1.142 & 1.149 & 1.155 & 1.154 & 1.141 \\
& (0.099) & (0.130) & (0.103) & (0.104) & (0.103) & (0.104) & (0.101) \\
$\hat{\beta}_1$  & 1.099 & 0.930 & 1.052 & 1.026 & 1.020 & 1.004 & 1.044  \\
& (0.107) & (0.134) & (0.121) & (0.127) & (0.128) & (0.124) & (0.124) \\
$\hat{\beta}_2$  &  1.102 & 1.122 & 1.215 & 1.130 & 1.157 & 1.137 & 1.151 \\
&  (0.107) & (0.134) & (0.124) & (0.128) & (0.129) & (0.129) & (0.119) \\
$R^2$ & 0.243 & 0.235 & 0.186 & & & & \\
\hline
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[h]
\begin{center}
\caption{\label{tab:27.5} Восстановление пропущенных данных: логит-регрессия с 10\% пропущенных данных и высокой корреляций, MCMC алгоритм}
\begin{tabular}[t]{lccccccc}
\hline
\hline
 & Нет пропущенных & Полное & Замещение & \multicolumn{4}{c}{Длина марковской цепи} \\
 & данных & удаление & средним  & 10 & 1000 & 5000 & 10000 \\
\hline
$\hat{\beta}_0$  &  -0.447  & -0.498  & -0.439  & -0.527   &  -0.534   & -0.531  & -0.539   \\
                 &  (0.070) & (0.078) & (0.070) &  (0.073) &   (0.073) & (0.072) & (0.073)  \\
$\hat{\beta}_1$  &  -0.597  & -0.658  & -0.602  & -0.620   &  -0.673   & -0.681  & -0.675    \\
                 &  (0.096) & (0.108) & (0.098) &  (0.106) &   (0.102) & (0.101) & (0.103)  \\
$\hat{\beta}_2$  &  -0.444  & -0.474  & -0.523  & -0.597   &  -0.540   & -0.536  & -0.553   \\
                 &  (0.092) & (0.103) & (0.094) &  (0.107) &   (0.103) & (0.099) & (0.101)  \\
\hline
\hline
\end{tabular}
\end{center}
\end{table}







\subsection{Логит-регрессия с пропущенными значениями регрессоров} 
Далее мы рассмотрим пример нелинейной модели с пропущенными значениями регрессоров, используя симулированные данные. В этом симуляционном примере мы продолжим работать с данными, устройство которых было описано ранее, но заменим зависимую переменную на дискретную бинарную. Сначала изменим процесс порождающий данные из примера линейной регрессии, теперь $y=y^*$ --- латентная переменная. То есть:
\begin{equation}
y_i=\beta_0+\beta_1 x_{1i}+\beta_2 x_{2i}+u_i, \, i=1, \, 2, \dots , \, N.
\end{equation}
Тогда бинарная переменная $y_i$ формируется согласно  правилу:



\begin{equation}
y_i=
\begin{cases}
1, \text{ если } y_i^*>0, \\
0, \text{ если } y_i^*\leqslant 0
\end{cases}
\end{equation}



Мы будем моделировать вероятность того, что $y_i=0$, используя логит-модель, хоть подобный процесс порождающий данные подходит и для пробит-модели. Как было рассмотрено в Разделе 14.4.1, в логит-модели идентифицируется вектор параметров $\mathbf{\beta}/\sigma$, где дисперсия равна $\sigma^2=\pi^2/3$. Если все компоненты $\mathbf{\beta}$ равны единице, логит-модель даёт оценки истинных значений параметров, равные приблизительно -0.551. Оценивание с помощью MCMC осуществляется, как и раньше, с использованием неинформативного априорного распределения.

В Таблице 27.5 представлен <<благоприятный>> случай с 10\% пропусков в данных и высокой корреляции между $x_1$ и $x_2$, а в Таблице 27.6 представлен <<менее удачный>> случай с 25\% пропусков и низкой корреляцией между $x_1$ и $x_2$.

В первом случае даже при отсутствии пропусков оценка $\widehat{\beta}_2$ значительно отличается от своего ожидаемого значения. Оценки методом MCMC несколько изменяются, когда длина марковской цепи возрастает с 10 до 1000. Тем не менее, при дальнейшем увеличении длины цепи имеет место лишь небольшое изменение в точечных оценках, что можно интерпретировать как довод в пользу сходимости цепи к стационарному распределению.

Для второго примера с менее удачным процессом порождающим данные результаты представлены в Таблице 27.6. Основная разница состоит в том, что расхождение ожидаемых точечных оценок и оценённых значений несколько больше, чем в предыдущем случае. Тем не менее, говоря в общем, результаты множественного восстановления пропусков в случае логистической регрессии схожи с результатами для линейной регрессии.



\section{Практические соображения}

Основной вывод, касающийся методов анализа, о которых шла речь в этой Главе, состоит в том, что множественное восстановление данных имеет преимущество перед однократным. Более того, подходы, основанные на моделях, более адекватные, чем механические подходы, такие как замена средним или метод карточной колод. Тем не менее, во многих случаях практического применения процедур восстановления данных  реализация алгоритма MCMC может быть существенно труднее, чем относительно простые примеры, рассмотренные в последнем Разделе.

Следует различать множественное восстановление пропусков, где конечным результатом являются сами данные, и восстановлением, в котором конечный результат состоит из оценок коэффициентов для получения выводов. Хотя обе процедуры могут быть основаны на моделях, вторая может использовать более сложные эконометрические модели. Примеры этого можно найти в работах Браунстоуна и Валетта (1996), Штинебрикнера (1999), Кенникела (1998), а также Дэйви, Шанахам и Шефера (2001).

Даже когда первостепенно именно восстановление пропущенных значений, без подробного моделирования проблема может быть далеко не простой. Например, в своём исследовании 1995 года <<Обследования Финансового Положения Потребителей>> (Survey of Consumer Finances) Кинникел (1998, стр.. 5) отмечает, что:
!!!!!!!!
[Когда] исследование содержит большое количество переменных, имеет место значительная или частичная потеря (области) данных, характер пропущенной информации гетерогенен, законы распределения многих переменных искажены, данные имеют сложную структуру, [тогда] анализ в условиях отсутствия восстановления пропусков будет очень трудной задачей. Более того, любой, кто использует опубликованные базы данных, будет испытывать недостаток в ключевой информации, которая может оказаться важной для понимания закона распределения пропущенных значений. Так, даже при полной эффективности есть случаи, в которых стоит восстанавливать пропущенные данные.

Несмотря на сложность проблемы, Kennickell мог пользоваться процедурами восстановления пропусков, сходными с рассмотренными в этой Главе.

Stinebrinkner (1999), также столкнувшийся с проблемой пропусков в данных, в которой исключение списком !!!!! listwise deletion <<оставляет эконометриста со слишком маленьким набором данных, чтобы оценивать интересующую модель>>, развил двухшаговую симуляционную процедуру, основанную на максимальном правдоподобии, для оценивания совместного распределения пропущенных значений и для оценивания моделей длительности для продолжительности первого обучения \emph{(first teaching spell)}.

Для относительно простых случаев может быть использовано такое программное обеспечение, как пакет SAS Proc MI. S-Plus и SOLAS также помогают. Полезное пособие и исследование компьютерных программных пакетов предложили Horton и Lipsitz (2001). За дополнительной информацией стоит смотреть Интернет сайты с соответствующей тематикой.

Большинство методов анализа, на которых строится эта Глава, основаны на предпосылке об игнорируемости механизма появления пропусков в данных. С эконометрической точки зрения, это может служить значительным упрощением. Например, смотрите работу Lillard, Smith и Welch (1986), которые критиковали метод !!!!!!\emph{Census hot deck} для восстановления данных о зарплатах. Как же себя вести, если механизм не игнорируем? Как отмечено в Секции 27.4, неигнорируемый механизм появления пропусков в данных подразумевает, что параметры $\mathbf{\beta}$ и $\mathbf{\psi}$ взаимосвязаны. Тогда нужно специфицировать в явном виде этот механизм, как в случае моделей отбора \emph{(selection models)} и моделей систематической ошибки, связанной с выбыванием наблюдений из выборки \emph{(models of attrition bias)} (см. Главу 16 и Секцию 23.5.2). Шефер (1997, p. 28) привёл ссылки на литературу по этой теме.

\section{Библиографические заметки}
Среди важных ранних работ следует отметить работы Литтла и Рубина (1987) и Рубина (1987). Эллисон (2002) приводит относительно нетехническое, но понятное введение в проблему пропущенных значений переменных и соответствующую литературу. Рубин (1996) приводит обзор с историческим аспектом. Шефер (1997) предложил более полный анализ, который включает в себя случаи категориальных переменных, смешенных дискретно-непрерывных данных и данных комплексных обследований.
\begin{enumerate}
\item Менг (2000) предложил историческую справку по механизмам появления пропусков.
\item Литтл (1988, 1992) привёл хороший обзор литературы о линейных регрессиях с пропусками в регрессорах, затрагивающей как случаи подходов, основанных на моделях, так и не требующие их.
\end{enumerate}

\section*{Упражнения} 

\begin{enumerate}
\item Пусть есть некоторая регрессионная модель, линейная или нелинейная, с зависимой переменной $y$ и экзогенными переменными $\mathbf{x}$, а также независимыми одинаково распределёнными ошибками $\mathbf{\epsilon}$. Покажите, что, если вероятность пропущенных значений $\mathbf{x}$ независима от $y$, то регрессия, основанная на полном удалении наблюдений с пропусками, обеспечивает состоятельную оценку условной функции среднего. [Подсказка: покажите, что условное распределение $y$ при заданном $\mathbf{x}$ не искажается при наличии пропусков.]

\item (Адаптировано из Гурьеру и Монфора, 1981). Пусть есть регрессионная модель $\mathbf{y}=\beta_1\mathbf{x}+\mathbf{Z\beta_2} + \mathbf{u}$, где $\mathbf{y}$ --– вектор размерности $N \times 1$, $\mathbf{Z}$ --– матрица размерности $N \times K$, а $\mathbf{x}$ --– скалярный регрессор, вектор размерности $N \times 1$, некоторые компоненты которого пропущены. Предположим, что наблюдения пропущены случайно и $\Expect[\mathbf{u}|\mathbf{x, \, Z}]=\mathbf{0}$ и $\Expect[\mathbf{uu'}|\mathbf{x, \, Z}]=\sigma^2\mathbf{I}_N$. И $\mathbf{Z}$, и $\mathbf{y}$ полностью наблюдаемы. Следующий механизм предложен для работы с пропущенными значениями. Предположим линейную регрессионную модель, связывающую $\mathbf{x}$ и $\mathbf{Z}$, $\mathbf{x}=\mathbf{Z\gamma}+ \mathbf{\epsilon}$, где $\Expect[\mathbf{\epsilon}| \, \mathbf{Z}]=\mathbf{0}$ и $\Expect[\mathbf{\epsilon \epsilon'}| \, \mathbf{Z}]=\sigma^2_{\epsilon}\mathbf{I}_N$. Тогда пусть $\widehat{\mathbf{\gamma}}=[\mathbf{Z'_c Z_c}]^{-1}\mathbf{Z'_c x_c}$, где индекс c относится к <<полным данным>> \emph{(complete data)}. Восстановим значения согласно $\widehat{\mathbf{x}}_m=\mathbf{Z_m}[\mathbf{Z'_c Z_c}]^{-1}\mathbf{Z'_c x_c}$, где $\mathbf{x}_m$ относится к пропущенным наблюдениям и $\mathbf{Z_m}$ –-- к соответствующим значениям $\mathbf{Z}$. Исходная регрессия тогда переоценивается с использованием полного набора из $N$ наблюдений после восстановления пропусков в $\mathbf{x}$ с помощью восстановленных значений.
\begin{enumerate}
\item	Объясните, почему МНК-оценки, основанные на полных и восстановленных данных могут быть смещенными в случае малой выборки.
\item	Какие дополнительные предпосылки нужны для того, чтобы доказать состоятельность МНК-оценок, основанных на полных и восстановленных данных?
\item	Будет ли МНК-оценка эффективной?
\end{enumerate}

\item Рассмотрим утверждение о том, что при оценивании модели после восстановления пропусков в данных  оценки точности оценок скорее всего будут завышены, если не будет сделано никакой корректировки на наличие шага восстановления. Другими словами, восстановленные данные могут быть рассмотрены как сгенерированные переменные и, таким образом, с ними возникают проблемы как при двухшаговогом оценивании, о котором шла речь в Разделе 6.6. Объясните, будет ли корректировка, связанная с восстановлением пропущенных данных, необходима в асимптотике.

\end{enumerate}



