
\chapter{Оценивание с помощью метода максимального правдоподобия и
нелинейнего метода наименьших квадратов}

\section{Вступление}

Нелинейная оценка --- это нелинейная функция зависимой переменной. Большинство оценок, используемых в микроэконометрике, кроме оценок метода наименьших квадратов и метода инструментальных переменных в линейной регрессионной модели, представленной в Главе 4, нелинейны. Нелинейность может возникнуть несколькими путями. Условное математическое ожидание может быть нелинейно по параметрам. Функция потерь может привести к нелинейной оценке даже в том случае, когда условное математическое ожидание линейно по параметрам. Усечение и цензурирование также могут привести к нелинейной оценке даже, если условное математическое в первоначальной модели линейно по параметрам.

Здесь мы представляем наиболее важные статистические результаты для нелинейного оценивания. Для нелинейных оценок на конечных выборках доступны очень ограниченные результаты. Большинство статистических методов основаны на асимптотической теории, которая может быть применена для больших выборок. Оценки, которые используются в микроэконометрике, состоятельны и асимптотически нормальны. 

Асимптотическая теория влечёт за собой два главных отличия от работы с линейной регрессионной моделью, которые изложены во вводном курсе. Во-первых, альтернативные методы доказательства необходимы, поскольку нет явной формулы для большинства оценок. Во-вторых, асимптотическое распределение обычно получают при самых слабых предпосылках о распределении. Это отличие было введено в Разделе 4.4 для получения оценок, устойчивых к гетероскедастичности, методом наименьших квадратов. При таких более слабых предпосылках стандартные ошибки, полученные в обычной регрессии по умолчанию, неверны. Однако необходимо быть осторожным, поскольку эти более слабые предпосылки могут привести к несостоятельности оценки самих коэффициентов, что представляет собой более существенную проблему.

По возможности в этой главе мы приводим пояснения. Определения сходимости по вероятности и по распределению, закон больших чисел (ЗБЧ), центральная предельная теорема (ЦПТ) представлены в большом количестве литературы, в этой книге данные темы помещены в Приложение А. В прикладных исследованиях редко ставится цель формально доказать состоятельность и асимптотическую нормальность. Однако можно нередко столкнуться с прикладными исследованиями, в которых есть новые или сложные проблемы с оцениванием, что ведёт к необходимости ознакомления с передовыми статьями в эконометрических журналах. Тогда знание доказательства состоятельности и асимптотической нормальности очень полезно, особенно для того, чтобы заранее хорошо представлять вероятную форму ковариационной матрицы оценки.

В Разделе 5.2 представлен обзор ключевых результатов. Более формальное рассмотрение оценок экстремума, которые максимизируют или минимизируют любую целевую функцию, приведено в Разделе 5.3. Определения оценок, основанных на оценивании уравнений, и сами оценки приведены в Разделе 5.4. Статистические результаты, основанные на стандартных ошибках с поправкой, кратко представлены в Разделе 5.5 с последующей полной интерпретацией в Главе 7. Оценивание методом максимального правдоподобия и методом квази-максимального правдоподобия представлены в Разделах 5.6 и 5.7. Оценивание нелинейным методом наименьших квадратов приведено в Разделе 5.8. В Разделе 5.9 приведён подробный пример. 

Другие важные процедуры параметрического оценивания --- обобщённый метод моментов и нелинейный метод инструментальных переменных --- представлены в Главе 6.

\section{Обзор нелинейных оценок}

В данном разделе кратко изложены асимптотические свойства нелинейных оценок, которые более тщательно разобраны в Разделе 5.3. Также здесь представлены пути интерпретации регрессионных коэффициентов в нелинейных моделях. Материал необходим для понимания моделей для работы с пространственными и панельными данными, которые приведены в последующих главах.

\subsection{Пример пуассоновской регрессии}

Полезно ввести конкретный пример нелинейного оценивания. Здесь мы рассмотрим регрессию Пуассона, которая более подробно проанализирована в Главе 20.

Пуассоновское распределение подходит для зависимой переменной $y$, которая принимает только неотрицательные целые значения $0,1, \dots$. Оно может применяться для моделирования количества произошедших событий, например числа патентных заявок, поданных фирмой, или числа визитов человека к врачу.

Функция вероятности для распределения Пуассона с параметром $\lambda$, выглядит следующим образом:
\[
f(y|\lambda)= e^{-\lambda}\lambda^{y} / y! , y = 0,1,2, \dots,
\]
где может быть показано, что $\E[y]=\lambda$ и $\Var[y]=\lambda$.

Регрессионная модель предполагает, что параметр $\lambda$ меняется между индивидами соответственно определённой функции от вектора регрессоров $x$ и вектора параметров $\beta$. Обычная спецификация Пуассона выглядит так:
\[
\lambda = \exp(x'\beta).
\]
Такая спецификация автоматически обеспечивает $\lambda > 0$. Таким образом в регрессии Пуассона вероятность для отдельного наблюдения равна:
\begin{equation}
f(y|x,\beta)= e^{-\exp(x'\beta)} \exp(x'\beta)^{y}/y!.
\end{equation}

Рассмотрим оценку методом максимального правдоподобия, основанную на выборке $\{(y_i,x_i),i=1, \dots, N \}$. Оценка максимального правдоподобия максимизирует логарифмическую функцию правдоподобия (См. Раздел 5.6). Функция правдоподобия --- это совместная функция плотности или вероятности, которая при условии независимости наблюдений является произведением $\prod_i f(y_i|x_i,\beta)$ индивидуальных функций плотности. Здесь мы говорим об условной вероятности при фиксированных регрессорах. Логарифмическая функция правдоподобия --- это логарифм от произведения, который равен сумме логарифмов, или $\sum_i{\ln f(y_i|x_i,\beta)}$.

Для регрессии Пуассона (5.1) логарифм вероятности для $i$-ого наблюдения следующий:
\[
\ln f(y_i|x_i,\beta)= -\exp(x'_i\beta)+ y_i x'_i \beta - \ln y_i!.
\]

Таким образом, оценка ММП $\hat{\beta}$ в регрессии Пуассона максимизирует
\begin{equation}
\mathcal{Q}_N(\beta)= \frac{1}{N} \sum_{i=1}^{N}\{-\exp(x'_i\beta)+ y_i x'_i \beta - \ln y_i!\},
\end{equation}
где включается нормирование $1/N$ для того, чтобы $\mathcal{Q}_N(\beta)$ оставалось конечным, поскольку $N \rightarrow \infty $. Оценка ММП регрессии Пуассона является решением условия первого порядка $ \left. \partial \mathcal{Q}_N(\beta) / \partial \beta \right|_{\hat{\beta}} = 0$ или
\begin{equation}
\frac{1}{N} \sum_{i=1}^{N}(y_i-\exp(x'_i\beta))x_i |_{\hat{\beta}}=0.
\end{equation}

Не существует явного решения для $\hat{\beta}$ в (5.3). Численные методы для расчёта $\hat{\beta}$ представлены в Главе 10. В данной Главе мы концентрируемся на статистических свойствах полученной оценки $\hat{\beta}$.

\subsection{М-оценки}

В более общем случае мы называем М-оценкой $\hat{\theta}$ вектора параметров $\theta$ размера $q \times 1$ оценку, которая максимизирует целевую функцию, которая представляет собой сумму или среднее $N$ дополнительных функций
\begin{equation}
\mathcal{Q}_N(\theta)= \frac{1}{N} \sum_{i=1}^{N} q(y_i,x_i,\theta),
\end{equation}
где $q(\cdot)$ --- скалярная функция, $y_i$ --- зависимая переменная, $x_i$ --- вектор регрессоров. К тому же во всех результатах, полученных в данном Разделе, предполагается независимость по $i$.

Для простоты $y_i$ записывается в скалярном виде, однако результаты могут быть распространены на случай векторного $y_i$, и соответственно покрывают многомерные и панельные данные, а также системы уравнений. У целевой функции нижний индекс $N$ введен для того, чтобы показать что она зависит от выборочных данных. В данной книге $q$ используется для обозначения размерности $\theta$. Обратите внимание на то, что в данном случае $q$ используется для обозначения дополнительной функции $q(\cdot)$ в (5.4).

Многие эконометрические оценки и модели --- это М-оценки для конкретных функциональных форм $q(y,x,\theta)$. Главными примерами могут послужить метод максимального правдоподобия (см. (5.39) далее) и нелинейный метод наименьших квадратов (НМНК) (см. (5.67) далее). Оценка максимального правдоподобия регрессии Пуассона, которая максимизирует (5.2), является примером (5.4) при $\theta=\beta$ и $q(y,x,\beta)=-\exp(x'\beta)+y x' \beta - \ln y!$.

Мы сконцентрируемся на оценке $\hat{\theta}$, которая является решением условия первого порядка  
$ \partial \mathcal{Q}_N(\theta) / \partial \theta |_{\hat{\theta}} = 0$ или
\begin{equation}
\left. \frac{1}{N} \sum_{i=1}^{N} \frac{\partial q(y_i,x_i,\theta)}{\partial \theta} \right|_{\hat{\theta}}=0.
\end{equation}

Это система из $q$ уравнений с $q$ неизвестными, в которой нет явного решения для $\hat{\theta}$.

Термин М-оценка, который был введён Губером (1967), интерпретируется как сокращение для оценки, похожей на оценку максимального правдоподобия. Многие эконометристы, включая Амэмия (1985, p. 105), Грина (2003, p. 461), и Вулдриджа (2002, p. 344), определяют М-оценку как точку максимума суммы слагаемых, то есть как (5.4). Другие авторы, включая Серфлинга (1980), определяют М-оценку как решение уравнений, как (5.5). Губер (1967) рассматривал оба случая и он (1981, p. 43) явно определил М-оценку для обоих случаев. В данной книге мы берём первое определение для М-оценки, а под вторым мы подразумеваем оценку метода оценочных уравнений (это будет рассмотрено отдельно в Разделе 5.4).

\subsection{Асимптотические свойства М-оценок}

Ключевые асимптотические свойства оценки заключаются в том, что она может быть состоятельна и иметь асимптотическое распределение. Это необходимо для того, чтобы делать статистические  выводы хотя бы для больших выборок.

\begin{center}
Состоятельность
\end{center}

Первый шаг в определении свойств $\hat{\theta}$ заключается в необходимости точно определить, что $\hat{\theta}$ должна оценивать. Мы предполагаем, что существует единственное значение $\theta$, которое обозначается как $\theta_0$ и называется реальным значением параметра, описывающим данные. Данное условие идентификации (см. Раздел 2.5) требует правильной спецификации интересующего процесса, порождающего данные, а также единственности данного представления. Таким образом, для примера с распределением Пуассона можно предположить, что процесс порождающий данные --- это пуассоновское распределение с параметром $\exp(x'\beta_0)$ и регрессор $x$ таков, что $x'\beta^{(1)}=x'\beta^{(2)}$, если и только если $\beta^{(1)}=\beta^{(2)}$. 

Формальная запись с нижним индексом 0 для реального значения параметра широко применяется в Главах с 5 по 8. Это обусловлено тем, что $\theta$ может принимать много разных значений, но интерес представляют два особых значения --- настоящее значение $\theta_0$ и оценочное значение $\hat{\theta}$.

Оцениваемое $\hat{\theta}$ никогда не будет равно $\theta_0$ даже для очень больших выборок из-за случайности выборки. Вместо этого нам необходимо, чтобы $\hat{\theta}$ была состоятельной оценкой для $\theta_0$ (см. определение А.2 в Приложении А), подразумевая под этим, что $\hat{\theta}$ должна сходиться по вероятности к $\theta_0$, что обозначается следующим образом:  $\hat{\theta} \xrightarrow{p} \theta_0$. 

Строго доказать состоятельность М-оценок бывает достаточно затруднительно. Формальные результаты приведены в Разделе 5.3.2, а полезное неформальное условие приведено в 5.3.7. Более конкретно оценки метода максимального правдоподобия и нелинейного метода наименьших квадратов рассматриваются в последующих Разделах.

\begin{center}
Предельное нормальное распределение
\end{center}

При состоятельности, при $N \rightarrow \infty$, оценка $\hat{\theta}$ имеет распределение, сосредоточенное около $\theta_0$. Для метода наименьших квадратов мы увеличиваем или масштабируем $\hat{\theta}$, умножая на $\sqrt{N}$, для того, чтобы получить случайную величину, которая имеет невырожденное распределение при $N \rightarrow \infty$. Затем делаются статистические выводы при предположении о том, что $N$ достаточно большое для того, чтобы асимптотическая теория давала хорошую аппроксимацию, но не такое большое, чтобы $\hat{\theta}$ вырождалось в точку $\theta_0$.

В связи с этим мы анализируем поведение $\sqrt{N}(\hat{\theta} - \theta_0)$. Для большинства оценок распределение в конечной выборке у такой конструкции слишком сложное, чтобы  можно было делать выводы. Вместо этого используется асимптотическая теория, чтобы получить предельное распределение при $N \rightarrow \infty$. Для большинства оценок в микроэконометрике этим пределом является многомерное нормальное распределение. Более формально 
$\sqrt{N}(\hat{\theta} - \theta_0)$ сходится по распределению к многомерному нормальному распределению (определение сходимости по распределению можно найти в Приложении А).

Вспомним из Раздела 4.4, что оценка МНК может быть представлена следующим образом:
\[
N(\hat{\beta} - \beta_0)= \left( \frac{1}{N} \sum_{i=1}^{N} x_i x'_i \right)^{-1} \frac{1}{\sqrt N} \sum_{i=1}^{N} x_i u_i.
\]

Предел распределения был выведен с помощью получения предела по вероятности первого члена справа и предельного нормального распределения второго члена. Предел распределения М-оценки может быть получен аналогичным образом. В Разделе 5.3.3 мы показываем, что для оценки, которая является решением (5.5), всегда верно представление
\begin{equation}
\sqrt{N}(\hat{\theta} - \theta_0)=- \left. \left(\frac{1}{N} \sum_{i=1}^{N} \frac{\partial^2 q_i(\theta)}{\partial \theta \partial \theta'} \right|_{\theta^+} \right)^{-1} \left. \frac{1}{\sqrt N} \sum_{i=1}^{N} \frac{\partial q_i(\theta)}{\partial \theta} \right|_{\theta_0},
\end{equation}
где $q_i(\theta)=q(y_i,x_i,\theta)$ для какого-то $\theta^+$ между $\hat{\theta}$ и $\theta_0$ при условии, что существуют вторые производные и обратная матрица. Этот результат может быть получен разложением в ряд Тейлора.

При подходящих предпосылках это приводит к следующему предельному распределению М-оценки
\begin{equation}
\sqrt{N}(\hat{\theta} - \theta_0) \xrightarrow{d} \mathcal{N}[0,A_0^{-1}B_0 A_0^{-1}],
\end{equation}
где $A_0^{-1}$ --- предел по вероятности первого члена справа в (5.6), и предполагается, что второй член сходится к нормальному распределению $\mathcal{N}[0,B_0]$. Выражения $A_0$ и $B_0$ приведены в Таблице 5.1.

\begin{center}
Асимптотическая нормальность
\end{center}

Чтобы получить распределение $\hat{\theta}$ из результата предельного распределения (5.7), разделим левую часть (5.7) на $\sqrt{N}$, следовательно, разделим дисперсию на $N$. Тогда
\begin{equation}
\hat{\theta} \stackrel{a}{\sim}\mathcal{N}[\theta_0,\Var[\hat{\theta}]],
\end{equation}
где $\stackrel{a}{\sim}$ означает <<асимптотически распределена>>, $\Var[\hat{\theta}]$ --- это асимптотическая дисперсия $\hat{\theta}$ такая, что 
\begin{equation}
\Var[\hat{\theta}]=N^{-1}A_0^{-1}B_0 A_0^{-1}.
\end{equation}
Более полное обсуждение термина <<асимптотическое распределение>> уже было дано в Разделе 4.4.4, а также оно приведено в Разделе А.6.4. 

\begin{table}[h]
\caption{\label{tab:as}Асимптотические свойства М-оценок}
\begin{minipage}{\textwidth}
\begin{tabular}[t]{ll}
\hline
\hline
\bf{Свойство}\footnote{Дисперсия предельного распределения и асимптотическая оценка дисперсии являются робастными сэндвич формами, которые предполагают независимость по $i$. См. Раздел 5.5.2 для других оценок дисперсии.} & \bf{Алгебраическая формула} \\
\hline
Целевая функция &   $\mathcal{Q}_N(\theta)=N^{-1}\sum_i q(y_i,x_i,\theta)$ максимизируется по $\theta$ \\
Примеры &  ММП: $q_i=\ln f(y_i|x_i,\theta)$ --- логарифмическая плотность \\
& НМНК: $q_i= - (y_i-g(x_i,\theta))^2$ --- минус квадрат ошибки \\
Условие первого порядка & 
$ \partial \mathcal{Q}_N(\theta) / \partial \theta =
N^{-1} \sum_{i=1}^{N} \partial q(y_i,x_i,\theta) / \partial \theta |_{\hat{\theta}}=0 $\\
Состоятельность & Максимален ли $\plim \mathcal{Q}_N(\theta)$ при $\theta=\theta_0$? \\
Состоятельность (неформально) & Выполняется ли $\E[\partial q(y_i,x_i,\theta) / \partial \theta |_{\theta_0}]=0$?\\
Предельное распределение & $\sqrt{N}(\hat{\theta} - \theta_0) \xrightarrow{d} \mathcal{N}[0,A_0^{-1}B_0 A_0^{-1}]$ \\
& $A_0=\plim N^{-1} \sum_{i=1}^{N} \partial^2 q_i(\theta) / \partial \theta \partial \theta'|_{\theta_0}$\\
& $B_0=\plim N^{-1} \sum_{i=1}^{N} \partial q_i/ \partial \theta \times \partial q_i/ \partial \theta'|_{\theta_0}$ \\
Асимптотическое распределение &  $\hat{\theta} \stackrel{a}{\sim}\mathcal{N}[\theta_0,N^{-1} \hat{A}^{-1} \hat{B} \hat{A}^{-1}]$\\
& $\hat{A}=N^{-1} \sum_{i=1}^{N} \partial^2 q_i(\theta) / \partial \theta \partial \theta'|_{\hat{\theta}}$\\
& $\hat{B}=N^{-1} \sum_{i=1}^{N} \partial q_i/ \partial \theta \times \partial q_i/ \partial \theta'|_{\hat{\theta}}$ \\
\hline
\hline
\end{tabular}
\end{minipage}
\end{table}

Результат (5.9) зависит от неизвестного истинного значения параметра $\theta_0$. На практике вычисляют оценку асимптотической дисперсии
\begin{equation}
\widehat{\Var}[\hat{\theta}]=N^{-1} \hat{A}^{-1} \hat{B} \hat{A}^{-1},
\end{equation}
где $\hat{A}$ и $\hat{B}$ --- это состоятельные оценки для $A_0$ и $B_0$.

Вместо этого многие статистические пакеты по умолчанию часто используют более простую оценку $\widehat{\Var}[\hat{\theta}]=-N^{-1}\hat{A}^{-1}$, которую правомерно использовать только в некоторых особых случаях. См. раздел 5.5 для дальнейшего обсуждения различных способов оценки $A_0$ и $B_0$ и проверки гипотез.

Два важных примера М-оценок --- это оценки ММП и НМНК. Формальные результаты для этих оценок представлены, соответственно, утверждениями 5.5 и 5.6. Более простые представления асимптотических распределений этих оценок даны в (5.48) и (5.77).

\begin{center}
Пример ММП для распределения Пуассона 
\end{center}

Как и другие оценки ММП, оценка ММП для распределения Пуассона состоятельна, если плотность правильно специфицирована. Однако, подставляя (5.25) из Раздела 5.3.7 в (5.3), видно, что необходимым условием состоятельности является на самом деле более слабое условие  $\E[y|x]=\exp(x'\beta_0)$, что есть правильная спецификация математического ожидания. Похожая робастность к частично неправильной спецификации оценки ММП справедлива и для некоторых других особых случаев, они подробно описаны в Разделе 5.7.

Оценка ММП для распределения Пуассона $\partial q(\beta) / \partial \beta= (y-\exp(x'\beta_0))x$ приводит к 
\[
A_0=-\plim N^{-1} \sum_i \exp(x'_i\beta_0)x_i x'_i
\]
и
\[
B_0=\plim N^{-1} \sum_i \Var[y_i|x_i] x_i x'_i.
\]
 
Тогда $\hat{\beta} \stackrel{a}{\sim}\mathcal{N}[\theta_0,N^{-1} \hat{A}^{-1} \hat{B} \hat{A}^{-1}]$, где $\hat{A}=- N^{-1} \sum_i \exp(x'_i \hat{\beta})x_i x'_i$ и $\hat{B}=N^{-1} \sum_i (y_i - \exp(x'_i \hat{\beta}))^2 x_i x'_i$.

\begin{table}[h]
\begin{center}
\caption{\label{tab:pred3est}Предельный эффект: три разные оценки}
\begin{tabular}[t]{ll}
\hline
\hline
\bf{Формула} & \bf{Описание} \\
\hline
$N^{-1} \sum_i \partial \E[y_i|x_i] / \partial x_i$ & Средний эффект по всем индивидам \\
$\partial \E[y|x] / \partial x |_{\bar{x}}$ & Предельный эффект для среднего человека \\
$\partial \E[y|x] / \partial x |_{x^*}$ & Предельный эффект для человека с $x=x^*$\\
\hline
\hline
\end{tabular}
\end{center}
\end{table}

Если данные на самом деле распределены по Пуассоновскому закону, тогда $\Var[y|x]=\E[y|x]= \exp(x'\beta_0)$, что ведёт к возможному упрощению, поскольку $A_0$=$-B_0$, то есть $A_0^{-1}B_0 A_0^{-1}=-A_0^{-1}$. Однако в большинстве приложений с численными данными $\Var[y|x]>\E[y|x]$, поэтому лучше не вводить это ограничение.

\subsection{Интерпретация коэффициентов нелинейной регрессии}

Важной целью оценивания часто являются предсказания, а не проверка статистической значимости регрессоров.

\begin{center}
Предельные эффекты
\end{center}

Часто интерес заключается в измерении предельных эффектов, то есть в изменении условного математического ожидания $y$, когда регрессор $x$ изменяется на единицу.

Для линейной регрессионной модели, $\E[y|x]= x'\beta$ подразумевает $\partial \E[y|x] / \partial x = \beta$ для того, чтобы коэффициент имел прямую интерпретацию как предельный эффект. Для нелинейных регрессионных моделей эта интерпретация не применима. Например, если $\E[y|x]= \exp(x'\beta)$, то $\partial \E[y|x] / \partial x = \exp(x'\beta)\beta$ является функцией двух параметров и регрессоров, и размер предельного эффекта зависит и от $\beta$, и от $x$.

\begin{center}
Общее уравнение регрессии
\end{center}

Для общего уравнения регрессии
\[
\E[y|x]= g(x, \beta),
\]
предельный эффект меняется со значением $x$.

Общепринято представлять одну из оценок предельного эффекта, приведенных в Таблице 5.2. Первая оценка усредняет предельные эффекты для всех индивидов. Вторая оценка вычисляет предельный эффект при $x=\bar{x}$. Третья оценка вычисляет предельный эффект при определённых характеристиках $x=x^*$. Например, $x^*$ может представлять человека, который является женщиной с 12-ью годами обучения в школе и так далее. Может быть рассмотрен более, чем один репрезентативный индивид.

Эти три показателя отличаются в нелинейных моделях, в то время как в линейной модели все они равны $\beta$. Даже знак эффекта может быть не связан со знаком параметра. Значение $\partial \E[y|x] / \partial x_j$ может быть положительным для некоторых значений $x$ и отрицательным для других значений $x$. Значительное внимание должно быть уделено интерпретации коэффициентов в нелинейных моделях.

Компьютерные программы и прикладные исследования часто приводят второй из этих предельных эффектов. Это может быть полезно для того, чтобы определить величину предельного эффекта, но при принятии решений важен общий эффект, то есть первый показатель, или эффект на репрезентативного индивида или группу, то есть третий показатель. Первый показатель имеет тенденцию изменяться относительно мало для различных функциональных форм $g(\cdot)$, в то время как два другие показателя могут изменяться существенно. Можно также представить полное распределение предельных эффектов, используя гистограмму или непараметрическую оценку плотности.

\begin{center}
Одноиндексные модели
\end{center}

Прямая интерпретация коэффициентов регрессии возможна для одноиндексных моделей, которые определяют
\begin{equation}
\E[y|x]= g(x'\beta),
\end{equation}
то есть данные и параметры входят в нелинейную функцию среднего $g(\cdot)$ в виде одного индекса $x'\beta$. Здесь мы видим нелинейность в легкой форме --- ожидание является нелинейной функцией линейной комбинации регрессоров и параметров. Для одноиндексных моделей эффект от изменения в $j$-ом регрессоре получается взятием производной:
\[
\frac{\partial \E[y|x]}{\partial x_j}=g'(x'\beta)\beta_j,
\]
где $g'(z)= \partial g(z)/\partial z$. Из этого следует, что относительный эффект изменений в регрессорах задаётся отношением коэффициентов, поскольку
\[
\frac{\partial \E[y|x] / \partial x_j}{\partial \E[y|x] / \partial x_k}=\frac{\beta_j}{\beta_k},
\]
потому что общий член $g'(x'\beta)$ сокращается. Таким образом, если $\beta_j$ равен удвоенному $\beta_k$, тогда изменение на одну единицу $x_j$ имеет в два раза больший эффект, чем эффект при изменении на одну единицу $x_k$. Если $g(\cdot)$ ещё и монотонна, то из этого следует, что знаки коэффициентов дают знаки эффектов, для всех возможных $x$.

Одноиндексные модели полезны из-за их простой интерпретации. Многие стандартные нелинейные модели, таких как логит, пробит, и тобит имеют одноиндексную форму. Кроме того, некоторые функции $g(\cdot)$ позволяют дать дополнительную интерпретацию, особенно экспоненциальная функция, которая будет рассмотрена далее в этом Разделе, и логистическая функция распределения, проанализированная в Разделе 14.3.4.

\begin{center}
Метод конечных разностей
\end{center}

Мы подчеркнули использование методов математического анализа. Вместо взятия производной метод конечных разностей вычисляет предельный эффект путём сравнения условного среднего, когда $x_j$ увеличивается ровно на одну единицу, со значением до увеличения. Таким образом,
\[
\frac{\Delta \E[y|x]}{ \Delta x_j}=g(x+e_j,\beta)-g(x,\beta),
\]
где $e_j$ --- это вектор, где на $j$-ом месте стоит единица, а на остальных --- нули.

Для линейной модели конечно-разностный метод и взятие производной приводят к тем же оцениваемым эффектам, поскольку $\Delta \E[y|x] / \Delta x_j = (x'\beta + \beta_j)-x'\beta = \beta_j$. Однако для нелинейных моделей эти два подхода дают одинаковые оценки предельного эффекта, только если изменение $x_j$ очень малое.

Дифференциирование часто применяется для непрерывных регрессоров, а конечно-разностный метод используется для целочисленных регрессоров таких, как (0, 1) переменной-индикатора.

\begin{center}
Экспоненциальное условное математическое ожидание
\end{center}

В качестве примера рассмотрим интерпретацию коэффициента функции экспоненциального условного среднего, т.е. $\E[y|x]= \exp(x'\beta)$. Многие счетные модели и модели длительности используют экспоненциальную форму.

Небольшое алгебраическое преобразование приводит к тому, что $\partial \E[y|x] / \partial x_j=\E[y|x]\times \beta_j$. Таким образом, параметры могут быть интерпретированы как полу-эластичности, с изменением $x_j$ на единицу условное математическое ожидание увеличивается в $\beta_j$ раз. Например, если $\beta_j$=0.2, то изменение $x_j$ на единицу, по прогнозам, приведёт к пропорциональному увеличению $\E[y|x]$ на 0.2 раза, или увеличению на 20$\%$.

Если же наоборот используется конечно-разностный метод, то предельный эффект высчитывается как $\Delta \E[y|x] / \Delta x_j = \exp(x'\beta + \beta_j)-\exp(x'\beta) = \exp(x'\beta)(e^{\beta_j}-1)$. Это не отличается от результата, полученного с помощью производной, если только $\beta_j$ настолько малое, что $e^{\beta_j} \backsimeq 1+\beta_j$. Например,  $\beta_j$=0.2, то увеличение составляет 22.14$\%$, а не 20$\%$.

\section{Экстремальные оценки}

Данный раздел предназначен для продвинутого курса по микроэконометрике. В нём представлены основные результаты по состоятельности и асимптотической нормальности экстремальных оценок, очень общего класса оценок, которые минимизируют или максимизируют целевую функцию. Представленные результаты очень сжаты. Более полное понимание требует большего внимания как, например, у Амэмия (1985), где представлены основы, или у Ньюи и МакФаддена (1994).

\subsection{Экстремальные оценки}

Для анализа пространственных данных по одной зависимой переменной, выборка состоит из $N$ наблюдений, 
$\{(y_i,x_i),i=1,\cdots,N\}$, зависимой переменной $y_i$ и вектора-столбца регрессоров $x_i$. 
В матричном виде --- $(y,X)$, где $y$ представляет собой вектор размера $N \times 1$ с $i$-ым значением $y_i$, и $X$ представляет собой матрицу с $i$-ой строкой $x_i$, как это определено более подробно в Разделе 1.6.

Интерес заключается в оценке вектора параметров $\theta=[\theta_1 \cdots \theta_q]'$ размера $q \times 1$. Значением $\theta_0$, называемого истинным значением параметра, является конкретное значение $\theta$ в процессе, порождающем данные.

Мы считаем оценки $\hat{\theta}$, которые максимизируют по $\theta_0 \in \Theta$ стохастическую целевую функцию $\mathcal{Q}_N(\theta)=\mathcal{Q}_N(y,X,\theta)$, где для простоты обозначения зависимость $\mathcal{Q}_N(\theta)$ от данных выражена за счёт нижнего индекса $N$. Такие оценки называются экстремальными оценками, так как они решают максимизационную или минимизационную задачу.

Экстремальная оценка может быть глобальным максимумом, так
\begin{equation}
\hat{\theta}= \argmax _{\theta \in \Theta} \mathcal{Q}_N(\theta)
\end{equation}

Обычно  экстремальная оценка --- локальный максимум, который вычисляется как решение связанных условий первого порядка
\begin{equation}
\left. \frac{\partial \mathcal{Q}_N(\theta)}{\partial \theta} \right|_{\hat{\theta}}=0,
\end{equation}
где $\partial \mathcal{Q}_N(\theta)/ \partial \theta$ --- вектор-столбец размера $q \times 1$, в котором $k$-ый член --- это $\partial \mathcal{Q}_N(\theta)/ \partial \theta_k$. Обратим внимание на локальный максимум, потому что именно локальный максимум может быть распределён асимптотически нормально. Локальный и глобальный максимумы совпадают, если $\mathcal{Q}_N(\theta)$ глобально вогнута.

Есть два основных примера экстремальных оценок. Для М-оценок, рассмотренных в этой главе, в частности, для ММП и НМНК оценок, $\mathcal{Q}_N(\theta)$ является средним арифметическим, например, средним квадратов остатков. В обобщенном методе моментов оценка (см. Раздел 6.3) $\mathcal{Q}_N(\theta)$ является квадратичной формой выборочных средних.

Для определенности речь пойдёт об одном уравнении для пространственной регрессии. Но результаты носят довольно общий характер и могут быть применимы к любым оценкам, основанным на оптимизации со свойствами, приведенными в данном разделе. В частности, нет никаких ограничений на скалярную зависимую переменную и некоторые авторы используют обозначение $z_i$ вместо $(y_i,x_i)$. Тогда $\mathcal{Q}_N(\theta)$ равна $\mathcal{Q}_N(Z,\theta)$, а не $\mathcal{Q}_N(y,X,\theta)$.

\subsection{Формальные теоремы о состоятельности}

Рассмотрим сначала спецификацию параметров, введенную в Разделе 2.5. Интуитивно параметр $\theta$ идентифицируем, если распределение данных или особенность распределения, определяется с помощью $\theta_0$, в то время как любые другие значения $\theta$ приводят к другому распределению. Например, в линейной регрессии мы потребовали, чтобы $\E[y|X]=X\beta_0$ и $X\beta^{(1)}=X\beta^{(2)}$ тогда и только тогда, когда $\beta^{(1)}=\beta^{(2)}$.

Процедура оценки может не идентифицировать $\theta_0$. Например, это имеет место, если процедура оценки пропускает значимые регрессоры. Будем говорить, что метод оценки идентифицирует $\theta_0$, если предел по вероятности целевой функции, взятый согласно процессу порождающему данные с параметром $\theta=\theta_0$, достигает максимума только при $\theta=\theta_0$. Это условие идентификации является асимптотическим. Практические проблемы оценки, которые могут возникнуть в конечной выборке, рассматриваются в Главе 10.

Состоятельность определяется следующим образом. При $ N \rightarrow \infty $ стохастическая целевая функция $\mathcal{Q}_N(\theta)$, средняя в случае М-оценки, может сходиться по вероятности к предельной функции, обозначаемой $\mathcal{Q}_0(\theta)$, которая в простейшем случае не является стохастической. Соответствующие максимумы (глобальной или локальной) из $\mathcal{Q}_N(\theta)$ и $\mathcal{Q}_0(\theta)$ тогда должны существовать для значений $\theta$, близких друг к другу. Поскольку максимум $\mathcal{Q}_N(\theta)$ --- это $\hat{\theta}$ по определению, то отсюда следует, что $\hat{\theta}$ сходится по вероятности к $\theta_0$ при условии, что $\theta_0$ максимизирует $\mathcal{Q}_0(\theta)$.

Очевидно, что состоятельность и идентифицируемость тесно связаны, и у Амэмия (1985, с. 230) говорится, что наиболее простой подход заключается в том, чтобы под идентифицируемостью понимать существование состоятельной оценки. Для дальнейшего обсуждения см. Ньюи и МакФаддена (1994, с. 2124) и Дейстлера и Зейферта (1978).

Основные области применения этого подхода можно найти в работах Дженриха (1969) и Амэмия (1973). Амэмия (1985) и Ньюи и МакФадден (1994) представляют достаточно общие теоремы. Эти теоремы используют разные предпосылки, в том числе о гладкости (непрерывности) и существовании необходимых производных целевой функции, предпосылки о процессе порождающем данные для обеспечения сходимости $\mathcal{Q}_N(\theta)$  к $\mathcal{Q}_0(\theta)$, и максимальном значении $\mathcal{Q}_0(\theta)$  при $\theta=\theta_0$. Разные теоремы о состоятельности используют различные предпосылки.

Приведём две теоремы о состоятельности из Амэмия (1985), одну для глобального максимума и одну для локального максимума. Обозначения в теоремах Амэмия были изменены, поскольку он (1985) определяет целевую функцию без нормирования $1/N$, которая присутствует, например, в (5.4).

\begin{theorem}[Состоятельность глобального максимума] (Амэмия, 1985, теорема 4.1.1): 
Сделаем следующие предположения:
\begin{enumerate}
\item Множество возможных значений параметра $\Theta$ --- компактное подмножество в $R^q$.
\item Целевая функция $\mathcal{Q}_N(\theta)$ является измеримой функцией от данных для всех $\theta \in \Theta$, и $\mathcal{Q}_N(\theta)$ непрерывна при $\theta \in \Theta$.
\item $\mathcal{Q}_N(\theta)$ равномерно сходится по вероятности к нестохастический функции $\mathcal{Q}_0(\theta)$, и $\mathcal{Q}_0(\theta)$ достигает единственного глобального максимума в $\theta_0$.
\end{enumerate}

Тогда оценка $\hat{\theta}= \arg\max _{\theta \in \Theta} \mathcal{Q}_N(\theta)$ состоятельна для $\theta_0$, то есть $ \hat{\theta} \xrightarrow{p} \theta_0$.
\end{theorem}


Равномерная сходимость по вероятности $\mathcal{Q}_N(\theta)$ к 
\begin{equation}
\mathcal{Q}_0(\theta)=\plim \mathcal{Q}_N(\theta)
\end{equation}
в третьем условии означает, что $sup _{\theta \in \Theta} |\mathcal{Q}_N(\theta)-\mathcal{Q}_0(\theta)| \xrightarrow{p} 0$.

Для локального максимума должны существовать первые производные, но затем нужно рассматривать поведение $\mathcal{Q}_N(\theta)$ и её производной в окрестности $\theta_0$.

\begin{theorem}[Состоятельность локального максимума] (Амэмия, 1985, теорема 4.1.2): 
Сделаем следующие предположения:
\begin{enumerate}
\item Множество возможных значений параметра $\Theta$ --- открытое подмножество $R^q$.
\item $\mathcal{Q}_N(\theta)$ является измеримой функцией от данных для всех $\theta \in \Theta$, и  $\partial \mathcal{Q}_N(\theta) / \partial \theta $ существует и непрерывна в открытой окрестности $\theta_0$.
\item  Целевая функция $\mathcal{Q}_N(\theta)$ равномерно сходится по вероятности к $\mathcal{Q}_0(\theta)$ в открытой окрестности $\theta_0$, и  $\mathcal{Q}_0(\theta)$  достигает единственного локального максимума в $\theta_0$.
\end{enumerate}

Тогда одно из решений $\partial \mathcal{Q}_N(\theta) / \partial \theta =0 $ состоятельно для $\theta_0$. 
\end{theorem}

Пример применения теоремы 5.2 приведён далее в Разделе 5.3.4.

Условие 1 в теореме 5.1 позволяет глобальному максимуму быть на границе множества значений параметров, в то время как в теореме 5.2 локальный максимум должен быть строго внутренней точка множества возможных значений параметров. Условие 2 в теореме 5.2 также подразумевает непрерывность $\mathcal{Q}_N(\theta)$ в открытой окрестности $\theta_0$. Окрестность $N(\theta_0)$ $\theta_0$ открыта тогда и только тогда, когда существует шар с центром $\theta_0$, которая целиком содержится в $N(\theta_0)$ $\theta_0$. В обеих теоремах условие 3 является необходимым. Максимум, глобальный или локальный,  $\mathcal{Q}_0(\theta)$ должен достигаться при $\theta=\theta_0$. Вторая часть условия 3 --- это условие идентифицируемости $\theta_0$: это значение должно иметь  содержательную интерпретацию и быть единственным.

Для локального максимума анализ прост, если существует только один локальный максимум. Тогда 
$\hat{\theta}$ определяется единственным образом как $\partial \mathcal{Q}_N(\theta) / \partial \theta|_{\hat{\theta}}=0$. Когда существует больше одного локального максимума, теорема говорит, что один из локальных максимумов состоятельный, но не даёт указаний, какой конкретно является состоятельным. Лучше всего в таких случаях рассматривать глобальный максимум и применять теорему 5.1. См. Ньюи и МакФаддена (1994, с. 2117) для обсуждения.

Нужно обратить внимание на разницу между спецификацией модели, отражающееся в выборе целевой функции $\mathcal{Q}_N(\theta)$ и фактическим процессом порождающим данные $(y,X)$, используемом при получении $\mathcal{Q}_0(\theta)$ в (5.14). Для некоторых процессов порождающих данные оценка может быть состоятельной, тогда как для других --- нет. В некоторых случаях таких, как для оценок ММП Пуассона и МНК, состоятельность возникает в широком классе процессов порождающих данные при условии, что правильно специфицировано условное математическое ожидание. В других случаях состоятельность требует более сильных предположений о процессе порождающем данные, например, правильную спецификацию плотности.

\subsection{Асимптотическая нормальность}

Результаты по асимптотической нормальности, как правило, ограничиваются локальным максимумом $\mathcal{Q}_N(\theta)$. Оценка $\hat{\theta}$ является решением (5.13), которое в общем случае нелинейно по $\hat{\theta}$ и не имеет явного решение для $\hat{\theta}$. Вместо этого, мы заменим левую часть этого уравнения линейной функцией от $\hat{\theta}$ путём использования разложения в ряд Тейлора, а затем решим его для $\hat{\theta}$.

Наиболее часто используемый вариант теоремы Тейлора --- это приближение с остаточным членом. Здесь мы вместо этого рассмотрим точное разложение Тейлора первого порядка. Для дифференцируемой функции $f(\cdot)$ всегда существует точка $x^+$ между $x$ и $x_0$ такая, что
\[
f(x)=f(x_0)+f'(x^+)(x-x_0),
\]
где $f'(x)=\partial f(x) / \partial x$ --- производная $f(x)$. Это результат также известен как теорема о среднем значении.

Применение к текущей задаче требует ряда изменений. Скалярная функция $f(\cdot)$ заменяется на векторную функции $\mathfrak{f}(x)$, и скалярные аргументы $x$, $x_0$ и $x^+$ заменяются векторами $\hat{\theta}$, $\theta_0$ и $\theta^+$. Тогда 
\begin{equation}
\left. \mathfrak{f}(\hat{\theta})=\mathfrak{f}(\theta_0)+\frac{\partial \mathfrak{f}(\theta)}{\partial \theta} \right|_{\theta^+} (\hat{\theta}-\theta_0),
\end{equation}
где $ \partial \mathfrak{f}(\theta)/ \partial \theta$ является матрицей для некоторого $\theta^+$ между $\hat{\theta}$ и $\theta_0$, и формально $\theta^+$ отличается для каждой строки этой матрицы (см. Ньюи и МакФадден, 1994, с. 2141). Для оценки локального экстремума функция $\mathfrak{f}(\theta)= \partial \mathcal{Q}_N(\theta) / \partial \theta$ --- это уже первая производная. Тогда точное разложение в ряд Тейлора первого порядка в окрестности $\theta_0$:
\begin{equation}
\left. \frac{\partial \mathcal{Q}_N(\theta)}{\partial \theta} \right|_{\hat{\theta}} = \left. \frac{\partial \mathcal{Q}_N(\theta)}{\partial \theta} \right|_{\theta_0} + \left. \frac{\partial^2 \mathcal{Q}_N(\theta)}{\partial \theta \partial \theta'} \right|_{\theta^+} (\hat{\theta}-\theta_0),
\end{equation}
где $\partial^2 \mathcal{Q}_N(\theta) / \partial \theta \partial \theta'$ --- это матрица размера $q \times q$ c $(j,k)$-ым элементом равным $\partial^2 \mathcal{Q}_N(\theta) / \partial \theta_j \partial \theta_k$, а $\theta^+$ лежит между $\hat{\theta}$ и $\theta_0$.

Условие первого порядка подразумевает, что левая часть (5.16) приравнивается к 0, тогда правая часть тоже приравнивается к 0 и решение для $(\hat{\theta}-\theta_0)$ следующее:
\begin{equation}
\left. \sqrt{N}(\hat{\theta} - \theta_0)=- \left( \frac{\partial^2 \mathcal{Q}_N(\theta)}{\partial \theta \partial \theta'} \right|_{\theta^+} \right) ^{-1} \sqrt{N} \left. \frac{\partial \mathcal{Q}_N(\theta)} {\partial \theta} \right|_{\theta_0},
\end{equation}
где мы нормируем на $\sqrt{N}$, чтобы гарантировать невырожденное предельное распределение (что обсуждается далее).

Результат (5.17) содержит решение для $\hat{\theta}$. Это не нужно для численного нахождения $\hat{\theta}$, поскольку решение зависит от $\theta_0$ и $\theta^+$, оба из которых неизвестны, но это полезно для теоретического анализа. В частности, если было установлено, что $\hat{\theta}$ состоятельна для $\theta_0$, тогда неизвестная $\theta^+$ сходится по вероятности к $\theta_0$, потому что лежит между $\hat{\theta}$ и $\theta_0$, а из-за состоятельности $\hat{\theta}$ сходится по вероятности к $\theta_0$. 

Из результата (5.17) $\sqrt{N}(\hat{\theta} - \theta_0)$ выражается в виде, аналогичном тому, который используется для получения предельного распределения МНК-оценки (см. Раздел 5.2.3). Всё, что нам нужно сделать, --- это предположить, что существуют предел по вероятности для первого члена в правой части (5.17) и предельное нормальное распределение для второго члена.

Это приводит к теореме из Амэмия (1985) об экстремальной оценке, удовлетворяющей локальному максимуму. Снова отметим, что Амэмия (1985) определяет целевую функцию без нормирования $1/N$. Кроме того, Амэмия определяет $A_0$ и $B_0$ как $\lim E$, а не $\plim$.

\begin{theorem}[Предельное распределение локального максимума] (Амэмия, 1985, теорема 4.1.3): В дополнение к условиям предыдущей теоремы о состоятельности локального максимума сделаем следующие предположения:
\begin{enumerate}
\item $\partial^2 \mathcal{Q}_N(\theta) / \partial \theta \partial \theta'$ существует и непрерывна в открытой выпуклой окрестности $\theta_0$.
\item $\partial^2 \mathcal{Q}_N(\theta) / \partial \theta \partial \theta'|_{\theta^+}$ сходится по вероятности к конечной невырожденной матрице
\begin{equation}
A_0=\plim \partial^2 \mathcal{Q}_N(\theta) / \partial \theta \partial \theta'|_{\theta_0}
\end{equation}
для любой последовательности $\theta^+$ такой, как $\theta^+ \xrightarrow{p} \theta_0$.
\item $\sqrt{N} \partial \mathcal{Q}_N(\theta) / \partial \theta |_{\theta_0} \xrightarrow{d} \mathcal{N}[0,B_0]$, где
\begin{equation}
B_0=\plim[N \partial \mathcal{Q}_N(\theta) / \partial \theta \times \partial \mathcal{Q}_N(\theta) / \partial \theta'|_{\theta_0}].
\end{equation}
\end{enumerate}
Тогда предельное распределение экстремальной оценки
\begin{equation}
\sqrt{N}(\hat{\theta}-\theta_0) \xrightarrow{d} \mathcal{N}[0,A_0^{-1}B_0A_0^{-1}],
\end{equation}
где оценка $\hat{\theta}$ --- состоятельное решение $\partial \mathcal{Q}_N(\theta) / \partial \theta=0$.
\end{theorem}

Доказательство непосредственно вытекает из теоремы о нормальности предела произведения (теорема А.17), применяемого к (5.17). Отметим, что доказательство предполагает, что состоятельность $\hat{\theta}$ уже была установлена. Выражения для $A_0$ и $B_0$, приведённых в таблице 5.1, --- уточнения для случая $\mathcal{Q}_N(\theta)=N^{-1} \sum_i q_i(\theta)$ при условии независимости по $i$.

Пределы по вероятности в (5.18) и (5.19) получены с учетом процесса порождающего данные для $(y,X)$. В некоторых приложениях предполагается, что регрессоры нестохастические и математическое ожидание берется только по $y$. В других случаях рассматриваются стохастические регрессоры и математическое ожидание берется и по $y$, и по $X$.

\subsection{Пример с асимптотическими свойствами ММП-оценки Пуассона}

Мы формально доказываем состоятельность и асимптотическую нормальность  ММП-оценки Пуассона при экзогенной стратифицированной выборке со стохастическими регрессорами, где $(y_i,x_i)$ независимы но неодинаково распределены. При этом необязательно предполагать, что $y_i$ распределены по Пуассону.

Ключевыми моментами для доказательства состоятельности являются получение $\mathcal{Q}_0(\beta)=\plim \mathcal{Q}_N(\beta)$ и проверка того, что $\mathcal{Q}_0(\beta)$ достигает максимума в точке $\beta=\beta_0$. Для $\mathcal{Q}_N(\beta)$, которая была определена в (5.1), получается
\[
\mathcal{Q}_0(\beta)= \plim N^{-1} \sum_i \{-e^{x'_i\beta}+y_i x'_i \beta - \ln y_i! \}
\]
\[
= \lim N^{-1} \sum_i \{-\E[e^{x'_i\beta}]+\E[y_i x'_i \beta] - \E[\ln y_i!] \} \\
\]
\[
= \lim N^{-1} \sum_i \{-\E[e^{x'_i\beta}]+\E[e^{x'_i \beta_0} x'_i \beta] - \E[\ln y_i!] \}
\]

Второе равенство предполагает, что закон больших чисел может быть применен к каждому члену. Так как $(y_i,x_i)$ независимы и неодинаково распределены, закон больших чисел Маркова (теорема A.8) может быть применён, если каждая из ожидаемых величин, приведённых во второй строке, дополнительно существует и соответствующий $(1+\delta)$-й абсолютный момент существует для некоторого $\delta>0$ и граничное условие в теореме А.8 выполнено. Например, допустим, что $\delta=1$, так чтобы были применены моменты второго порядка. Для третьей строки требуется предположение, что процесс порождающий данные является таким, что $\E[y|x]=\exp(x'\beta_0)$. Первые два математических ожидания в третьей строке зависят от $x$, который является стохастическим. Обратите внимание, что $\mathcal{Q}_0(\beta)$ зависит и от $\beta$, и от $\beta_0$. Дифференцируя по $\beta$, и предполагая, что пределы, производные и математические ожидания могут быть поменены местами, мы получаем
\[
\frac{\partial \mathcal{Q}_0(\beta)}{\partial \beta}= -\lim N^{-1} \sum_i \E[e^{x'_i \beta} x_i] + \lim N^{-1} \sum_i \E[e^{x'_i \beta_0} x_i],
\]
где производная $\E[\ln y!]$ по $\beta$ равна 0, поскольку $\E[\ln y!]$ зависит от $\beta_0$, истинного значения параметра в процессе порождающем данные, но не от $\beta$. Очевидно, что $\partial \mathcal{Q}_0(\beta)/ \partial \beta=0$ при $\beta=\beta_0$ и $\partial^2 \mathcal{Q}_0(\beta) / \partial \beta \partial \beta' = -\lim N^{-1} \sum_i \E[e^{x'_i \beta} x_i x'_i]$ отрицательно определена, поэтому $\mathcal{Q}_0(\beta)$ достигает локального максимума при $\beta=\beta_0$, и ММП-оценка Пуассона состоятельна по теореме 5.2. Так как здесь $\mathcal{Q}_N(\beta)$ глобально вогнута, то локальный максимум совпадает с глобальным максимумом, и состоятельность также может быть показана с использованием теоремы 5.1.

Перейдем к асимптотической нормальности ММП-оценки Пуассона. Точное  разложение Тейлора до первого члена условия первого порядка ММП-оценки Пуассона (5.3) даёт:

\begin{equation}
\sqrt{N}(\hat{\beta}-\beta_0)=-\left[-N^{-1} \sum_i e^{x'_i \beta^+} x_i x'_i \right]^{-1} N^{-1/2} \sum_i (y_i-e^{x'_i \beta_0}) x_i,
\end{equation}
для некоторой неизвестной $\beta^+$, лежащей в промежутке между $\hat{\beta}$ и $\beta_0$.

Сделав достаточные предположения о регрессорах $x$, чтобы было можно применить закон больших Маркова к первому члену, и, используя $\beta^+ \xrightarrow{p} \beta_0$, поскольку $\hat{\beta} \xrightarrow{p} \beta_0$, получаем
\begin{equation}
-N^{-1} \sum_i e^{x'_i \beta^+} x_i x'_i \xrightarrow{p} A_0 = -\lim N^{-1} \sum_i \E[e^{x'_i \beta_0} x_i x'_i].
\end{equation}
Для второго члена в (5.21) начнём с предположения о скалярном регрессоре $x$. Тогда у $X=(y-\exp(x \beta_0))x$ математическое ожидание $\E[X]=0$, поскольку для состоятельности было уже введено предположение о $\E[y|x]=\exp(x \beta_0)$, и дисперсия $\Var[X]=\E[\Var[y|x]x^2]$. Центральная предельная теорема Ляпунова (теорема A.15) может быть применена, если граничное условие, включающее абсолютный момент $y-\exp(x \beta_0)x$ порядка $(2+\delta)$, выполнено. В данном примере с $y>0$ достаточно предположить, что момент третьего порядка для $y$ существует, то есть $\delta=1$ и $x$ ограничен. Применение ЦПТ даёт следующее:
\[
Z_N=\frac{\sum_i (y_i-e^{\beta_0 x_i})x_i}{\sqrt{\sum_i \E[\Var[y_i|x_i]x_i^2]}} \xrightarrow{d} \mathcal{N}[0,1],
\]
поэтому 
\[
N^{-1/2} \sum_i (y_i-e^{\beta_0 x_i})x_i \xrightarrow{d} \mathcal{N}\left[0,\lim N^{-1}\sum_i \E[\Var[y_i|x_i]x_i^2]\right],
\]
предполагая, что в выражении предел асимптотической дисперсии существует. Этот результат может быть распространен на случай вектора регрессоров, используя теорему Крамера-Вольда (см. теорему A.16). Тогда
\begin{equation}
N^{-1/2} \sum_i (y_i-e^{x'_i \beta_0})x_i \xrightarrow{d} \mathcal{N} \left[0,B_0= \lim N^{-1} \E[\Var[y_i|x_i]x_i x'_i]\right].
\end{equation}

Таким образом из (5.21) следует, что $\sqrt{N}(\hat{\beta}-\beta_0) \xrightarrow{d} \mathcal{N}[0,A_0^{-1}B _0 A_0^{-1}]$, где $A_0$ было определено в (5.22), а $B_0$ в (5.23).

Отметим, что для этого конкретного примера нет необходимости в том, чтобы $y|x$ была распределена по Пуассону для того, чтобы ММП-оценка Пуассона была состоятельна и асимптотически нормальна. Существенным предположением для состоятельности ММП-оценки Пуассона является то, что процесс порождающий данные таков, что $\E[y|x]=\exp(x' \beta_0)$.

Для асимптотической нормальности существенным предположением является то, что $\Var[y|x]$ существует, хотя дополнительные предположения о существовании моментов высших порядков необходимы, чтобы было можно применять ЗБЧ и ЦПТ. Если на самом деле $\Var[y|x]=\exp(x' \beta_0)$, то $A_0=-B_0$ и проще $\sqrt{N}(\hat{\beta}-\beta_0) \xrightarrow{d} \mathcal{N}[0,-A_0^{-1}]$. Результаты этого примера ММП распространяются на экспоненциальное семейство распределений, которое определены в Разделе 5.7.3.

\subsection{Доказательства состоятельности и асимптотической нормальности}

Предположения, сделанные в теоремах 5.1-5.3, носят довольно общий характер и не обязательно должны присутствовать при каждом применении. Эти предположения должны быть проверены в каждом конкретном случае на индивидуальной основе, по аналогии с предыдущим примером ММП-оценки Пуассона. Здесь мы подробно остановимся на М-оценках.

Для состоятельности ключевым шагом является получение предела по вероятности $\mathcal{Q}_N(\theta)$. Это делается путем применения ЗБЧ, потому что для М-оценки $\mathcal{Q}_N(\theta)$ является средним $N^{-1} \sum_i q_i(\theta)$. Различные предположения о процессе порождающем данные ведут к использованию различных вариантов ЗБЧ и к существенным различиям в выражении $\mathcal{Q}_0(\theta)$.

Асимптотическая нормальность требует предположений о процессе порождающем данные в дополнение к тем, которые необходимы для состоятельности. В частности, нам необходимы предположения о процессе порождающем данные, чтобы обеспечить применение ЗБЧ для получения $A_0$ и применение ЦПТ для получения $B_0$.

Для М-оценки ЗБЧ скорее всего удовлетворяет условию 2 теоремы 5.3, поскольку каждый элемент матрицы $\partial^2 \mathcal{Q}_N(\theta) / \partial \theta \partial \theta'$ является средним, так как $\mathcal{Q}_N(\theta)$ является средним. Из ЦПТ может следовать условие 3 теоремы 5.3, так как у $\sqrt{N} \partial \mathcal{Q}_N(\theta) / \partial \theta|_{\theta_0}$ математическое ожидание равно нулю из неформального условия состоятельности (5.24) в Разделе 5.3.7 и конечная дисперсия $\E[N \partial \mathcal{Q}_N(\theta) / \partial \theta \times \partial \mathcal{Q}_N(\theta) / \partial \theta'|_{\theta_0}]$.

Частные случаи ЦПТ и ЗБЧ, которые используются для получения предельного распределения оценки, меняются в зависимости от предположений о процессе порождающем данные для $(y|X)$. Во всех случаях зависимая переменная является стохастической. Тем не менее, регрессоры могут быть детерминированными или стохастическими, и в последнем случае они могут иметь зависимость во времени. Эти вопросы уже рассматривались для МНК в Разделе 4.4.7.

Частое предположение в микроэконометрике заключается в том, что регрессоры стохастические и  наблюдения независимы, что разумно для пространственных данных из обследований населения. Для простой случайной выборки данные $(y_i,x_i)$ независимы и одинаково распределены, и ЗБЧ Колмогорова и ЦПТ Линдеберга-Леви (теоремы А.8 и А.14) могут быть применены. Более того, согласно простой случайной выборке (5.18) и (5.19) упростим 
\[
A_0=\E \left[  \left. \frac{\partial^2 q(y,x,\theta)}{\partial \theta \partial \theta'} \right|_{\theta_0} \right]
\]
и
\[
B_0=\E \left[ \left. \frac{\partial q(y,x,\theta)}{\partial \theta} \frac{\partial q(y,x,\theta)}{\partial \theta'} \right|_{\theta_0} \right],
\]
где $(y,x)$ обозначает одно наблюдение и математические ожидания зависят от совместного распределения $(y,x)$. Это простое обозначение используется в нескольких текстах.

Для стратифицированной случайной выборки и для фиксированных регрессоров данные $(y_i,x_i)$  являются независимыми и неодинаково распределенными, поэтому необходимо использовать ЗБЧ Маркова и ЦПТ Ляпунова (теоремы A.9 и A.15). Они требуют дополнительных предположений о моментах к тем, которые применяются в случае независимо и одинаково распределённых данных. В случае стохастических регрессоров математические ожидания зависят от совместного распределения $(y,x)$, тогда как в случае детерминированных регрессоров, например, в контролируемом эксперименте, где уровень $x$ может быть задан, математические ожидания в (5.18) и (5.19) зависят только от $y$.

Для временных рядов предполагается, что регрессоры стохастические, но также предполагается зависимость разных наблюдений, что является необходимой основой для использования лаговых зависимых переменных. Гамильтон (1994) фокусируется на этом случае, который также был изучен Уайтом (2001a). Самая простая процедура предусматривает, что случайные величины $(y,x)$ имеют стационарное распределение. Если вместо этого данные нестационарные и есть единичный корень, тогда скорость сходимости больше не может быть $\sqrt{N}$ и предельное распределение может быть ненормальным.

Однако, несмотря на эти важные концептуальные и теоретические разногласия по поводу стохастического характера $(y,x)$, для пространственной регрессии вариация предельной теоремы, как правило, приводится в общей форме, описанной в теореме 5.3.

\subsection{Обсуждение}

Форма ковариационной матрицы (5.20) называется сэндвич-формой с $B_0$, помещённым между $A_0^{-1}$ и $A_0^{-1}$. Сэндвич-форма, введённая в Разделе 4.4.4, будет рассмотрена более подробно в Разделе 5.5.2.

Асимптотические результаты можно распространить на несостоятельные оценки. Тогда $\theta_0$ будет заменён на псевдо-истинное значение $\theta^*$, которое определяется как такое значение $\theta_0$, которое даёт локальный максимум $\mathcal{Q}_0(\theta)$. Это рассматривается более подробно для оценки квази-ММП в Разделе 5.7.1. В большинстве случаев, однако, оценка состоятельна, и в последующих главах индекс 0 часто опускается для более простого обозначения.

В предыдущих результатах целевая функция $\mathcal{Q}_N(\theta)$ первоначально определяется с нормированием на $1/N$, тогда первая производная $\mathcal{Q}_N(\theta)$ нормируется на $\sqrt{N}$, а вторая производная не нормируется, что приводит к $\sqrt{N}$-состоятельной оценке. В некоторых случаях могут быть необходимы альтернативные способы нормирования, в первую очередь, для временных рядов с нестационарным трендом.

Результаты предполагают, что $\mathcal{Q}_N(\theta)$ является непрерывной дифференцируемой функцией. Это исключает некоторые оценки, такие, как оценку методом наименьших абсолютных значений, для которой  $\mathcal{Q}_N(\theta)=N^{-1} \sum_i |y_i-x'_i \beta|$. Один из способов продолжить в этом случае --- получение дифференцируемой апроксимирующей функции  $\mathcal{Q^*}_N(\theta)$ такой, что  $\mathcal{Q^*}_N(\theta)-\mathcal{Q}_N(\theta) \xrightarrow{p} 0$, и тогда применить предыдущую теорему к  $\mathcal{Q^*}_N(\theta)$.

Ключевым компонентом для получения предельного распределения является линеаризация с использованием разложения в ряд Тейлора. Разложение в ряд Тейлора может быть не очень хорошей глобальной аппроксимацией функции. Оно хорошо работает в нашем статистическом приложении, поскольку аппроксимация асимптотически локальна, так как состоятельность предполагает, что при больших выборках $\hat{\theta}$ близка к разложению в точке $\theta_0$. Более точную асимптотическую теорию можно построить при использовании разложение Эджуорта (см. Раздел 11.4.3 ). Бутстреп (см. Главу 11) является методом для реализации разложения Эджуорта на практике.

\subsection{Неформальный подход к состоятельности М-оценки}

На практике предельную нормальность из теоремы 5.3 гораздо легче доказать, чем состоятельность, используя теорему 5.1 или 5.2. Здесь мы представляем неформальный подход к определению характера и силы предположений о распределении, необходимых для того, чтобы М-оценка была состоятельной.

Для М-оценки, которая является локальным максимумом, из условия первого порядка (5.4) следует, что $\hat{\theta}$ выбирается таким образом, чтобы средняя $\partial q_i(\theta) / \partial \theta |_{\hat{\theta}}$ равнялась нулю. Интуитивно, необходимым условием для этого, чтобы получить состоятельную оценку для $\theta_0$ является то, что в пределе математическое ожидание $\partial q(\theta) / \partial \theta |_{\theta_0}$ стремится к 0, или 
\begin{equation}
\left. \plim \frac{\partial \mathcal{Q}_N(\theta)} {\partial \theta} \right|_{\theta_0}= \left. \lim \frac{1}{N} \sum_{i=1}^{N} \E \left[ \frac{\partial q_i(\theta)}{\partial \theta} \right|_{\theta_0} \right] =0,
\end{equation}
где первое равенство требует предположения, что закон больших чисел может быть применён и математическое ожидание в (5.24) зависит от процесса порождающего данные для $(y,X)$. Предел используется, поскольку равенство не обязательно должно быть точным при условии, что любое отклонение от нуля исчезает при $N \rightarrow \infty$. Например, состоятельность должна иметь место, если математическое ожидание равно $1/N$.

Условие (5.24) представляет собой весьма полезную проверку для практика. Неформальный подход к состоятельности заключается в том, чтобы посмотреть на условие первого порядка для оценки $\hat{\theta}$ и определить, равно ли математическое ожидание в пределе нулю, когда $\theta=\theta_0$.

Еще менее формально, если мы рассмотрим компоненты в сумме, существенным условием для состоятельности является следующее равенство для типичного наблюдения
\begin{equation}
\E[\partial q(\theta)/ \partial \theta|_{\theta_0}]=0.
\end{equation}

Это условие может оказаться очень полезным для практика. Тем не менее, оно не является ни необходимым, ни достаточным условием. Если математическое ожидание в (5.25) равно $1/N$, то всё ещё возможно, что предел по вероятности в (5.24) равен нулю, поэтому условие (5.25) не является необходимым. Чтобы увидеть, что этого недостаточно, предположим, что $y$ независимы и одинаковы распределены с математическим ожиданием $\mu_0$ и оценены с использованием только одного наблюдения, допустим, первого наблюдения $y_1$. Тогда $\hat{\mu}$ --- решение $y_1-\mu=0$ и (5.25) выполнено. Но ясно, что $y_1 \not \xrightarrow{p} \mu_0$, поскольку у одного наблюдения $y_1$ дисперсия не стремится к нулю. Проблема в том, что здесь $\plim$ в (5.24) не равен $\lim E$. При формальном доказательстве несостоятельности необходимо применять такие теоремы, как теоремы 5.1 или 5.2.

Для регрессии Пуассона применение (5.25) показывает, что необходимым условием для состоятельности является правильная спецификации условного среднего $ (y|x) $ (см. Раздел 5.2.3). Похожим образом, оценка МНК является решением $N^{-1} \sum_i x_i (y_i-x'_i \beta)=0$, поэтому согласно (5.25) условие состоятельности требует, чтобы $\E[x(y-x'\beta_0)]=0$. Это условие не выполняется, если $\E[y|x] \not= x'\beta_0$, что может произойти по многим причинам, как указано в Разделе 4.7. В других примерах использование (5.25) может указать на то, что состоятельность будет требовать значительно большего числа параметрических предположений, чем правильная спецификации условного математического ожидания.
 
Чтобы связать использование (5.24) с условием 3 в теореме 5.2, обратим внимание на следующее:
\[
\begin{matrix}
\partial \mathcal{Q}_0(\theta)/ \partial \theta =0 & \text{(условие 3 в теореме 5.2)} \\
\Longrightarrow \partial(\plim (\mathcal{Q}_N(\theta))/ \partial \theta =0 & \text{(из определения} \mathcal{Q}_0(\theta)) \\
\Longrightarrow \partial(\lim \E[\mathcal{Q}_N(\theta)])/ \partial \theta =0 & \text{ЗБЧ} \Longrightarrow \mathcal{Q}_0=\plim \mathcal{Q}_N = \lim \E[\mathcal{Q}_N]) \\
\Longrightarrow \lim  \partial \E[\mathcal{Q}_N(\theta)]/ \partial \theta =0 & \text{(поменяем местами предел и дифференциал) и } \\
\Longrightarrow \lim \E[ \partial \mathcal{Q}_N(\theta)/ \partial \theta ]=0 & \text{(поменяем местами дифференциал и математическое ожидание).}
\end{matrix}
\]

Последняя строка является неформальным условием (5.24). Однако получение этого результата требует дополнительных предположений, в том числе ограничения на локальный максимум, применения закона больших чисел, права менять местами предел и производную, права менять местами производную и математическое ожидание (например, интегрирование). В скалярном случае достаточным условием для того, чтобы поменять производную и предел местами, является то, чтобы $\lim_{h \rightarrow 0} (\E[\mathcal{Q}_N (\theta+h)]-\E[\mathcal{Q}_N(\theta)])/h=d\E[\mathcal{Q}_N(\theta)]/d\theta$ равномерно в $\theta$.

\section{Оценочные уравнения}

Вывод предельного распределения, который приведён в Разделе 5.3.3, может быть расширен от локальной экстремальной оценки на оценки, которые определяются как решение оценочного уравнения, в котором среднее приравнивается к нулю. Несколько примеров приведено в Главе 6.

\subsection{Оценка методом оценочных уравнений}

Пусть $\hat{\theta}$ --- это решение системы из $q$ оценочных уравнений
\begin{equation}
h_N(\hat{\theta})=\frac{1}{N} \sum_{i=1}^{N} h(y_i,x_i,\hat{\theta})=0,
\end{equation}
где $h(\cdot)$ --- вектор размера $q \times 1$, к тому же предполагается независимость по $i$. Примеры $h(\cdot)$ приведены далее в Разделе 5.4.2.

Поскольку $\hat{\theta}$ выбрано так, чтобы выборочное среднее $h(y,x,\hat{\theta})$ было равно нулю, мы ожидаем, что $\hat{\theta} \xrightarrow{p} \theta_0$, если в пределе среднее $\hat{\theta} \xrightarrow{p} \theta_0$ стремится к нулю, то есть, если $\plim h_N(\theta_0)=0$. Если, ЗБЧ может быть применён, необходимо, чтобы $\lim \E[h_N(\theta_0)]=0$, или, проще говоря, что для $i$-ого наблюдения
\begin{equation}
\E[h(y_i,x_i,\theta_0)]=0.
\end{equation}
Самый простой способ, чтобы формально установить состоятельность, заключается в выводе (5.26) в качестве условия первого порядка для М-оценки.

Предполагая состоятельность, предельное распределение оценки оценочных уравнений может быть получено таким же образом, как и в Разделе 5.3.3 для экстремальной оценки. Возьмём точное разложение в ряд Тейлора первого порядка $h_N(\theta_0)$ в окрестности $\theta_0$, как и в (5.15) с $\mathfrak{f}(\theta)=h_N(\theta_0)$, и приравняем правую сторону к 0 и решим. Тогда
\begin{equation}
\left. \sqrt{N}(\hat{\theta}-\theta_0)=- \left( \frac{\partial h_N(\theta_0)}{\partial \theta'} \right|_{\theta^{+}} \right) ^{-1} \sqrt{N} h_N(\theta_0).
\end{equation}
Это приводит к следующей теореме.

\begin{theorem}[Предельное распределение оценок оценочных уравнений]:
Предположим, что оценка оценочных уравнений, которая является решением (5.26), состоятельная для $\theta_0$, и сделаем следующие предположения:
\begin{enumerate}
\item $\partial h_N(\theta)/ \partial \theta'$ существует и непрерывна в открытой выпуклой окрестности $\theta_0$. 
\item $\partial h_N(\theta)/ \partial \theta'|_{\theta^+}$ сходится по вероятности к конечной невырожденной матрице
\begin{equation}
A_0= \left. \plim \frac{\partial h_N(\theta)}{\partial \theta'} \right|_{\theta_0}=\plim \frac{1}{N} \sum_{i=1}^{N} \left. \frac{\partial h_i(\theta)}{\partial \theta'} \right|_{\theta_0},
\end{equation}
для любой последовательности $\theta^+$ такой, что $\theta^+ \xrightarrow{p} \theta_0$.
\item $\sqrt{N} h_N(\theta_0) \xrightarrow{d} \mathcal{N}[0,B_0]$, где
\begin{equation}
B_0= \plim N h_N(\theta_0) h_N(\theta_0)'=\plim \frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{N} h_i(\theta_0) h_j(\theta_0)'.
\end{equation}
\end{enumerate}

Тогда предельное распределение оценки оценочных уравнений это
\begin{equation}
\sqrt{N}(\hat{\theta}-\theta_0) \xrightarrow{d} \mathcal{N}[0,{A_0}^{-1} B_0 {A'_0}^{-1}], 
\end{equation}
где, в отличие от экстремальной оценки, матрица $A_0$ может быть несимметричной, поскольку это уже необязательно матрица Гессе.
\end{theorem}

Эта теорема может быть доказана путём адаптации доказательства Амэмия теоремы 5.3. Отметим, что теорема 5.4 предполагает, что состоятельность уже была установлена.

Годамбе (1960) показал, что для анализа, зависящего от регрессоров, наиболее эффективная оценка оценочных уравнений подразумевает, что $h_i(\theta) = \partial \ln f(y_i|x_i, \theta)/ \partial \theta$. Тогда (5.26) является условием первого порядка для ММП-оценки.

\subsection{Принцип аналогии}


Принцип аналогии использует условия для генеральной совокупности, чтобы сформулировать оценки. Книга Мански (1988a) подчёркивает важность принципа аналогии в качестве объединяющей темы для оценивания. Мански (1988a, стр. xi) приводит следующую цитату из Голдбергера (1968, стр. 4.):

Согласно принципу аналогии в оценивании $\dots$, параметры генеральной совокупности следует оценивать такими статистиками, которые обладают тем же свойством в выборке, что и параметры в генеральной совокупности.

Условия моментов для генеральной совокупности предлагают в качестве оценки решение соответствующего условия для момента выборки.

Примеры применения принципа аналогии для экстремальных оценок были приведены в Разделе 4.2. Например, если целью предсказания является минимизация ожидаемых потерь в генеральной совокупности и используются потери в квадратах ошибок, то параметры регрессии $\beta$ оцениваются путём минимизации выборочной суммы квадратов ошибок.

Оценки метода моментов также могут послужить примером. Например, в случае одинаково и независимо распределённых величин, если $\E[y_i-\mu]=0$ для генеральной совокупности, то мы используем в качестве оценки $\hat{\mu}$, которая является решением соответствующих условий выборочных моментов $N^{-1} \sum_i (y_i-\mu)=0$, что приводит к равенству с выборочным средним $\hat{\mu}=\bar{y}$.


Оценка оценочных уравнений может быть определена как оценка метода аналогий. Если (5.27) верно для генеральной совокупности, тогда можно оценить $\theta$, решая соответствующее условие выборочного момента (5.26).

Оценки оценочных уравнений широко используются в микроэконометрике. Соответствующая теория может быть рассмотрена как частный случай обобщённого метода моментов. Обобщенный метод моментов представлен в следующей главе, и допускает, что условий моментов больше, чем параметров. В прикладной статистике подход используется в контексте обобщённых оценочных уравнений.

\section{Cтатистические выводы} 

Подробное изложение проверки гипотез и построение доверительных интервалов приведены в Главе 7. Здесь мы опишем, как проверить линейные ограничения, в том числе исключающие ограничения. Мы используем самый распространенный метод, тест Вальда для оценок, которые у нас могут быть нелинейными.  Используется асимптотическая теории, поэтому формальные результаты приводят к распределению хи-квадрат и нормальному распределению, а на небольших выборках --- к $F$- и $t$-распределениям в линейной регрессии в условиях нормальности. Кроме того, существует несколько способов, чтобы получить состоятельную оценку ковариационной матрицы экстремальной оценки, что приводит к альтернативным оценкам стандартных ошибок и связанным с ними тестовых статистик и Р-значений.

\subsection{Тест Вальда для проверки гипотезы о линейных ограничениях}

Рассмотрим тестирование $h$ линейных независимых ограничений, проверим гипотезу $H_0$ против альтернативной $H_a$, где
\[
H_0: R \theta_0 - r = 0,
\]
\[
H_a: R \theta_0 - r \not= 0,
\]
c матрицей констант $R$ размера $h \times q$ и вектором констант $r$ размера $h \times 1$. Например, если $\theta=[\theta_1, \theta_2, \theta_3]$, то, чтобы протестировать верно ли, что $\theta_{10}-\theta_{20}=2$ можно взять $R=[1,-1,0]$ и $r=-2$.

Тест Вальда отвергает $H_0$, если $R\hat{\theta} - r$, выборочная оценка $R \theta_0 - r$, сильно отличается от $0$. Это определяет необходимость знать распределение $R\hat{\theta} - r$. Предположим, что $\sqrt{N}(\hat{\theta}-\theta_0) \xrightarrow{d} \mathcal{N}[0,C_0]$, где $C_0={A_0}^{-1} B_0 {A'_0}^{-1}$ из (5.20). Тогда 
\[
\hat{\theta} \stackrel{a}{\sim} \mathcal{N}[\theta_0, N^{-1} C_0],
\]
поэтому при верной $H_0$ линейная комбинация 
\[
R\hat{\theta}-r \stackrel{a}{\sim} \mathcal{N}[0,R(N^{-1}C_0)R'],
\]
где математическое ожидание равно 0, поскольку $R \theta_0 - r = 0$ в $H_0$.

\begin{center}
Тесты хи-квадрат
\end{center}

Удобно перейти от многомерного нормального распределения к распределению хи-квадрат, применив квадратичную форму. Это даёт статистику Вальда:
\begin{equation}
W=(R\hat{\theta}-r)'(R(N^{-1}\hat{C})R')^{-1}(R\hat{\theta}-r) \xrightarrow{d} \chi^{2}(h)
\end{equation}
при верной $H_0$, где $R(N^{-1}C_0)R'$ имеет полный ранг $h$ в предположении линейно независимых ограничений, а $\hat{C}$ является состоятельной оценкой $C_0$. Большие значения $W$ приводят к отвержению $H_0$, и $H_0$ отвергается на уровне $\alpha$, если $W>\chi_{\alpha}^{2}(h)$ и не отвергается в противном случае.

Практики часто вместо этого используют $F$-статистику $F=W/h$. Вывод тогда основывается на распределении $F(h,N-q)$ в надежде, что это может обеспечить лучшую аппроксимацию для конечной выборки. Обратите внимание, что $h$ умноженное на $F(h,N)$ сходится к распределению $\chi^{2}(h)$ при $N \rightarrow \infty$.

Замена $C_0$ на $\hat{C}$ при получении (5.32) не оказывает никакого влияния в асимптотическом случае, но в конечных выборках различные $\hat{C}$ приведут к различным значениям $W$. В случае классической линейной регрессии этот шаг соответствует замене $\sigma^2$ на $s^2$. Тогда $W/h$ имеет $F$-распределение, если ошибки распределены нормально (см. Раздел 7.2.1).

\begin{center}
Тесты на один коэффициент
\end{center}

Часто внимание уделяется тестированию отличия от нуля одного коэффициента, например, $j$-ого. Тогда $R \theta - r = \theta_j$ и $W=\hat{\theta_j}^2/(N^{-1}\hat{c_{jj}})$, где $\hat{c}_{jj}$ --- диагональный элемент $\hat{C}$. Если извлечём корень из $W$, то получим
\begin{equation}
t=\frac{\hat{\theta_j}}{se[\hat{\theta_j}]} \xrightarrow{d} \mathcal{N}[0,1]
\end{equation}
при верной $H_0$, где $se[\hat{\theta_j}]=\sqrt{N^{-1}\hat{c}_{jj}}$ --- асимптотическая стандартная ошибка $\hat{\theta_j}$. Большие значения $t$ приводят к отвержению $H_0$, и в отличие от $W$, $t$-cтатистика может быть применена для односторонних тестов.

Формально $\sqrt{W}$ является асимптотической $z$-статистикой, но мы используем обозначение $t$, поскольку это обозначает обычную <<$t$-статистику>>, получаемой делением оценки на её стандартную ошибку. В конечных выборках некоторые статистические пакеты используют стандартное нормальное распределение, в то время как другие используют $t$-распределения для расчёта критических значений, $P$-значений и доверительных интервалов. Ни то, ни другое не является правильным для конечных выборок, за исключением весьма частного случая линейной регрессии с ошибками, которые нормально распределены, и в этом случае $t$-распределение является точным. Оба способа ведут к одним и тем же результатам на бесконечно больших выборках, так как $t$-распределение сходится к стандартному нормальному распределению.

\begin{center}
Оценка ковариационной матрицы
\end{center}

Есть много возможных способов оценки ${A_0}^{-1} B_0 {A'_0}^{-1}$, потому что есть много способов состоятельно оценить $A_0$ и $B_0$. Таким образом, различные эконометрические программы  должны давать одинаковые оценки коэффициентов, но, вполне резонно, что они могут давать стандартные ошибки, $t$-статистики, и $P$-значения, отличающиеся в конечных выборках. Именно эконометрист определяет метод и мощность соответствующих предположений о распределении процесса порождающего данные.

\begin{center}
Сэндвич-оценка ковариационной матрицы
\end{center}

Предельное распределение $\sqrt{N}(\hat{\theta}-\theta_0)$ имеет ковариационную матрицу ${A_0}^{-1} B_0 {A'_0}^{-1}$. Отсюда следует, что у $\hat{\theta}$ асимптотическая ковариационная матрица $N^{-1}{A_0}^{-1} B_0 {A'_0}^{-1}$, где появляется деление на $N$, потому что мы рассматриваем $\hat{\theta}$, а не $\sqrt{N}(\hat{\theta}-\theta_0)$.

Сэндвич-оценка асимптотической ковариационной матрицы $\hat{\theta}$ --- это оценка вида
\begin{equation}
\hat{\Var}[\hat{\theta}]= N^{-1} \hat{A}^{-1} \hat{B} \hat{A}'^{-1},
\end{equation}
где $\hat{A}$ --- состоятельная оценка $A_0$ и $\hat{B}$ --- состоятельная оценка $B_0$. Это называется сэндвич-формой, поскольку $\hat{B}$ находится между $\hat{A}^{-1}$ и $\hat{A}'^{-1}$. Для многих оценок $A$ --- это матрица Гессе, поэтому $\hat{A}^{-1}$ симметрична, но это не всегда верно.

Скорректированной сэндвич-оценкой является сэндвич-оценка, где оценка $\hat{B}$ --- состоятельная оценка $B_0$ при относительно слабых предположениях. Это приводит к тому, что называют скорректированными стандартными ошибками. Ярким примером является устойчивая к гетероскедастичности оценка Уайта ковариационной матрицы МНК-оценки (см. Раздел 4.4.5). В различных конкретных контекстах, подробно описанных в последующих Разделах, скорректированными сэндвич-оценками называются оценки Губера, в честь Губера (1967); оценки Эйкера и Уайта, в честь Эйкера (1967) и Уайта (1980a,б, 1982), а также в применении к стационарным временным рядам оценки Ньюи-Веста, в честь Ньюи и Веста (1987b).

\begin{center}
Оценки $A$ и $B$
\end{center}

Здесь мы представляем различные оценки для $A_0$ и $B_0$ как для оценок методом оценочных уравнений, которые являются решением $h_{N}(\hat{\theta})=0$ так и для локальных экстремальных оценок, которые являются решением $\partial \mathcal{Q}_N(\theta) / \partial \theta |_{\hat{\theta}}=0 $

Две стандартные оценки $A_0$ в (5.29) и (5.18) --- это матрица Гессе
\begin{equation}
\hat{A}_H=\left. \frac{\partial h_{N}(\theta)}{\partial \theta'} \right|_{\hat{\theta}}= \left. \frac{\partial^2 \mathcal{Q}_N(\theta)}{\partial \theta \partial \theta'} \right|_{\hat{\theta}},
\end{equation}
где второе равенство объясняет использование термина матрица Гессе, и ожидаемая матрица Гессе
\begin{equation}
\hat{A}_{EH}=\E \left. \left[ \frac{\partial h_{N}(\theta)}{\partial \theta'}\right] \right|_{\hat{\theta}}= \E \left. \left[ \frac{\partial^2 \mathcal{Q}_N(\theta)}{\partial \theta \partial \theta'} \right] \right|_{\hat{\theta}}.
\end{equation}

Первая матрица аналитически проще и потенциально полагается на меньшее число предположений о распределении; последняя более вероятно, будет отрицательно определена и обратима.

Для $B_0$ в (5.30) или (5.19) нет возможности использовать очевидную оценку $Nh_{N}(\hat{\theta})h_{N}(\hat{\theta})'$, поскольку выражение равно нулю, так как $\hat{\theta}$ задаётся таким образом, чтобы удовлетворять условию $h_{N}(\hat{\theta})=0$. При потенциально сильных предположениях о распределении получаем оценку
\begin{equation}
\hat{B}_{E}=\E \left. \left[ Nh_{N}(\theta)h_{N}(\theta)' \right] \right|_{\hat{\theta}}= \E \left. \left[ N \frac{\partial \mathcal{Q}_N(\theta)}{\partial \theta} \frac{\partial \mathcal{Q}_N(\theta)}{\partial \theta'} \right] \right|_{\hat{\theta}}.
\end{equation}

Более слабые предположения возможны для M-оценок и оценок методом оценочных уравнений с данными, которые независимы по $i$. Тогда (5.30) упрощается до следующего вида:
\[
B_0=\E \left[ \frac{1}{N} \sum_{i=1}^{N} h_i(\theta) h_i(\theta)' \right],
\]
поскольку независимость означает, что для $i \not= j$, $\E[h_i h_j']=\E[h_i] \E[h'_j]$, которая, в свою очередь, равна нулю при условии, что $\E[h_i(\theta)]=0$. Мы получаем оценки с использованием внешнего произведения или оценкам BHHH (в честь Берндт, Холла, Холла, и Хаусмана, 1974)

\begin{equation}
\hat{B}_{OP}=  \frac{1}{N} \left. \sum_{i=1}^{N} h_i(\hat{\theta}) h_i(\hat{\theta})'= \frac{1}{N} \sum_{i=1}^{N} \frac{\partial q_i(\theta)}{\partial \theta} \right|_{\hat{\theta}} \left. \frac{\partial q_i(\theta)}{\partial \theta'} \right|_{\hat{\theta}}.
\end{equation}
$\hat{B}_{OP}$ требует меньшего числа предположений, чем $\hat{B}_{E}$.

На практике корректировка степеней свободы часто используется при оценке $B_0$, при
делении в (5.38) $\hat{B}_{OP}$ на $(N-q)$, а не $N$, и аналогичном умножении $\hat{B}_{E}$ в (5.37) на $N/(N-q)$. Нет теоретического обоснования для этой корректировки в нелинейных моделях, но в некоторых модельных исследованиях эта корректировка приводит к улучшению результатов в конечных выборках, и оно совпадает с корректировкой степеней свободы для МНК с гетероскедастичными ошибками. Никаких подобных корректировок не производится для $\hat{A}_H$ или $\hat{A}_{EH}$.

Упрощение происходит в некоторых особых случаях с $A_0=-B_0$. Наиболее яркими примерами являются МНК или НМНК с гетероскедастичными ошибкам (см. Раздел 5.8.3) и ММП с правильно специфицированным распределением (см. Раздел 5.6.4). Тогда либо $-\hat{A}^{-1}$, либо
$\hat{B}^{-1}$ может быть использованы для оценки дисперсии $\sqrt{N}(\hat{\theta} - \theta_0)$. Эти оценки являются менее устойчивыми к неправильной спецификации процесса порождающего данные, чем те, которые применяют сэндвич-форму. Неправильная спецификация процесса порождающего данные, тем не менее, может дополнительно привести к несостоятельности $\hat{\theta}$, в этом случае даже выводы, основанные на скорректированной сэндвич-оценке, будут недействительными.

Для примера с регрессией Пуассона в Разделе 5.2 $\hat{A}_H = \hat{A}_{EH}=-N^{-1} \sum_i \exp(x'_i \hat{\beta})x_i x'_i$ и  $\hat{B}_{OP}=(n-q)^{-1} \sum_i (y_i-\exp(x'_i \hat{\beta}))^{2} x_i x'_i$. Если $\Var[y|x]=\exp(x' \beta_0)$, что будет верно, если $y|x$ распределён действительно по Пуассону, то $\hat{B}_{E}=-[N/(N-q)] \hat{A}_{EH}$ и имеет место упрощение.

\section{Метод максимального правдоподобия}

Оценки ММП занимают особое место среди оценок. Это самые эффективные оценки среди состоятельных асимптотически нормальных оценок. Они также важны в учебных целях, так как многие методы нелинейной регрессии такие, как M-оценки, можно рассматривать как расширения и адаптацию результатов, впервые полученных для оценок ММП.

\subsection{Функция правдоподобия}

\begin{center}
Принцип максимального правдоподобия
\end{center}

Принцип максимального правдоподобия, введённый Р.А. Фишером (1922), заключается в выборе в качестве оценки вектора параметров $\theta_0$ того значения $\theta$, которое максимизирует вероятность получения фактической выборки. В дискретном случае это вероятность, полученная из функции вероятности, в непрерывном случае --- это плотность. Рассмотрим дискретный случай. Если значение $\theta$ означает, что вероятность наблюдаемых данных --- 0.0012, в то время как второе значение $\theta$ даёт более высокую вероятность 0.0014, то второе значение $\theta$ является лучшей оценкой.

Совместная функция вероятности $f(y,X|\theta)$ рассматривается здесь как функция от $\theta$ при фиксированных данных $(y,X)$. Это называется функцией правдоподобия и обозначается как $L_{N}(\theta|y,X)$. Максимизация $L_{N}(\theta)$ эквивалентна максимизации логарифмической функции правдоподобия.
\[
\mathcal{L}_{N}(\theta)=\ln L_{N}(\theta).
\]

Возьмём натуральный логарифм, потому что после логарифмирования мы получим сумму, а не произведение $N$ членов.

\begin{center}
Условная функция правдоподобия
\end{center}

Функция правдоподобия $L_{N}(\theta)=f(y,X|\theta)=f(y|X,\theta)f(X|\theta)$ требует задания как условной плотности $y$ при заданном $x$, так и предельной плотности $X$.

Вместо этого, оценки, как правило, основываются на условной функции правдоподобия $L_{N}(\theta)=f(y|X,\theta)$, так как цель регрессии заключается в моделировании поведения $y$ при заданном $X$. Это не является ограничением, если $f(y|X)$ и $f(X)$ зависят от взаимоисключающих наборов параметров. Когда это так, часто опускают слово условный. Для редких исключений таких, как эндогенные выборки (см. Главы 3 и 24), состоятельное оценивание требует, чтобы оценка основывалась на полной совместной плотности $f(y,X|\theta)$, а не условной плотности $f(y|X,\theta)$.

\begin{table}[h]
\begin{center}
\caption{\label{tab:max} Максимальное правдоподобие: часто используемые плотности}
\begin{tabular}[t]{llll}
\hline
\hline
\bf{Распределение} & \bf{Диапозон $y$} & \bf{Плотность $f(y)$} & \bf{Наиболее частая} \\
 & & & \bf{параметризация} \\
\hline
Нормальное & $(-\infty,\infty)$ & $[2\pi\sigma^{2}]^{-1/2}e^{-(y-\mu)^{2}/2\sigma^{2}}$ & $\mu=x'\beta, \sigma^{2}=\sigma^{2}$\\
Бернулли & $0$ или $1$ & $p^{y}(1-p)^{1-y}$ & $Logit$ $p=e^{x'\beta}/(1+e^{x'\beta})$\\
Экспоненциальное & $(0,\infty)$ & $\lambda e^{-\lambda y}$ & $\lambda=e^{x'\beta}$ или $1/\lambda=e^{x'\beta}$ \\
Пуассона & $0,1,2,\cdots$ & $e^{-\lambda}\lambda^{y}/y!$ & $\lambda=e^{x'\beta}$ \\
\hline
\hline
\end{tabular}
\end{center}
\end{table}

\vspace{3cm}

Для пространственных данных наблюдения $(y_i,x_i)$ независимы по $i$ с функцией условной плотности $f(y_i|x_i,\theta)$. Тогда в силу независимости совместной условной плотности $f(y|X,\theta)=\prod_{i=1}^{N}f(y_i|x_i,\theta)$, что приводит к 
\begin{equation}
\mathcal{Q}_{N}(\theta)=N^{-1}\mathcal{L}_{N}(\theta)=\frac{1}{N} \sum_{i=1}^N \ln f(y_i|x_i,\theta),
\end{equation}
где мы делим на $N$ для того, чтобы целевая функция была средним арифметическим.

Результаты распространяются на многомерные данные, системы уравнений и панельные данные путём замены скаляра $y_i$ на вектор $y_i$, в этом случае $f(y_i|x_i,\theta)$ --- совместная плотность $y_i$
при фиксированных $x_i$. См. также Раздел 5.7.5.

\begin{center}
Примеры
\end{center}

Для разных типов данных следующий метод используется для построения полностью параметрической пространственной модели регрессии. Сначала выберем распределение с одним параметром или с двумя (или в некоторых редких случаях с тремя), данное распределение будет использоваться для зависимой переменной $y$ в случае одинаково и независимо распределённых величин, как в базовом курсе статистики. Один или два основных параметра распределения сделаем функциями от  регрессоров $x$ и параметров $\theta$.

Некоторые часто используемые распределения и параметризации приведены в таблице 5.3. Дополнительные распределения приведены в Приложении B, в котором также представлены методы для генерирования псевдо-случайных чисел.

Для непрерывных на $(-\infty,\infty)$ данных, нормальное распределение является распространенным распределением. В классической модели линейной регрессии $\mu=x'\beta$ и $\sigma^2$ постоянна.

Для дискретных бинарных данных, принимающих значения $0$ или $1$, всегда используется распределение Бернулли, частный случай биномиального распределения при одной попытке. Обычные параметризации для вероятности Бернулли приводят к логит-модели, представленной в таблице 5.3, и пробит-модели с  $p=\Phi(x'\beta)$, где $\Phi(\cdot)$ является  функцией стандартного нормального распределения. Эти модели анализируются в главе 14.

Для положительных непрерывных данных на $(0,\infty)$, в частности, данных по длительности, которые рассматриваются в Главах 17-19, более сложные модели такие, как модель Вейбулла, гамма модель и лог-нормальная модель часто используются в дополнение к экспоненциальному распределению, приведённому в таблице 5.3.

Для целочисленных данных, принимающих значения $0,1,2, \dots$, (см. главу 20), более сложное отрицательное биномиальное распределение часто используется в дополнение к распределению Пуассона, представленному в Разделе 5.2.1. Равенство $\lambda=\exp(x'\beta)$ обеспечивает положительное условное математическое ожидание.

Для неполностью наблюдаемых данных могут быть использованы цензурированные или усечённые варианты этих распределений. Наиболее распространённым примером является цензурированное нормальное распределение, которое называют моделью тобит, и оно представлено в Разделе 16.3.

Модели, основанные на стандартном методе максимального правдоподобия, редко специфицируются с помощью введения предположений о распределении ошибок. Напротив, они определяются непосредственно в терминах распределения зависимой переменной. В частном случае при $y \sim \mathcal{N}[x'\beta,\sigma^2]$ можно эквивалентно определить $y=x'\beta+u$, где остаточный член $u \sim \mathcal{N}[0,\sigma^2]$. Тем не менее, здесь используется аддитивность нормального распределения, которая также есть у некоторых других распределений. Например, если $y$ распределён по Пуассону с математическим ожиданием $\exp(x'\beta)$, мы всегда можем написать $y=\exp(x'\beta)+u$, но ошибка уже имеет нестандартное распределение.

\subsection{Оценка максимального правдоподобия}

Оценка метода максимального правдоподобия (ММП) --- оценка, которая максимизирует (условную) логарифмическую функцию правдоподобия и, очевидно, является экстремальной оценкой. Обычно ММП-оценка --- локальный максимум, который является решением условия первого порядка
\begin{equation}
\frac{1}{N} \frac{\partial \mathcal{L}_{N}(\theta)}{\partial \theta} = \frac{1}{N} \sum_{i=1}^{N} \frac{\partial \ln f(y_i|x_i,\theta)}{\partial \theta}=0.
\end{equation}
Более формально эта оценка является условной ММП-оценкой, так как она основана на условной плотности $y$ при заданном $x$, но часто используют просто термин ММП-оценка.

Вектор градиентов $\partial \mathcal{L}_{N}(\theta) / \partial \theta$ называется скор-вектором, так как он является суммой первых производных логарифмической функции плотности. Когда он оценивается в точке $\theta_0$, он называется эффективным значением (efficient score).

\subsection{Равенство информационных матриц}

Результаты Раздела 5.3 можно упростить для ММП при условии, что плотность правильно специфицирована и для неё диапазон значений $y$ не зависит от $\theta$.

\begin{center}
Условия регулярности
\end{center}

Условия регулярности ММП: 
\begin{equation}
\E_{f} \left[ \frac{\partial \ln f(y|x,\theta)}{\partial \theta} \right] = \int \frac{\partial \ln f(y|x,\theta)}{\partial \theta} f(y|x,\theta)=0
\end{equation}
и
\begin{equation}
-\E_{f} \left[ \frac{\partial^2 \ln f(y|x,\theta)}{\partial \theta \partial \theta'} \right] = \E_{f} \left[ \frac{\partial \ln f(y|x,\theta)}{\partial \theta} \frac{\partial \ln f(y|x,\theta)}{\partial \theta'}\right],
\end{equation}
где введено обозначение $\E_{f}[\cdot]$, чтобы сделать явным, что математическое ожидание зависит от указанной плотности $f(y|x,\theta)$. Из результата (5.41) следует, что ожидаемая оценка скор-вектора равна нулю, и (5.42) приводит к (5.44).

Выкладки из Раздела 5.6.7 требуют, чтобы диапазон значений $y$ не зависел от $\theta$ для того, чтобы интегрирование и дифференцирование можно было поменять местами.

\begin{center}
Равенство информационных матриц
\end{center}

Информационная матрица является математическим ожиданием внешнего произведения градиентов,
\begin{equation}
\mathcal{I}=\E \left[ \frac{\partial \mathcal{L}_{N}(\theta)}{\partial \theta} \frac{\partial \mathcal{L}_{N}(\theta)}{\partial \theta'}\right].
\end{equation}
Применяется термин информационная матрица, так как $\mathcal{I}$ --- это дисперсия $\partial \mathcal{L}_{N}(\theta) / \partial \theta$, поскольку в силу (5.41) математическое ожидание $\partial \mathcal{L}_{N}(\theta) / \partial \theta$ равно 0. Тогда большие значения $\mathcal{I}$ говорят о том, что небольшие изменения в $\theta$ приведут к большим изменениям в логарифме правдоподобия, который, соответственно, содержит значительную информацию о состоянии $\theta$. Величина $\mathcal{I}$ более точно называется информацией Фишера, так как есть альтернативные измерители информации.

Для логарифмической функции правдоподобия (5.39) из условия регулярности (5.42) следует, что
\begin{equation}
-\E_{f} \left[ \left. \frac{\partial^2 \mathcal{L}_{N}(\theta)}{\partial \theta \partial \theta'} \right|_{\theta_0} \right] = \E_{f} \left[ \left. \frac{\partial \mathcal{L}_{N}(\theta)}{\partial \theta} \frac{\partial \mathcal{L}_{N}(\theta)}{\partial \theta'} \right|_{\theta_0} \right],
\end{equation}
если математическое ожидание зависит от $f(y|x,\theta_0)$. Соотношение (5.44) называется равенством информационных матриц и означает, что информационная матрица также равна $-\E [\partial^2 \mathcal{L}_{N}(\theta) / \partial \theta \partial \theta']$. Из равенства информационных матриц (5.44) следует, что $-A_0=B_0$, где $A_0$ и $B_0$ определены в (5.18) и (5.19). Теорема 5.3 в таком случае упрощается, поскольку ${A_0}^{-1}B_0{A_0}^{-1}=-A_0^{-1}=B_0^{-1}$. 

Равенство (5.42), в свою очередь, является частным случаем равенства обобщённых информационных матриц
\begin{equation}
\E_{f} \left[ \frac{\partial m(y,\theta)}{\partial \theta'} \right] = -\E_{f} \left[ m(y,\theta) \frac{\partial \ln  f(y|\theta)}{ \partial \theta'} \right],
\end{equation}
где $m(\cdot)$ является векторной функцией моментов с $\E_{f}[m(y,\theta)]=0$, и математическое ожидание зависит от плотности  $f(y|\theta)$. Этот результат также был получен в Разделе 5.6.7 и используется в главах 7 и 8, чтобы получить более простые формы некоторых тестовых статистик.

\subsection{Распределение ММП-оценок}

Условия регулярности (5.41) и (5.42) приводят к упрощению общих результатов Раздела 5.3.

Необходимое условие состоятельности (5.25) заключается в том, что $\E[\partial \ln f(y|x,\theta) / \partial \theta|_{\theta_0}] = 0$. Это имеет место в силу условия регулярности (5.41) при условии, что математическое ожидание зависит от $f(y|x,\theta_0)$. Таким образом, если процесс порождающий данные --- это $f(y|x,\theta_0)$, то есть плотность была правильно специфицирована, ММП-оценка состоятельная для $\theta_0$.

Для асимптотического распределения упрощение происходит, поскольку $-A_0=B_0$ в силу равенства информационных матриц, которое опять же предполагает, что плотность правильно определена. Эти результаты могут быть собраны в следующем утверждении.

\begin{proposition}[Распределение  ММП-оценки]: Если выполнены допущения:
\begin{enumerate}
\item Процесс порождающий данные описывается условной плотностью $f(y_i|x_i,\theta_0)$, которая используется для того, чтобы определить функцию правдоподобия.
\item Функция плотности $f(\cdot)$ удовлетворяет следующему условию $f(y,\theta^{(1)})=f(y,\theta^{(2)})$ тогда и только тогда, когда $\theta^{(1)}=\theta^{(2)}$.
\item Матрица 
\begin{equation}
A_0= \left. \plim \frac{1}{N} \frac{\partial^2 \mathcal{L}_{N}(\theta)}{\partial \theta \partial \theta'} \right|_{\theta_0}
\end{equation}
существует и невырождена.
\item Порядок дифференцирования и интегрирования логарифмической функции правдоподобия может быть изменен несколько раз.
\end{enumerate}
Тогда ММП-оценка $\hat{\theta}_{ML}$, которая определяется как решение условий первого порядка $ \partial N^{-1} \mathcal{L}_{N}(\theta) / \partial \theta=0$, состоятельна для $\theta_0$ и
\begin{equation}
\sqrt{N} (\hat{\theta}_{ML} - \theta_0) \xrightarrow{d} \mathcal{N}[0,{-A_0}^{-1}].
\end{equation}
\end{proposition}

Условие 1 говорит о том, что условная плотность правильно специфицирована, условия 1 и 2 гарантируют, что можно найти $\theta_0$; условие 3 аналогично предположению о $\plim N^{-1}X'X$ в случае оценки МНК; и условие 4 является необходимым для регулярности. Как и в общем случае предел по вероятности и математическое ожидание зависят от процесса порождающего данные для $(y,X)$, или только от $y$, если регрессоры предполагаются нестохастическими или  производится условный анализ, то есть при заданном $X$.

Ослабление условия 1 подробно рассматривается в Разделе 5.7. Большинство примеров ММП удовлетворяют условию 4, но оно исключает некоторые модели такие, как равномерно распределённый $y$ на интервале $[0,\theta]$, так как в этом случае диапазон $y$ зависит от $\theta$. Тогда не только $A_0 \not= -B_0$, но и глобальная ММП-оценка сходится со скоростью отличной от $\sqrt{N}$ и имеет предельное распределение, которое не является нормальным. См., например, Хирано и Портер (2003).

Учитывая утверждение 5.5, полученное асимптотическое распределение ММП-оценки  часто выражается следующим образом:
\begin{equation}
\hat{\theta}_{ML} \stackrel{a}{\sim} \mathcal{N} \left[ \theta, - \left( E \left[ \frac{\partial^2 \mathcal{L}_{N}(\theta)}{\partial \theta \partial \theta'} \right] \right) ^{-1} \right],
\end{equation}
где для простоты записи указание точки $\theta_0$ опускается, и мы предполагаем, что ЗБЧ применим, поэтому оператор $\plim$ в определении $A_0$ заменяется на $\lim E$, а затем убирается предел. Это обозначение часто используется в последующих главах.

Правая часть (5.48) является нижней границей Рао-Крамера, которая, из базового курса статистики, является нижней границей дисперсии несмещённых оценок в малых выборках. В больших выборках, рассматриваемых здесь, это нижняя граница для ковариационной матрицы состоятельных и асимптотически нормальных оценок с равномерной сходимостью к нормальному распределению $\sqrt{N}(\hat{\theta}-\theta_0)$  на компактных интервалах $\theta_0$ (см. Рао, 1973 , стр. 344-351). Грубо говоря ММП-оценка привлекательна тем, что имеет наименьшую асимптотическую дисперсию среди $\sqrt{N}$-состоятельных оценок. Этот результат требует сильного предположения о правильной спецификации условной плотности.

\subsection{Пример регрессии Вейбулла}

В качестве примера рассмотрим регрессию, основанную на распределении Вейбулла, которое используется для моделирования данных о длительности, таких как продолжительность периода безработицы (см. Главу 17).

Плотность распределения Вейбулла $f(y)=\gamma \alpha y^{\alpha-1}\exp(-\gamma y^{\alpha})$, где $y>0$ и параметры $\alpha>0$ и $\gamma>0$. Можно показать, что $\E[y] = \gamma^{-1/\alpha} \Gamma( \alpha^{-1}+1)$, где $\Gamma(\cdot)$ --- гамма-функция. Стандартная модель регрессии Вейбулл получается, если  $\gamma=\exp(x'\beta)$, в этом случае $\E[y|x]= \exp(-x' \beta/ \alpha) \Gamma(\alpha^{-1}+1))$. Учитывая независимость по $i$, логарифмическая функция правдоподобия выглядит следующим образом:
\[
N^{-1} \mathcal{L}_{N}(\theta)= N^{-1} \sum_i \{x'_i \beta + \ln\alpha + (\alpha-1)\ln y_i - \exp(x'_i\beta){y_i}^{\alpha} \}.
\]
Дифференцирование по $\beta$ и $\alpha$ приводит к условиям первого порядка:
\[
N^{-1} \sum_i \{1- \exp(x'_i\beta){y_i}^{\alpha} \} x_i=0, 
\]
\[
N^{-1} \sum_i \{\frac{1}{\alpha}+\ln y_i- \exp(x'_i\beta){y_i}^{\alpha}\ln y_i \}=0.
\]

В отличие от примера Пуассона, состоятельность по существу требует правильной спецификации распределения. Чтобы убедиться в этом, рассмотрим условия первого порядка для $\beta$. Неформальное условие (5.25), $\E[\{1-\exp(x'\beta){y}^{\alpha}\}x]=0$, требует, чтобы $\E[y^{\alpha}|x]=\exp(-x'\beta)$, где степень $\alpha$ может быть не только целым числом. Условия первого порядка для $\alpha$ приводят к ещё более необычным условиям на моменты $y$.

Итак, мы должны исходить из того, что это, в действительности, плотность Вейбулла с $\gamma=\exp(x'\beta_0)$ и $\alpha=\alpha_0$. Теорема 5.5 может быть применена, поскольку диапазон $y$ не зависит от параметров. Тогда из (5.48), ММП-оценка Вейбулла асимптотически нормальна с асимптотической дисперсией
\begin{equation}
\Var \begin{bmatrix} \hat{\beta} \\ \hat{\alpha} \end{bmatrix}  = \begin{pmatrix} -\E \begin{bmatrix} \sum_i -e^{x'_i\beta_0} {y_i}^{\alpha_0} x_i x'_i & \sum_i -e^{x'_i\beta_0}{y_i}^{\alpha_0}\ln(y_i)x_i \\ \sum_i -e^{x'_i\beta_0}{y_i}^{\alpha_0}\ln(y_i)x_i & \sum_i d_i \end{bmatrix} \end{pmatrix} ^{-1}.
\end{equation}
где $d_i=-(1/{\alpha_0}^{2})-e^{x'_i\beta_0}{y_i}^{\alpha_0} (\ln y_i)^2$, обратная матрица (5.49) должна быть получена обращением по блокам, потому что математическое ожидание недиагонального элемента $\partial^2 \mathcal{L}_{N}(\beta,\alpha)/ \partial \beta \partial \alpha$ не равно нулю. Упрощение имеет место в моделях с нулевым математическим ожиданием смешанной производной $\E[\partial^2 \mathcal{L}_{N}(\beta,\alpha)/ \partial \beta \partial \alpha']=0$ таких, как регрессия с нормально распределёнными ошибками, и в этом случае информационная матрица называется блочно-диагональной по $\beta$ и $\alpha$.

\subsection{Оценка ковариационной матрицы ММП-оценок} 

Есть несколько способов, чтобы состоятельно оценить ковариационную матрицу
экстремальной оценки, как уже отмечалось в Разделе 5.5.2 . Для ММП-оценки возникают дополнительные возможности, если равенство информационных матриц считается выполненным. Тогда ${A_0}^{-1}B_0{A_0}^{-1}$, ${-A_0}^{-1}$ и ${B_0}^{-1}$, все асимптотически эквивалентны, как и соответствующие состоятельные оценки этих величин. Детальное обсуждение ММП-оценок приведено у Дэвидсона и МакКиннона (1993, глава 18).

Сэндвич-оценка $\hat{A}^{-1}\hat{B}\hat{A}^{-1}$ называется оценкой Губера, в честь Губера (1967), или оценкой Уайта, в честь Уайта (1982), который работал над распределением ММП-оценки, не применяя равенство информационных матриц. Сэндвич-оценка в теории считается
более устойчивой, чем $-\hat{A}^{-1}$ или $\hat{B}^{-1}$. Важно отметить, однако, что причина неравенства информационных матриц может дополнительно привести к более фундаментальному осложнению в виде несостоятельности $\hat{\theta}_{ML}$. Это является предметом рассмотрения в Разделе 5.7.

\subsection{Вывод ММП условий регулярности}

Теперь мы формально получим  условия регулярности, изложенные в Разделе 5.6.3. Для простоты обозначений индекс $i$ и вектор регрессоров опускаются.

Начнём с получения первого условия (5.41). Интеграл плотности равен одному, то есть
\[
\int f(y|\theta)dy=1.
\]
Взяв производную по $\theta$ от обеих частей, получим $\frac{\partial}{\partial \theta} \int f(y|\theta)dy=0$. Если границы интегрирования (диапазон $y$) не зависят от $\theta$, то
\begin{equation}
\int \frac{\partial f(y|\theta)}{\partial \theta}dy=0.
\end{equation} 
Тогда из  $\partial \ln f(y|\theta)/\partial \theta= [\partial f(y|\theta) / \partial \theta]/ [f(y|\theta)]$ следует
\begin{equation}
\frac{\partial f(y|\theta)}{\partial \theta}=\frac{\partial \ln f(y|\theta)}{\partial \theta}f(y|\theta).
\end{equation} 
Подставляя (5.51) в (5.50), получим
\begin{equation}
\int \frac{\partial \ln f(y|\theta)}{\partial \theta}f(y|\theta)dy=0,
\end{equation} 
что то же самое, что (5.41) при условии, что математическое ожидание берется по плотности $f(y|\theta)$.

Теперь рассмотрим второе условие (5.42), чтобы получить более общий результат. Предположим
\[
\E[m(y,\theta)]=0,
\]
для некоторой (возможно, векторной) функции $m(\cdot)$. Тогда, когда ожидание берётся
по функции плотности $f(y|\theta)$
\begin{equation}
\int m(y,\theta)f(y|\theta)dy=0.
\end{equation} 
Продифференцировав обе части по $\theta'$ и предположив, что дифференцирование и интегрирование можно поменять местами, получим
\begin{equation}
\int \left( \frac{\partial m(y,\theta)}{\partial \theta'}f(y|\theta)+m(y,\theta)\frac{\partial f(y|\theta)}{\partial \theta'}\right)dy=0.
\end{equation} 
Подставляя (5.51) в (5.54), получим
\begin{equation}
\int \left(\frac{ \partial m(y,\theta)}{\partial \theta'}f(y|\theta)+m(y,\theta)\frac{\partial \ln  f(y|\theta)}{\partial \theta'}f(y|\theta) \right)dy=0,
\end{equation} 
или
\begin{equation}
\E \left[ \frac{\partial m(y,\theta)}{\partial \theta'} \right] = - \E \left[ m(y,\theta)\frac{\partial \ln  f(y|\theta)}{\partial \theta'} \right],
\end{equation} 
когда математическое ожидание берётся по плотности  $f(y|\theta)$. Условие регулярности (5.42) является частным случаем $m(y,\theta)= \partial \ln f(y|\theta)/\partial \theta$ и приводит к равенству информационных матриц (5.44). Более общий результат (5.56) приводит к обобщённому равенству информационных матриц (5.45).

Что происходит, когда интегрирование и дифференцирование нельзя поменять местами? Стартовая формула (5.50) уже не является верной, поскольку на основании фундаментальной теоремы анализа производная по $\theta$ от $\int f(y|\theta)dy$ включает дополнительный член, отражающий наличие функции $\theta$ в пределах интеграла. Тогда $\E[\partial \ln f(y|\theta)/\partial \theta] \not=0$.

Что происходит, когда плотность специфицирована неправильно? Тогда (5.52) остаётся в силе, но это не обязательно означает, что и (5.41) тоже, так как в (5.41) математическое ожидание больше не будет считаться с использованием плотности $f(y|\theta)$.

\section{Метод квази-максимального правдоподобия}

Оценка квази-ММП $\hat{\theta}_{QML}$ определяется как оценка, которая максимизирует логарифмическую функцию правдоподобия, которая неправильно специфицирована в результате неправильный спецификации плотности. Обычно такая неправильная спецификация приводит к несостоятельным оценкам.

В этом разделе сначала представлены общие свойства оценки квази-ММП, а затем некоторые особые случаи, когда оценка квази-ММП сохраняет состоятельность.

\subsection{Псевдо-истинные значения}

В принципе любая неправильная спецификация плотности может привести к несостоятельности, поскольку тогда математическое ожидание оценки $\E[\partial \ln f(y|x,\theta)/\partial \theta|_{\theta_0}]$ (см. раздел 5.6.4) считается с использованием функции отличной от $f(y|x,\theta_0)$.

Путём модификации общего доказательства состоятельности в Разделе 5.3.2 оценка квази-ММП $\hat{\theta}_{QML}$ сходится по вероятности к псевдо-истинному значению $\theta^*$, которое определяется следующим образом:
\begin{equation}
\theta^*= \argmax _{\theta \in \Theta} (\plim N^{-1} \mathcal{L}_{N}(\theta)).
\end{equation}

Предел по вероятности берётся по отношению к истинному процессу порождающему данные. Если истинный процесс порождающий данные отличается от предполагаемой плотности $f(y|x,\theta)$, используемой для формирования  $\mathcal{L}_{N}(\theta)$, то обычно $\theta^* \not = \theta_0$ и оценка квази-ММП несостоятельна.

Губер (1967) и Уайт (1982) показали, что асимптотическое распределение оценки квази-ММП похоже на асимптотическое распределение для оценки ММП за исключением того, что оно сосредоточено вокруг $\theta^*$ и больше нет равенства информационных матриц. Тогда

\begin{equation}
\sqrt{N}(\hat{\theta}_{QML} - \theta^*) \xrightarrow{d} \mathcal{N}[0,A^{*-1}B^{*}A^{*-1}],
\end{equation}
где $A^*$ и $B^*$ определены в (5.18) и (5.19) за исключением  того, что предел по вероятности берётся по неизвестному истинному процессу порождающему данные и оценивается  в точке $\theta^*$. Состоятельные оценки $\hat{A}^*$ и $\hat{B}^*$ могут быть получены как в Разделе 5.5.2 с помощью оценки $\hat{\theta}_{QML}$.

Этот результат используется для статистических выводов, если оценка квази-ММП сохраняет состоятельность. Если оценка квази-ММП не состоятельна, тогда обычно $\theta^*$ не имеет простой интерпретации кроме той, которая дана в следующем разделе. Тем не менее, (5.58) всё ещё может быть полезен, если, тем не менее, важна точность оценивания. Результат (5.58) также является мотивацией для теста Уайта для информационных матриц (см. Раздел 8.2.8), а также для теста Вуонга на различие параметрических моделей (см. Раздел 8.5.3).

\subsection{Расстояние Кульбака-Лейблера}

Вспомним из Раздела 4.2.3, что если $\E[y|x] \not = x' \beta_0$, то МНК-оценка ещё может быть интерпретирована как наилучший линейный предиктор $\E[y|x]$ при потере квадратных ошибок. Уайт (1982) предложил в сущности аналогичную интерпретацию для оценки квази-ММП.

Пусть $f(y|\theta)$ обозначает предполагаемую совместную плотность $y_1, \dots, y_n$ и пусть $h(y)$ обозначает истинную плотность, которая неизвестна, где для простоты зависимость от регрессоров не обозначена. Определим информационный критерий Кульбака-Лейблера (Kullback–Leibler information criterion):
\begin{equation}
KLIC = \E \left[ \ln \left( \frac{h(y)}{f(y|\theta)} \right) \right],
\end{equation}
где математическое ожидание берется по $h(y)$. Информационный критерий Кульбака-Лейблера принимает минимальное значение 0, когда существует $\theta_0$ такое, что $h(y)=f(y|\theta_0)$, то есть плотность правильно специфицирована, а большие значения информационного критерия Кульбака-Лейблера указывают на большую степень неуверенности относительно истинной плотности.

Тогда оценка квази-ММП $\hat{\theta}_{QML}$ минимизирует расстояние между $f(y|\theta)$ и $h(y)$, где расстояние измеряется с помощью информационного критерия Кульбака-Лейблера. Для получения этого результата обратим внимание, что при подходящих предположениях $\plim N^{-1} \mathcal{L}_{N}(\theta)=\E[\ln f(y|\theta)]$, поэтому $\hat{\theta}_{QML}$ сходится к $\theta^*$, которая максимизирует $\E[\ln f(y|\theta)]$. Однако это эквивалентно минимизации информационного критерия Кульбака-Лейблера, поскольку он равен $KLIC=\E[\ln h(y)]-\E[\ln f(y|\theta)]$ и первое слагаемое не зависит от $\theta$, поскольку математическое ожидание считается с использованием функции $h(y)$.

\subsection{Экспоненциальное семейство распределений}

В некоторых особых случаях оценки квази-ММП состоятельны даже тогда, когда плотность частично неправильно специфицирована. Одним хорошо известным примером является то, что оценка квази-ММП для линейной регрессионной модели при нормальности является состоятельной, даже если ошибки ненормальные при условии, что $\E[y|x]=x' \beta_0$. ММП-оценка Пуассона может послужить вторым примером (см. Раздел 5.3.4).

Похожая устойчивость к неправильной спецификации есть и в других моделях, основанных на плотности из экспоненциального семейства распределений. Для данного семейства плотность или вероятность может быть выражена как
\begin{equation}
f(y|\mu)=\exp \{ a(\mu)+b(y)+c(\mu)y \},
\end{equation}
для параметризации семейства с помощью математического ожидания, $\mu=\E[y]$. Можно показать, что для этой плотности $\E[y]=-[c'(\mu)]^{-1}a'(\mu)$ и $\Var[y]=[c'(\mu)]^{-1}$, где $c'(\mu)=\partial c(\mu) / \partial \mu$ и $a'(\mu)=\partial a(\mu) / \partial \mu$. Различные функции $a(\cdot)$ и $c(\cdot)$ приводят к различным плотностям. Величина $b(y)$ в (5.60) является нормализующей константой, которая обеспечивает, чтобы сумма вероятностей или интеграл были равны одному. Остальная часть плотности $\exp \{ a(\mu)+c(\mu)y \}$ является экспоненциальной функцией, линейной по $y$, что объясняет название семейства.

Большинство плотностей не может быть выражено в этой форме. Некоторые важные плотности --- это плотности из экспоненциального семейства, в том числе и приведенные в таблице 5.4. Эти плотности, уже представленные в таблице 5.3, выражены в таблице 5.4 в виде (5.60). Другие примеры экспоненциального семейства --- это биномиальное распределения с известным числом испытаний (частный случай --- Бернулли), некоторые негативные биномиальные модели (частные случаи --- геометрическое распределение и распределение Пуассона), а также однопараметрическое гамма-распределение (частный случай --- экспоненциальное).

\begin{table}[h]
\begin{center}
\caption{\label{tab:LEF} Плотности и вероятности экспоненциального семейства:  распространенные примеры}
\begin{tabular}[t]{llll}
\hline
\hline
\bf{Распределение} & $f(y)=\exp \{ a(\cdot)+b(y)+c(\cdot)y \}$ & $\E(y)$ &  $\Var[y]=[c'(\mu)]^{-1}$ \\
\hline
Нормальное ($\sigma^2$ известна) & $\exp \{ \frac{-\mu^2}{2 \sigma^2} - \frac{1}{2} \ln(2 \pi \sigma^2)- \frac{y^2}{2 \sigma^2} + \frac{\mu}{\sigma^2} y \}$ & $\mu$ & $\sigma^2$ \\
Бернулли & $\exp \{\ln(1-p) + \ln[p/(1-p)]y \}$ & $\mu=p$ & $\mu(1-\mu)$ \\
Экспоненциальное & $\ln \{ \ln \lambda - \lambda y \}$ & $\mu=1/\lambda$ & $\mu^2$  \\
Пуассона & $\ln \{ -\lambda - \ln y! + y \ln \lambda \}$ & $\mu=\lambda$ & $\mu$ \\
\hline
\hline
\end{tabular}
\end{center}
\end{table}

Для регрессии параметр $\mu=\E[y|x]$ моделируется как 
\begin{equation}
\mu=g(x,\beta),
\end{equation}
для заданной функции  $g(\cdot)$, которая меняется в зависимости от модели (см. Раздел 5.7.4), частично в зависимости от ограничений на диапазон $y$ и, следовательно, $\mu$. Логарифм правдоподобия в таком случае:
\begin{equation}
\mathcal{L}_{N}(\beta)=\sum_{i=1}^{N} \{ a(g(x_i,\beta))+b(y_i)+c(g(x_i,\beta))y_i \},
\end{equation}
с условием первого порядка, которое может быть представлено иначе, с использованием вышеупомянутой информации о первых двух моментах $y$ в следующем виде:
\begin{equation}
\frac{\partial \mathcal{L}_{N}(\beta)}{\partial \beta}=\sum_{i=1}^{N} \frac{y_i-g(x_i,\beta)}{{\sigma_i}^2} \times \frac{\partial g(x_i,\beta)}{\partial \beta}=0,
\end{equation}
где ${\sigma_i}^2=[c'(g(x_i,\beta))]^{-1}$ --- предполагаемая функция дисперсии, соответствующая конкретному распределению из семейства. Например, для распределения Бернулли, экспоненциального и Пуассона ${\sigma_i}^2$ равны, соответственно, $g_i(1-g_i)$,$1/{g_i}^2$,$g_i$, где $g_i=g(x_i,\beta)$.

Оценка квази-ММП является решением этих уравнений, но уже не предполагается, что распределение правильно специфицирована. Гурьеру, Монфор, и Трогнон (1984а) доказали, что оценка квази-ММП $\hat{\beta}_{QML}$ состоятельна при условии, что $\E[y|x]=g(x,\beta_0)$. Это ясно из ожидаемого значения условия первого порядка (5.63), которое оценивается при $\beta=\beta_0$ и является взвешенной суммой ошибок $y-g(x,\beta_0)$ с ожидаемым значением, равным нулю, если $\E[y|x]=g(x,\beta_0)$.

Таким образом, оценка квази-ММП, основанная на распределении экспоненциального семейства, состоятельна, если только условное математическое ожидание $y$ при заданном $x$ правильно специфицировано. Следует отметить, что фактический процесс порождающий данные для $y$ не обязательно должен быть из экспоненциального семейства. Семейству принадлежит плотность спецификации, и она может быть потенциально неправильна указана.

Однако даже при правильном условном математическом ожидании корректировка базового результата ММП для дисперсии, стандартных ошибок и $t$-статистики, основанной на $-{A_0}^{-1}$, является оправданной. В общем случае нужно применять сэндвич-форму ${A_0}^{-1}B_0{A_0}^{-1}$, кроме ситуации, когда условная дисперсия $y$ при заданном $x$ тоже определена правильно, в этом случае $A_0=-B_0$. Однако для моделей Бернулли $A_0=-B_0$ верно всегда. Состоятельные стандартные ошибки можно получить с помощью (5.36) и (5.38).

Экспоненциальное семейство является особым случаем. Обычно неправильная спецификация любого аспекта плотности приводит к несостоятельности ММП-оценок. Даже в случае экспоненциального семейства оценки квази-ММП могут использоваться только для прогнозирования условного математического ожидания, тогда как с правильно специфицированной плотностью можно предсказать условное распределение.

\subsection{Обобщённые линейные модели}

Модели, основанные на экспоненциально семействе, называются обобщёнными линейными моделями (ОЛМ) в литературе статистики (см. книгу с таким названием Маккаллоу и Нельдера, 1989). Класс обобщённых линейных моделей наиболее широко используется в прикладной статистике для нелинейных пространственных регрессий, как следует из таблицы 5.3, они включают следующие регрессионные модели: нелинейный метод наименьших квадратов, модель Пуассона, геометрическую модель, пробит, логит, биномиальную модель с известным числом испытаний, гамма и экспоненциальная модель. Мы предлагаем краткий обзор с введением стандартной терминологии ОЛМ.

Стандартные ОЛМ определяют условное математическое ожидание $g(x,\beta)$ в (5.61) в более простой одноиндексной форме такой, чтобы $\mu=g(x'\beta)$. Тогда  $g^{-1}(\mu)=x'\beta$, а функция $g^{-1}(\cdot)$ называется функцией связи. Например, обычная спецификация для модели Пуассона соответствует логарифмической  функции связи, так как если $\mu=\exp(x'\beta)$, то $\ln \mu =x'\beta$.

Условия первого порядка (5.63) можно представить в виде $\sum_i [(y_i-g_i)/c'(g_i)]g'_i x_i=0$, где $g_i=g(x'_i\beta)$ и $g'_i=g'(x'_i\beta)$. Есть вычислительные преимущества в выборе функции связи, для которой $c'(g(\mu))=g'(\mu)$, поскольку тогда условия первого порядка сводятся к
$\sum_i (y_i-g_i)x_i=0$, и ошибка $(y_i-g_i)$ ортогональна регрессорам. Каноническая функция связи --- такая функция $g^{-1}(\cdot)$, что приводит к $c'(g(\mu))=g'(\mu)$ и меняется при изменении $c(\mu)$ и, следовательно, ОЛМ. Каноническая функция связи приводит к $\mu=x'\beta$ для нормально распределённных данных, к $\mu=\exp(x'\beta)$ для данных, распределённых по Пуассону и к $\mu=\exp(x'\beta)/[1+\exp(x'\beta)]$ для бинарных данных. Последняя спецификация --- это логит модель, приведённая ранее в таблице 5.3.

Отклонением называется удвоенная разница между максимально достижимой логарифмической функцией правдоподобия и оценённой логарифмической функцией правдоподобия. Это мера, которая обобщает сумму квадратов остатков в линейной регрессии для других регрессионных моделей экспоненциального семейства.

Модели, основанные на экспоненциальном семействе, являются довольно ограниченными, поскольку все моменты зависят только от одного параметра $\mu=g(x'\beta)$. В литературе рассматривают дополнения к моделям, в которых сделано  удобное предположение, что дисперсия распределения из экспоненциального семейства потенциально неправильно специфицирована на множитель $\alpha$, так что $\Var[y|x]=\alpha \times [c'(g(x,\beta))]^{-1}$, где обязательно $\alpha \not = 1$.

Например, для модели Пуассона пусть $\Var[y|x]=\alpha g(x,\beta)$, а не $g(x,\beta)$. При данной неправильной спецификации дисперсии можно показать, что $B_0=-\alpha A_0$, поэтому ковариационная матрица оценки квази-ММП это $-\alpha {A_0}^{-1}$, что требует только изменения масштаба обычной (не сэндвич) оценки ММП ковариационной матрицы $-{A_0}^{-1}$ с помощью умножения на $\alpha$. Часто используемая состоятельная оценка для $\alpha$ --- это $\hat{\alpha}=(N-K)^{-1} \sum_i(y_i-\hat{g_i})^{2}/ {\hat{\sigma_i}}^2$, где $\hat{g_i}=g(x_i,\hat{\beta}_{QML})$, ${\hat{\sigma_i}}^2=[c'(\hat{g_i})]^{-1}$ и берётся деление на $ N-K$, а не $N$, чтобы обеспечить лучшую оценку в малых выборках. См. предыдущие ссылки и Кэмерона и Триведи (1986, 1998) для дальнейших подробностей.

Многие статистические пакеты включают ОЛМ модуль, по умолчанию дающий стандартные ошибки, которые являются правильными при условии $\Var[y|x]=\alpha [c'(g(x,\beta))]^{-1}$. Кроме того, можно получить оценки с использованием ММП со стандартными ошибками, полученными с помощью скорректированной сэндвич формулы ${A_0}^{-1}B_0{A_0}^{-1}$. На практике сэндвич стандартные ошибки аналогичны тем, которые получены с применением простой коррекции ОЛМ. Ещё одним способом оценки ОЛМ является метод взвешенных нелинейных наименьших квадратов, описанный в конце Раздела 5.8.6.

\subsection{Оценки квази-ММП для многомерных зависимых переменных}

Эта глава посвящена скалярным зависимым переменным, но теория может применяться также к многомерному случаю. Предположим, что зависимая переменная $y$ --- вектор размера $m \times 1$ и данные $(y_i,x_i)$, $i=1,\dots, N$ независимы по $i$. Примеры, приведённые в последующих главах, включают внешне несвязанные уравнения, панельные данные с $m$ наблюдениями для $i$-го индивида для той же зависимой переменной и кластерных данных, где данные для $i j$-ого наблюдения коррелированы по $m$ возможных значений индекса $j$.

При заданной спецификации $f(y|x,\theta)$ совместной функции плотности $y=(y_1, \dots, y_m)$ при условии $x$, полностью эффективная оценка ММП максимизирует $N^{-1} \sum_i \ln f(y_i|x_i,\theta)$, как отмечалось после (5.39). Тем не менее, в многомерном случае совместная плотность $y$ может быть сложной. Возможна более простая оценка при знании только $m$ одномерных плотностей $f_j(y_j|x,\theta)$, $j=1,\dots,m$, где $y_j$ --- $j$-ый элемент $y$. Например, для многомерных данных можно было бы работать с $m$ независимыми одномерными отрицательными биномиальными вероятностями для каждого значения, а не с более богатой многофакторной моделью, в которой возможна корреляция.

Рассмотрим  оценку квази-ММП $\hat{\theta}_{QML}$, основанную на произведении одномерных плотностей $\Pi_j f_j(y_j,|x,\theta)$, которая максимизирует
\begin{equation}
\mathcal{Q}_{N}(\theta)=\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{m} \ln f(y_{ij}|x_i,\theta).
\end{equation}
Вулдридж (2002) называет эту оценку частичной оценкой ММП, так как плотность была лишь частично специфицирована. 

Частичная оценка ММП --- это М-оценка с $q_i=\sum_j \ln f(y_{ij}|x_i,\theta)$. Необходимое условие состоятельности (5.25) требует, чтобы $\E[\sum_j \partial f(y_{ij}|x_i,\theta) / \partial \theta|_{\theta_0}]=0$. Это условие выполняется, если предельные плотности $f(y_{ij}|x_i,\theta)$ правильно специфицированы, поскольку тогда в силу условия регулярности (5.41) $\E[ \partial f(y_{ij}|x_i,\theta)/ \partial \theta|_{\theta_0}]=0$ .

Таким образом, частичная оценка ММП состоятельна при условии, что одномерные плотности $f_j(y_j|x,\theta)$ являются правильно специфицированными. Состоятельность не требует, чтобы  $f(y|x,\theta)= \Pi_j f_j(y_j,|x,\theta)$. Однако зависимость $y_1, \dots, y_m$  приведёт к невыполнению равенства информационных матриц, поэтому стандартные ошибки должны быть вычислены с использованием сэндвич-формы для ковариационной матрицы с
\begin{equation}
A_0= \left. \frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{m} \frac{\partial^2 \ln  f_{ij}}{ \partial \theta \partial \theta'} \right|_{\theta_0}, 
\end{equation}
\[
B_0= \left. \frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{m} \sum_{k=1}^{m} \frac{\partial \ln  f_{ij}}{\partial \theta} \right|_{\theta_0} \left. \frac{\partial \ln  f_{ik}}{\partial \theta'} \right|_{\theta_0} 
\]

где $f_{ij}=f(y_{ij}|x_i,\theta)$. Кроме того, частичная оценка ММП является неэффективной по сравнению с оценкой ММП, основанной на совместной плотности. Дальнейшее обсуждение приведено в Разделах 6.9 и 6.10.

\section{Нелинейный метод наименьших квадратов}

Оценка нелинейного метода наименьших квадратов является естественным продолжением оценки линейного метода наименьших квадратов для нелинейной модели с $\E[y|x]=g(x,\beta)$, где $g(\cdot)$ является нелинейной по $\beta$. Анализ и результаты являются по существу такими же, как для метода линейных наименьших квадратов с одним изменением, что в формуле для ковариационной матрицы вектор регрессоров $x$ заменяется на $\partial g(x,\beta)/ \partial \beta|_{\hat{\beta}}$, производную функции условного математического ожидания при $\beta=\hat{\beta}$.

Для микроэконометрического анализа контроль за гетероскедастичностью ошибок может оказаться необходимым так же, как и в линейном случае. Оценки НМНК и обобщения, которые моделируют гетероскедастичные ошибки, как правило, менее эффективны, чем оценки ММП, но они широко используются в микроэконометрике, потому что они полагаются на более слабые предположения о распределении.

\begin{table}[h]
\begin{center}
\caption{\label{tab:NLS} Нелинейный метод наименьших квадратов: распространённые примеры}
\begin{tabular}[t]{ll}
\hline
\hline
\bf{Модель} & \bf{Функция регрессии} $g(x,\beta)$ \\
\hline
Экспоненциальная & $\exp(\beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3)$ \\
Регрессор в степени & $\beta_1 x_1 + \beta_2 x_2^{\beta_3}$ \\
Производственная & $\beta_1 {x_1}^{\beta_2} {x_2}^{\beta_3}$\\ 
функция Кобба-Дугласа & \\
Производственная функция & $[\beta_1 {x_1}^{\beta_3} + \beta_2 x_2^{\beta_3}]^{1/\beta_3}$\\
с постоянной эластичностью замещения & \\ 
Нелинейные ограничения & $\beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3$, where $\beta_3=-\beta_2 \beta_1$ \\
\hline
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Нелинейная модель регрессии}

Нелинейная модель регрессии определяет скалярную зависимую переменную $y$ так, чтобы её условное математическое ожидание равнялось
\begin{equation}
\E[y_i|x_i]=g(x_i,\beta),
\end{equation}
где $g(\cdot)$ является заданной функцией, $x$ --- вектор объясняющих переменных, а $\beta$ --- вектор параметров размера $k \times 1$. Линейная модель регрессии в Главе 4 является частным случаем $g(x,\beta)=x'\beta$.

Наиболее распространённые причины спецификации нелинейной функции для $\E[y|x]$ включают ограничение множества значений (например, для обеспечения того, $\E[y|x]>0$) и спецификацию моделей спроса или предложения, стоимости или расходов, удовлетворяющих ограничениям из теории производства или потребления. Некоторые часто используемые нелинейные модели регрессий приведены в таблице 5.5.

\subsection{Оценки нелинейного метода наименьших квадратов}

Остатки определяются как разница между зависимой переменной и её условным математическим ожиданием, $y_i-g(x_i,\beta)$. Оценка нелинейного метода наименьших квадратов $\hat{\beta}_{NLS}$ минимизирует сумму квадратов остатков, $\sum_i (y_i-g(x_i,\beta))^{2}$, или, что эквивалентно, максимизирует
\begin{equation}
\mathcal{Q}_{N}(\beta)= - \frac{1}{2N} \sum_{i=1}^{N} (y_i-g(x_i,\beta))^{2},
\end{equation}
где коэффициент $1/2$ введён для упрощения последующего анализа.

Дифференциирование даёт условия первого порядка для НМНК:
\begin{equation}
\frac {\partial \mathcal{Q}_{N}(\beta)} {\partial \beta}= \frac{1}{N} \sum_{i=1}^{N} \frac{\partial g_i}{\partial \beta}(y_i-g_i)=0,
\end{equation}
где $g_i=g(x_i,\beta)$. Эти условия требуют, чтобы остатки $(y-g)$ были ортогональны $\partial g/ \partial \beta$, а не $x$, как и в линейном случае. Здесь нет явного решения для $\hat{\beta}_{NLS}$, которая вместо этого вычисляется с использованием итерационных методов (которые есть в Главе 10).

Нелинейная модель регрессии может быть более компактно представлена в матричном виде. Записав наблюдения одно под другим получим следующее:
\begin{equation}
\begin{bmatrix} y_1 \\ \cdot \\ \cdot \\ \cdot \\ y_N \end{bmatrix} = \begin{bmatrix} g_1 \\ \cdot \\ \cdot \\ \cdot \\ g_N \end{bmatrix} + \begin{bmatrix} u_1 \\ \cdot \\ \cdot \\ \cdot \\ u_N \end{bmatrix},
\end{equation}
где $g_i=g(x_i,\beta)$ или, что, эквивалентно,
\begin{equation}
y=g+u,
\end{equation}
где $y$, $g$ и $u$ --- векторы размера $N \times 1$ с $i$-ыми элементами $y_i$, $g_i$ и $u_i$ соответственно.
Тогда
\[
\mathcal{Q}_{N}(\beta)=- \frac{1}{2N} (y-g)'(y-g)
\]
и
\begin{equation}
\frac {\partial \mathcal{Q}_{N}(\beta)} {\partial \beta}= \frac{1}{N} \frac{\partial g'}{\partial \beta}(y-g),
\end{equation}
где
\begin{equation}
\frac{\partial g'}{\partial \beta}= \begin{matrix} \begin{bmatrix} 
\frac{\partial g_1}{\partial \beta_1} & \cdots & \frac{\partial g_N}{\partial \beta_1}\\
\cdot &  & \cdot \\
\cdot &  & \cdot \\
\cdot &  & \cdot \\
\frac{\partial g_1}{\partial \beta_k} & \cdots & \frac{\partial g_N}{\partial \beta_k}\\
\end{bmatrix} \end{matrix}
\end{equation}
это матрица размера $K \times N$ частных производных $g(x,\beta)'$ по $\beta$.

\subsection{Распределение оценок нелинейного метода наименьших квадратов}

Распределение оценки НМНК будет меняться в зависимости от процесса порождающего данные. Процесс порождающий данные всегда может быть записан в следующем виде:
\begin{equation}
y_i=g(x_i,\beta_0)+u_i,
\end{equation}
в виде нелинейной регрессионной модели с добавочным членом --- ошибкой $u$. Условное математическое ожидание правильно специфицировано, если $\E[y|x]=g(x,\beta_0)$ для процесса порождающего данные. Тогда ошибка должна удовлетворять условию: $\E[u|x]=0$.

Учитывая условия первого порядка НМНК (5.68), необходимое условие состоятельности (5.25) становится
\[
\E[\partial g(x,\beta) / \partial \beta|_{\beta_0} \times (y-g(x_i,\beta_0))]=0.
\]

Учитывая (5.73), это равносильно условию $\E[\partial g(x,\beta) / \partial \beta|_{\beta_0} \times u]=0$. Это верно, если $\E[u|x]=0$, таким образом, состоятельность требует правильной спецификации условного математического ожидания, как и в линейном случае. Если вместо этого $\E[u|x] \not = 0$, то для состоятельной оценки требуется применение нелинейных инструментальных методов (которые представлены в Разделе 6.5).

Предельное распределение $\sqrt{N}(\hat{\beta}_{NLS}-\beta_0)$ получают с использованием точного разложения первого порядка в ряд Тейлора условий первого порядка (5.68). Получаем
\[
\sqrt{N}(\hat{\beta}_{NLS}-\beta_0)=-\left( -\frac{1}{N}  \sum_{i=1}^{N} \frac{\partial g_i}{\partial \beta} \frac{\partial g_i}{\partial \beta'} + \left. \frac{1}{N} \frac{\partial^2 g_i}{\partial \beta \partial \beta'} (y_i-g_i) \right|_{\beta^+} \right) ^{-1} \left. \frac{1}{\sqrt{N}} \sum_{i=1}^{N} \frac{\partial g_i}{\partial \beta} u_i \right|_{\beta_0},
\]
для некоторого $\beta^+$  между $\hat{\beta}_{NLS}$ и $\beta_0$. Для $A_0$ в (5.18) возможно упрощение, так как член, включающий $(\partial^2 g/ \partial \beta \partial \beta')$ исчезает, поскольку $\E[u|x]=0$. Поэтому нам нужно рассматривать асимптотически только
\[
\sqrt{N}(\hat{\beta}_{NLS}-\beta_0)= \left( \left.\frac{1}{N} \sum_{i=1}^{N} \frac{\partial g_i}{\partial \beta} \frac{\partial g_i}{\partial \beta'} \right|_{\beta_0}  \right) ^{-1} \left. \frac{1}{\sqrt{N}} \sum_{i=1}^{N} \frac{\partial g_i}{\partial \beta} u_i \right|_{\beta_0},
\]
Эта формула совпадает с формулой для МНК, см. Раздел 4.4.4, за исключением того, что $x_i$ заменяется на $\partial g_i / \partial \beta'|_{\beta_0}$.

Получаем утверждение, аналогичное утверждению 4.1 для МНК-оценки.

\begin{proposition}[Распределение оценок НМНК]: Сделаем следующие предположения:
\begin{enumerate}
\item Модель задана уравнением (5.73), то есть $y_i=g(x_i,\beta_0)+u_i$.
\item В процессе порождающем данные $\E[u_i|x_i]=0$ и $\E[uu'|X]=\Omega_0$, где $\Omega_{0,ij}=\sigma_{ij}$.
\item Функция математического ожидания  $g(\cdot)$ удовлетворяет $g(x,\beta^{(1)})=g(x,\beta^{(2)})$, если и только если $\beta^{(1)}=\beta^{(2)}$.
\item Матрица
\begin{equation}
A_0= \left. \plim \frac{1}{N} \sum_{i=1}^{N} \frac{\partial g_i}{\partial \beta} \frac{\partial g_i}{\partial \beta'} \right|_{\beta_0} =\plim \left. \frac{1}{N} \frac{\partial g'}{\partial \beta} \frac{\partial g'}{\partial \beta'} \right|_{\beta_0}
\end{equation}
существует и конечная невырожденная.
\item $N^{-1/2} \sum_{i=1}^{N} \partial g_i/ \partial \beta \times u_i|_{\beta_0} \xrightarrow{d} \mathcal{N}[0,B_0]$, где
\begin{equation}
B_0= \left. \plim \frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{N} \sigma_{ij} \frac{\partial g_i}{\partial \beta} \frac{\partial g_j}{\partial \beta'} \right|_{\beta_0} = \left. \plim \frac{1}{N} \frac{\partial g'}{\partial \beta} \Omega_0 \frac{\partial g'}{\partial \beta'} \right|_{\beta_0}.
\end{equation}
\end{enumerate}

Тогда оценка НМНК $\hat{\beta}_{NLS}$, которая определяется как решение условий первого порядка $\partial N^{-1} \mathcal{Q}_{N}(\beta)/ \partial \beta=0$, состоятельна для $\beta_0$ и

\begin{equation}
\sqrt{N}(\hat{\beta}_{NLS}-\beta_0) \xrightarrow{d} \mathcal{N}[0,{A_0}^{-1}B_0{A_0}^{-1}].
\end{equation}
\end{proposition}

Из условий с 1 по 3 следует, что функция регрессии правильно специфицирована и регрессоры не коррелируют с ошибками и что определён $\beta_0$. Ошибки могут быть гетероскедастичны и коррелированы по $i$. Условия 4 и 5 подразумевают релевантные предельные результаты, необходимые для применения теоремы 5.3. Для выполнения условия 5 необходимо, чтобы были наложены некоторые ограничения на корреляцию ошибок по $i$. Пределы по вероятности в (5.74) и (5.75) для $X$ становятся обычными пределами, если $X$ нестохастические.

Матрицы $A_0$ и $B_0$ в предложении 5.6 такие же, как и матрицы $M_{XX}$ и $M_{X \Omega X}$ в Разделе 4.4.4 для оценок МНК при $x_i$ заменённом на $\partial g_i/\partial \beta|_{\beta_0}$. Асимптотическая теория для НМНК такая же, как и для МНК, не считая этого единственного изменения.

В частном случае сферических ошибок, $\Omega_0={\sigma_0}^{2}I$, поэтому $B_0={\sigma_0}^{2}A_0$ и $\Var[\hat{\beta}_{NLS}]={\sigma_0}^{2}{A_0}^{-1}$. Тогда оценки нелинейного метода наименьших квадратов асимптотически эффективны среди оценок наименьших квадратов. Тем не менее, для пространственных данных ошибки не обязательно гетероскедастичны.

С учётом утверждения 5.6 получающееся асимптотическое распределение оценки НМНК может быть выражено следующим образом:
\begin{equation}
\hat{\beta}_{NLS} \stackrel{a}{\sim} \mathcal{N}[\beta,(D'D)^{-1} D' \Omega_0 D (D'D)^{-1}],
\end{equation}
где производная матрицы $D=\partial g / \partial \beta'|_{\beta_0}$ имеет $i$-ую строку $\partial g_i/ \partial \beta'|_{\beta_0}$ (см. (5.72)), для простоты обозначений указание точки $\beta_0$ опускается, и мы предполагаем, что ЗБЧ может быть применён, чтобы оператор $\plim$ в определениях $A_0$ и $B_0$ был заменён на $\lim E$, а затем убираем знак предела. Этот приём часто используется в последующих главах.

\subsection{Ковариационная матрица оценок НМНК}

Мы рассматриваем статистические выводы для обычной ситуации в микроэконометрике с независимыми ошибками с гетероскедастичностью неизвестной функциональной формы. Это требует состоятельной оценки ${A_0}^{-1}B_0{A_0}^{-1}$, определённой в утверждении 5.6. 

Для $A_0$, определённой в (5.74), можно непосредственно использовать очевидную оценку
\begin{equation}
\hat{A}= \left. \frac{1}{N} \sum_{i=1}^{N} \frac{\partial g_i}{\partial \beta} \right|_{\hat{\beta}} \left. \frac{\partial g_i}{\partial \beta'} \right|_{\hat{\beta}},
\end{equation}
поскольку в $A_0$ не используются моменты ошибок.

Учитывая независимость по $i$ двойная сумма в $B_0$, определённая в (5.75), упрощается до одной суммы:
\[
B_0= \left. \plim \frac{1}{N} \sum_{i=1}^{N} {\sigma_i}^2 \frac{\partial g_i}{\partial \beta} \frac{\partial g_i}{\partial \beta'} \right|_{\beta_0}.
\]
Что касается МНК-оценки (см. Раздел 4.4.5) необходимо только состоятельно оценить сумму матриц $B_0$ размера $K \times K$. Это не требует состоятельности оценки ${\sigma_i}^2$, $N$ отдельных компонентов в сумме.

Уайт (1980b) определил условия, при которых
\begin{equation}
\hat{B}= \left. \frac{1}{N} \sum_{i=1}^{N} {\hat{u}_i}^{2} \frac{\partial g_i}{\partial \beta} \frac{\partial g_i}{\partial \beta'}\right|_{\hat{\beta}}= \left. \frac{1}{N}  \frac{\partial g'}{\partial \beta}|_{\hat{\beta}} \hat{\Omega} \frac{\partial g}{\partial \beta'}\right|_{\hat{\beta}} 
\end{equation}
состоятельна для $B_0$, где $\hat{u}_{i}=y_i-g(x_i,\hat{\beta})$, $\hat{\beta}$ состоятельна для $\beta_0$ и
\begin{equation}
\hat{\Omega}=\Diag[{\hat{u}_i}^{2}].
\end{equation}
Мы получаем следующую устойчивую к гетероскедастичности оценку асимптотической ковариационной матрицы оценки НМНК:
\begin{equation}
\widehat{\Var}[\hat{\beta}_{NLS}]=(\hat{D'}\hat{D})^{-1} \hat{D'} \hat{\Omega} \hat{D} (\hat{D'}\hat{D})^{-1},
\end{equation}
где $\hat{D}=\partial g/ \partial \beta'|_{\hat{\beta}}$. Это уравнение такое же, как в случае МНК в Разделе 4.4.5 при замене матрицы регрессоров $X$ на $\hat{D}$. На практике может быть использована коррекция степеней свободы, чтобы $\hat{B}$ в формуле (5.79) вычислялось с помощью деления на $(N-K)$, а не $N$. Тогда правую часть в (5.81) следует умножить на $N/(N-K)$.

Обобщение для ошибок, коррелированных по $i$, приведено в Разделе 5.8.7.

\subsection{Пример экспоненциальной регрессии}

В качестве примера предположим, что $y$ при заданном $x$ имеет экспоненциальное условное математическое ожидание, чтобы $\E[y|x]=\exp(x'\beta)$. Модель может быть выраженa в виде нелинейной регрессии с
\[
y=\exp(x'\beta)+u,
\]
где ошибка $u$ имеет $\E[u|x]=0$ и ошибки потенциально гетероскедастичны.

Оценки НМНК удовлетворяют следующим условиям первого порядка:
\begin{equation}
N^{-1} \sum_i (y_i-\exp(x'_i\beta))\exp(x'_i\beta)x_i=0,
\end{equation}
таким образом, состоятельность $\hat{\beta}_{NLS}$ требует только, чтобы условное математическое ожидание было правильно специфицировано с $\E[y|x]=\exp(x'\beta_0)$. Здесь $\partial g/ \partial \beta = \exp(x'\beta)x$, поэтому результат обобщённого НМНК (5.81) даёт оценки, устойчивые к гетероскедастичности:
\begin{equation}
\widehat{\Var}[\hat{\beta}_{NLS}]=\left( \sum_i e^{2 x'_i \hat{\beta}} x_i x'_i \right) ^{-1} \sum_i {\hat{u}_i}^{2} e^{2 x'_i \hat{\beta}} x_i x'_i \left( \sum_i e^{2 x'_i \hat{\beta}} x_i x'_i \right) ^{-1},
\end{equation}
где $\hat{u}_i=y_i-\exp(x'_i \hat{\beta}_{NLS})$.

\subsection{Взвешенный НМНК и допустимый обобщенный НМНК}

Для пространственных данных ошибки часто гетероскедастичны. Тогда допустимый обобщённый НМНК, который делает поправку на гетероскедастичность, более эффективен, чем НМНК.

Допустимый обобщённый нелинейный метод наименьших квадратов (ДОНМНК) всё же менее эффективен, чем ММП. Заметным исключением является то, что оценки ДОНМНК асимптотически эквивалентны оценкам ММП при условной плотности $y$ принадлежащей экспоненциальному семейству. Частным случаем этого исключения является тот факт, что оценка ДОМНК асимптотически эквивалентна оценке ММП в линейной регрессии при нормальности ошибок.

\begin{table}[h]
\begin{center}
\caption{\label{tab:NLSdisp} Оценки нелинейного метода наименьших квадратов и оценки их асимптотических ковариационных матриц}
\begin{minipage}{16cm}
\begin{tabular}[t]{llc}
\hline
\hline
\bf{Оценка}\footnote{Берутся функции для нелинейной регрессионной модели с ошибкой $u=y-g$, определённой в (5.70), и условной ковариационной матрицей остатков $\Omega$. $\hat{D}$ является производной от вектора условного математического ожидания в зависимости от $\beta'$ в точке $\hat{\beta}$. Для ДОНМНК предполагается, что $\hat{\Omega}$ состоятельна для $\Omega$. Для НМНК и взвешенного НМНК используются ковариационные матрицы, устойчивые к гетероскедастичности, $\hat{\Omega}$ равна диагональной матрице с квадратами остатков на диагонали, оценка, которая не обязана быть состоятельной для $\Omega$.} & \bf{Целевая функция} & \bf{Оценочная асимптотическая} \\
& & \bf{ковариационная матрица}\\ 
\hline
НМНК & $\mathcal{Q}_{N}(\beta)=\frac{-1}{2N}u'u$ & $(\hat{D'}\hat{D})^{-1} \hat{D'} \hat{\Omega} \hat{D} (\hat{D'}\hat{D})^{-1}$\\
ДОНМНК & $\mathcal{Q}_{N}(\beta)=\frac{-1}{2N}u' \Omega(\hat{\gamma})^{-1} u$ & $(\hat{D'} {\hat{\Omega}}^{-1} \hat{D})^{-1}$ \\
взвешенный НМНК & $\mathcal{Q}_{N}(\beta)=\frac{-1}{2N}u' \hat{\Sigma}^{-1} u$ & $(\hat{D'} \hat{\Sigma}^{-1} \hat{D})^{-1} \hat{D'} \hat{\Sigma}^{-1} \hat{\Omega} \hat{\Sigma}^{-1} \hat{D} (\hat{D'} \hat{\Sigma}^{-1} \hat{D})^{-1}$ \\ 
\hline
\hline
\end{tabular}
\end{minipage}
\end{center}
\end{table}

Если гетероскедастичность неправильно смоделирована, тогда оценка ДОНМНК остаётся состоятельной, но в таком случае нужно получить стандартные ошибки, которые являются устойчивыми к неправильной спецификации гетероскедастичности. Анализ очень похож на анализ для линейной модели, приведённый в Разделе 4.5.

\begin{center}
Допустимый обобщённый нелинейный метод наименьших квадратов
\end{center}

Оценка допустимого обобщённого нелинейного метода наименьших квадратов $\hat{\beta}_{FGNLS}$ максимизирует
\begin{equation}
\mathcal{Q}_{N}(\beta)=-\frac{1}{2N}(y-g)' \Omega(\hat{\gamma})^{-1} (y-g),
\end{equation}
где предполагается, что $\E[uu'|X]=\Omega(\gamma_0)$ и $\hat{\gamma}$ является состоятельной оценкой для $\gamma_0$.

Если наши предположения, сделанные для оценки НМНК, удовлетворены и фактически $\Omega_0=\Omega(\gamma_0)$, то оценка ДОНМНК является состоятельной и асимптотически нормальной с оценочной асимптотической ковариационной матрицей, приведённой в таблице 5.6. Оценки ковариационной матрицы аналогичны оценкам линейного ДОМНК, $[X'\Omega(\hat{\gamma})^{-1}X]^{-1}$, только $X$ заменяется на $\hat{D}=\partial g / \partial \beta'|_{\hat{\beta}}$. 

Оценка ДОНМНК является наиболее эффективной состоятельной оценкой, которая минимизирует
квадратичную функцию потерь вида $(y-g)'V(y-g)$, где $V$ --- матрица весов. 

В целом реализация ДОНМНК требует обращения матрицы $\Omega(\hat{\gamma})$ размера $N \times N$. Это может быть вычислительно невозможно для больших $N$, но на практике $\Omega(\hat{\gamma})$ обычно имеет структуру такую, как диагональность, что приводит к возможности найти обратную матрицу в явном виде.

\begin{center}
Взвешенный НМНК
\end{center}

Подход ДОНМНК полностью эффективен, но приводит к неправильным стандартным оценкам ошибок, если модель для $\Omega_0$ будет неправильно специфицирована. Здесь мы рассмотрим подход для НМНК и ДОНМНК, который определяет модель для ковариационной матрицы ошибок и затем получает скорректированные стандартные ошибки. Обсуждение похоже на то, которое представлено в Разделе 4.5.2.

Оценка взвешенного нелинейного метода наименьших квадратов (ВНМНК) $\hat{\beta}_{WGNLS}$ максимизирует
\begin{equation}
\mathcal{Q}_{N}(\beta)=-\frac{1}{2N}(y-g)' \hat{\Sigma}^{-1} (y-g),
\end{equation}
где $\Sigma = \Sigma (\gamma)$  является рабочей ковариационной матрицей ошибок $\hat{\Sigma}=\Sigma(\hat{\gamma})$, где $\hat{\gamma}$ является оценкой $\gamma$, и в отличие от ДОНМНК $\Sigma \not = \Omega_0$.

При предположениях, аналогичных тем, что были введены для оценок НМНК, и предполагая, что $\Sigma_0=\plim \hat{\Sigma}$, оценки ВНМНК являются состоятельными и асимптотически нормальными с оценочными асимптотическими ковариационными матрицами, приведёнными в таблице 5.6.

Эта оценка называется ВНМНК, чтобы отличить её от оценки ДОНМНК, которая предполагает, что $\Sigma=\Omega_0$. Оценки ВНМНК находятся между НМНК и ДОНМНК с точки зрения эффективности, хотя они могут быть менее эффективными, чем оценки НМНК, если  выбрана неудачная модель ковариационной матрицы ошибки. НМНК и МНК оценки являются частными случаями  ВНМНК с $\Sigma=\sigma^{2}I$.

\begin{center}
Гетероскедастичные ошибки
\end{center}

Очевидной рабочей моделью для гетероскедастичности является ${\sigma_i}^{2}=\E[{u_i}^2|x_i]=\exp(z'_i\gamma_0)$, где вектор $z$ является заданной функцией от $x$ (например, выбранные подкомпоненты $x$), и использование экспоненты обеспечивает положительную дисперсию.

Тогда $\Sigma=\Diag[\exp(z'_i\gamma)]$ и $\hat{\Sigma}=\Diag[\exp(z'_i\hat{\gamma})]$ , где $\hat{\gamma}$ может быть получено из нелинейной регрессии квадратов остатков НМНК $(y_i-g(x_i,\hat{\beta}_{NLS}))^2$ на $\exp(z'_i\gamma)$. Поскольку $\Sigma$ диагональная, $\Sigma^{-1}=\Diag[1/{\sigma_i}^{2}]$. Тогда (5.84) упрощается и оценки ВНМНК максимизирует
\begin{equation}
\mathcal{Q}_{N}(\beta)=-\frac{1}{2N} \sum_{i=1}^{N} \frac{(y_i-g(x_i,\beta))^2}{\hat{\sigma_i}^{2}}.
\end{equation}

Ковариационные матрицы оценок ВНМНК, приведённые в таблице 5.6, дают следующее:
\begin{equation}
\widehat{\Var}[\hat{\beta}_{WNLS}]= \left( \sum_{i=1}^{N} \frac{1}{\hat{\sigma_i}^{2}} \hat{d}_i \hat{d'}_i \right) ^{-1} \left( \sum_{i=1}^{N} {\hat{u}_i}^2 \frac{1}{\hat{\sigma_i}^{4}} \hat{d}_i \hat{d'}_i \right) \left( \sum_{i=1}^{N} \frac{1}{\hat{\sigma_i}^{2}} \hat{d}_i \hat{d'}_i \right) ^{-1},
\end{equation}
где $\hat{d}_i=\partial g(x_i,\beta) / \partial \beta|_{\hat{\beta}}$ и $\hat{u}_i=y_i-g(x_i,\hat{\beta}_{WNLS})$ --- остатки. На практике поправка на степени свободы может быть использована, чтобы правая часть (5.87) умножалась на $N/(N-K)$. Если сделано сильное предположение, что $\Sigma=\Omega_0$, то ВНМНК становится ДОНМНК и 
\begin{equation}
\widehat{\Var}[\hat{\beta}_{FGNLS}]=\left( \sum_{i=1}^{N} \frac{1}{\hat{\sigma_i}^{2}} \hat{d}_i \hat{d'}_i \right) ^{-1}.
\end{equation}

Оценки ВНМНК и ДОНМНК могут быть реализованы с использованием НМНК программ. Во-первых, сделаем НМНК регрессию $y_i$ на $g(x_i,\beta)$. Во-вторых, получим $\hat{\gamma}$, например, с помощью НМНК регрессии $(y_i-g(x_i,\hat{\beta}_{NLS}))^{2}$ на $\exp(z'_i\gamma)$, если ${\sigma_i}^{2}=\exp(z'_i\gamma)$. В-третьих, выполним НМНК регрессию $y_i/\hat{\sigma}_i$ на $g(x_i,\beta)/\hat{\sigma}_i$, где $\hat{\sigma_i}^{2}=\exp(z'_i\hat{\gamma})$. Это эквивалентно максимизации (5.86). Скорректированные сэндвич стандартные ошибки Уайта из этой преобразованной регрессии дают скорректированные стандартные ошибки ВНМНК, основанные на (5.87). Обычные нескорректированные стандартные ошибки из этой преобразованной регрессии дают стандартные ошибки ДОНМНК, основанные на (5.88).

С гетероскедастичными ошибками очень заманчиво сделать ещё один шаг вперёд и попробовать сделать ДОНМНК с использования $\hat{\Omega}=\Diag[{\hat{u}_i}^2]$. Однако это даст несостоятельную оценку параметра $\beta_0$, поскольку ДОНМНК регрессия $y_i$ на $g(x_i,\beta)$ сводится к регрессии НМНК $y_i/|\hat{u}_i|$ на $g(x_i,\beta)/|\hat{u}_i|$. Метод страдает от фундаментальной проблемы корреляции регрессоров и ошибок. Альтернативные полупараметрическое методы, которые дают такие же эффективные оценки, как оценки МНК, без уточнения функциональной формы $\Omega_0$, представлены в разделе 9.7.6.

\begin{center}
Обобщённые линейные модели
\end{center}

Реализация взвешенного НМНК требует разумной спецификации для рабочей матрицы. Подход ad-hoc, который уже был представлен, заключается в том, чтобы ${\sigma_i}^{2}=\exp(z'_i\gamma)$, где $z$ часто является подмножеством $x$. Например, в регрессии заработка на образование и на другие контрольные переменные мы можем моделировать гетероскедастичность в виде функции нескольких регрессоров, прежде всего школьного образования.

Некоторые типы пространственных данных приводят к очень экономной естественной модели гетероскедастичности. Например, для счетных данных при использовании распределения Пуассона дисперсия равна математическому ожиданию, то есть ${\sigma_i}^{2}=g(x_i,\beta)$. Это даёт рабочую модель гетероскедастичности, которая не вносит никаких дополнительных параметров, чем те, которые уже используются в моделировании условного математического ожидания.

Этот подход, подразумевающий, что рабочая модель для дисперсии --- функция математического ожидания, естественно возникает для обобщённой линейной модели, введённой в Разделах 5.7.3 и 5.7.4. Из (5.63) условия первого порядка для оценки квази-ММП на основе распределения экспоненциального семейства имеют следующий вид:
\[
\sum_{i=1}^{N} \frac{y_i-g(x_i,\beta)}{{\sigma_i}^{2}} \times \frac{\partial g(x_i,\beta)}{\partial \beta}=0,
\]
где ${\sigma_i}^2=[c'(g(x_i,\beta))]^{-1}$ --- предполагаемая функция дисперсии, соответствующая конкретной ОЛМ (см. (5.60)). Например, для Пуассона, Бернулли и экспоненциального распределений ${\sigma_i}^2$ равны, соответственно, $g_i$, $g_i(1-g_i)$ и $1/(g_i)^2$, где $g_i=g(x_i,\beta)$.

Эти условия первого порядка могут быть решены для $\beta$ в один шаг, который допускает зависимость ${\sigma_i}^2$ от $\beta$. В более простом двухшаговом методе вычисляют $ \hat{\sigma_i}^2=c'(g(x_i,\hat{\beta}))$ при исходной оценке НМНК $\beta$, а затем выполняют взвешенную регрессию НМНК $y_i/\hat{\sigma}_i$ на $g(x_i,\beta)/\hat{\sigma}_i$. Полученная оценка $\beta$ асимптотически эквивалентна оценке квази-ММП, которая является непосредственно решением (5.63) (см. Гурьеру, Монфор, и Трогнан 1984a, или Кэмерон и Триведи, 1986). Таким образом ДОНМНК асимптотически эквивалентна оценке ММП, когда распределение принадлежит экспоненциальному семейству. Чтобы предотвратить неправильную спецификацию ${\sigma_i}^2$, следует использовать скорректированные сэндвич стандартные ошибки, или взять $\hat{\sigma_i}^2=\hat{\alpha}[c'(g(x_i,\beta))]^{-1}$, где оценка $\hat{\alpha}$ приведена в Разделе 5.7.4.

\subsection{Временные ряды}

Общий результат НМНК в предложении 5.6 распространяется на все типы данных, в том числе на  временные ряды. Дальнейшие результаты по оценкам ковариационных матриц сосредоточены на пространственном случае гетероскедастичных ошибок, но они легко адаптируются на случай временных рядов с автокорреляцией ошибок. Действительно, результаты о робастном оценивании ковариационной матрицы с использованием спектральных  методов для случая временных рядов предшествовали тем, которые использовались для пространственного случая.

Нелинейная регрессионная модель временных рядов:
\[
y_t=g(x_t,\beta)+u_t, t=1, \dots, T.
\]
Если ошибки $u_t$ автокоррелированны, часто используют авторегрессию скользящего среднего или модель $ARMA(p,q)$:
\[
u_t=\rho_{1} u_{t-1}+\cdots+\rho_{p} u_{t-p} + \e_t + \alpha_{1} \e_{t-1}+\cdots+\alpha_{q} \e_{t-q},
\]
где $\e_t$ является одинаково и независимо распределённым с математическим ожиданием 0 и дисперсией $\sigma^2$, и могут быть введены ограничения на параметры $ARMA$ модели для обеспечения стационарности и обратимости. Модель ошибок $ARMA$ означает конкретную структуру ковариационной матрицы ошибок $\Omega_0=\Omega(\rho,\alpha)$.

$ARMA$ модель представляет собой хорошую модель для $\Omega_0$ для случая временных рядов. В отличие от временных рядов, в пространственном случае, гораздо труднее правильно смоделировать гетероскедастичность, поэтому в нём большее внимание уделяется робастным методам, которые не требуют спецификации модели для $\Omega_0$.

Что делать, если ошибки и гетероскедастичны, и коррелированы? Оценка НМНК состоятельна  хотя и неэффективна, если ошибки коррелированы при условии, что $x_t$ не включает лаговые зависимые переменные, ведь в этом случае она становится несостоятельной. Уайт и Домовитс (1984) обобщили (5.79), чтобы получить скорректированную оценку ковариационной матрицы оценки НМНК при гетероскедастичности и автокорреляции в неизвестной функциональной форме, предполагая, что корреляция отлична от нуля не более, чем, скажем, на $l$ лагов. На практике используются незначительные уточнения Ньюи и Веста (1987b). Это уточнение --- изменение масштаба, что гарантирует, что оценка ковариационной матрицы неотрицательно определена. Были предложены и несколько других уточнений, и предположение о фиксированной длине лага было смягчено. Например, допускается, что  $l \rightarrow \infty$ со скоростью существенно меньше, чем $N \rightarrow \infty$. Это условие допускает возможность $AR$ составляющей в ошибках.

\section{Пример: оценивание с помощью ММП и НМНК}

Оценки максимального правдоподобия и МНМК оценки, расчёт стандартных ошибок и интерпретация коэффициентов проиллюстрированы с помощью искусственных данных.

\subsection{Модель и оценки}
 
Экспоненциальное распределение используется для непрерывных положительных данных, особенно для данных по длительности, изучаемых в Главе 17. Экспоненциальная плотность задается функцией
\[
f(y)=\lambda e^{-\lambda y}, y>0, \lambda>0,
\]
с математическим ожиданием $1/\lambda$ и дисперсией $1/\lambda^2$. Введём регрессоры в эту модель, положив
\[
\lambda=\exp(x'\beta),
\]
при этом $\lambda>0$. Следует отметить, что
\[
\E[y|x]=\exp(-x'\beta)
\]
Вместо этого возможна альтернативная параметризация, $\E[y|x]=\exp(x'\beta)$, так что $\lambda=\exp(-x'\beta)$. Обратите внимание, что экспонента используется двумя различными способами: для плотности и для условного математического ожидания.

МНК-оценка регрессии $y$ на $x$ является несостоятельной, поскольку она соответствует прямой, когда функция регрессии на самом деле является экспоненциальной кривой.

Оценка ММП легко получается. Логистическая плотность --- $\ln f(y|x)=x'\beta-y\exp(x'\beta)$, что ведёт к следующим условиям первого порядка $N^{-1}\sum_i (1-y_{i}\exp(x'_i\beta))x_i=0$ или
\[
N^{-1}\sum_i \frac {y_{i}-\exp(-x'\beta)}{\exp(-x'\beta)}x_i=0.
\]

Чтобы оценить регрессию НМНК, отметим, что модель может также быть выражена как нелинейная регрессия с
\[
y=\exp(-x'\beta)+u,
\]
где ошибки $u$ имеют $\E[u|x]=0$, хотя они гетероскедастичны. Условия первого порядка для экспоненциального условного математического ожидания для этой модели, уже были приведены в (5.82) и приводят явно к оценке, отличной от оценки ММП, помимо изменения знака.

В качестве примера взвешенного НМНК предположим, что дисперсия ошибки пропорциональна  математическому ожиданию. Тогда рабочая дисперсия $\Var[y]=\E[y]$ и взвешенный метод наименьших квадратов может быть реализован с помощью оценки НМНК регрессии $y_{i}/\hat{\sigma}_i$ на $\exp(-x'_{i}\beta)/\hat{\sigma}_i$, где ${\hat{\sigma}_i}^2=\exp(-x'_{i}\hat{\beta}_{NLS})$. Эта оценка является менее эффективной, чем оценка ММП и может  быть более или менее эффективной, чем оценка НМНК.

Допустимый обобщённый НМНК может быть применен в данном случае, так как мы знаем процесс порождающий данные. Так как $\Var[y]=1/\lambda^2$ для экспоненциальной плотности, дисперсия равна квадрату математического ожидания, из этого следует, что $\Var[u|x]=[\exp(-x'\beta)]^2$. Оценка ДОНМНК оценивает ${\sigma_i}^2$ с помощью ${\hat{\sigma}_i}^2=[\exp(-x'_{i}\hat{\beta}_{NLS})]^2$, и может быть реализована с помощью НМНК регрессии $y_{i}/\hat{\sigma}_i$ на $\exp(-x'_{i}\beta)/\hat{\sigma}_i$. В общем оценки ДОНМНК менее эффективны, чем оценки ММП. В данном примере оценка на самом деле эффективна, так как экспоненциальная плотность лежит в экспоненциальном семействе (см. обсуждение в конце Раздела 5.8.6).

\subsection{Симуляции и результаты}

Для простоты рассмотрим регрессию на константу и регрессор. Данные были сгенерированы с помощью следующего процесса:
\[
y|x \sim exponential[\lambda],
\]
\[
\lambda=\exp(\beta_1 + \beta_2 x),
\]
где $x \sim \mathcal{N}[1,1^{2}]$ и $(\beta_1,\beta_2)=(2,-1)$. Была взята большая выборка размера 10 000, чтобы минимизировать различия в оценках, в частности, стандартных ошибках, возникающие в связи с изменчивостью выборки. Для конкретной выборки из 10 000, сделанной здесь, выборочное среднее $y$ составляет $0.62$ и выборочное стандартное отклонение составляет $1.29$.

\begin{table}[h]
\begin{center}
\caption{\label{tab:Expex} Экспоненциальный пример: оценки метода наименьших квадратов и оценки ММП}
\begin{minipage}{14cm}
\begin{tabular}[t]{cccccc}
\hline
\hline
& & & \bf{Оценка}\footnote{Все оценки соcтоятельны, кроме МНК. Приведены до трёх альтернативных стандартных оценок: нескорректированные в скобках, скорректированные с помощью внешнего произведения в квадратных скобках и альтернативные скорректированные оценки для НМНК в фигурных скобках. Процесс порождающий данные --- экспоненциальное распределение с константой $2$ и параметром наклона $-1$. Объём выборки N = 10 000.} & & \\
\hline
\bf{Переменная} & \bf{МНК} & \bf{ММП} & \bf{НМНК} & \bf{ВНМНК} & \bf{ДОНМНК} \\ 
\hline
Константа & $-0.0093$ & $1.9829$ & $1.8876$ & $1.9906$ & $1.9840$ \\
& $(0.0161)$ & $(0.0141)$ & $(0.0307)$ & $(0.0225)$ & $(0.0148)$ \\
& $[0.0172]$ & $[0.0144]$ & $[0.1421]$ & $[0.0359]$ & $[0.0146]$ \\
& & & $\{0.2110\}$ & & \\
$x$ & $0.6198$ & $-0.9896$ & $-0.9575$ & $-0.9961$ & $-0.9907$ \\
& $(0.0113)$ & $(0.0099)$ & $(0.0097)$  & $(0.0098)$ &  $(0.0100)$ \\
& $[0.0254]$ &  $[0.0099]$ & $[0.0612]$ & $[0.0224]$ & $[0.0101]$ \\
& & & $\{0.0880\}$ & & \\
$\ln L$ & --- & $-208.71$ & $-232.98$ & $-208.93$ & $-208.72$ \\
$R^2$ & $0.2326$ & $0.3906$ & $0.3913$ & $0.3902$ & $0.3906$ \\
\hline
\hline
\end{tabular}
\end{minipage}
\end{center}
\end{table}

Таблица 5.7 представляет оценки МНК, ММП, НМНК, ВНМНК и ДОНМНК. Также приведены до трёх разных стандартных ошибок. По умолчанию результаты регрессии дают нескорректированные стандартные ошибки, приведенные в скобках. Для оценок МНК и НМНК ошибки предполагаются одинаково и независимо распределёнными, что является ошибочным предположением здесь.  Для оценки ММП одинаковая распределенность ошибок приводит к равенству информационных матриц, выполненному в данном случае, так как процесс порождающий данные правильно специфицирован. Скорректированные стандартные ошибки, которые даются в квадратных скобках, используют устойчивую оценку дисперсии $N^{-1}{\hat{A}_H}^{-1}\hat{B}_{OP}{\hat{A}_H}^{-1}$, где $\hat{B}_{OP}$ является внешним произведением, оценённом при (5.38). Эти оценки устойчивы к гетероскедастичности. Для стандартных ошибок НМНК оценок приведена в фигурных скобках более корректная альтернативных оценка (объясняется в следующем Разделе). Оценки стандартных ошибок, представленные здесь, используют численные, а не аналитические производные при подсчёте $\hat{A}$ и $\hat{B}$.

\subsection{Сравнение оценок и стандартные ошибки}

Оценка МНК не состоятельна, и никак не связана с $(\beta_1,\beta_2)$  экспоненциального процесса, порождающего данные.

Остальные оценки являются состоятельными, и оценки ММП, НМНК, ВНМНК и ДОНМНК находятся в пределах двух стандартных отклонений от истинных значений параметров $(2,-1)$, где скорректированные стандартные ошибки должны быть использованы для НМНК. Оценки ДОНМНК весьма близки к оценкам ММП, что является следствием использования распределения из экспоненциального семейства в процессе, порождающем данные.

Для оценки ММП нескорректированные и скорректированные стандартные ошибки ММП очень похожи. Ожидается, что они асимптотически эквивалентны (поскольку имеет место равенство информационных матриц, если оценки ММП основаны на истинной плотности) и размер выборки здесь большой.

Для НМНК нескорректированные стандартные ошибки являются недействительными, так как у процесса порождающего данные гетероскедастичные ошибки, и значительно преувеличивают точность оценок НМНК. Формула для скорректированной оценки ковариационнной матрицы НМНК дана в (5.81), где $\hat{\Omega}=\Diag[{\hat{u}_i}^2]$. Альтернативная оценка, которая использует $\hat{\Omega}=\Diag[\hat{\E}[{u}_i^2]]$, где $\hat{\E}[{u}_i^2]=[\exp(-x'_i\hat{\beta})]^2$ приведена в скобках. 
Две оценки отличаются: $0.0612$ по сравнению с $0.0880$ для коэффициента наклона. Разница связана с тем, что ${\hat{u}_i}^2=(y_i-\exp(x'_i \hat{\beta}))^2$ отличается от $[\exp(-x'_i \hat{\beta})]^2$. В более общем случае стандартная ошибка, оценённая с использованием внешнего произведения (см. Раздел 5.5.2), может быть смещена даже в достаточно больших выборках. Оценки НМНК значительно менее эффективны, чем оценки ММП, и их стандартные ошибки (рекомендуемые --- в фигурных скобках) превышают во много раз стандартные ошибки оценок ММП.

Оценки ВНМНК не использует правильную модель гетероскедастичности, поэтому нескорректированные и скорректированные стандартные ошибки снова отличаются. При использовании робастных стандартных ошибок  ВНМНК даёт более эффективные оценки, чем оценки НМНК и менее эффективны, чем оценки ММП.

В этом примере оценки ДОНМНК столь же эффективны, как и оценки ММП, что является следствием использования распределения из экспоненциального семейства в процессе, порождающем данные. В результатах видно, что коэффициенты и стандартные ошибки очень близки к соответствующим оценкам ММП. Скорректированные и нескорректированные  стандартные ошибки для оценки ДОНМНК по существу такие же, как и ожидалось, поскольку здесь модель гетероскедастичности правильно специфицирована.

Таблица 5.7 также содержит оценки логарифма функции правдоподобия, $\ln L=\sum_i [x'_i\hat{\beta}-\exp(-x'_i\hat{\beta})y_i]$, и $R^2$, $R^2=1-\sum_i (y_i-\hat{y_i})^2/\sum_i (y_i-\bar{y})^2$, где $\hat{y_i}=\exp(-x'_i\hat{\beta})$ оценивается с помощью ММП, НМНК, ВНМНК и ДОНМНК. $R^2$ мало отличается в различных моделях и является самым низким для оценки МНМК, как и ожидалось, так как НМНК минимизирует $\sum_i (y_i-\hat{y_i})^2$. Логарифмическая функция правдоподобия достигает максимума в оценке ММП, как и ожидалось, и значительно выше, чем для оценки НМНК.

\subsection{Интерпретация коэффициентов}

Интерес заключается в подсчете изменения $\E[y|x]$ при изменении $x$. Рассмотрим оценки ММП $\hat{\beta}_2=-0.99$ приведенные в таблице 5.7.

Условное математическое ожидание $\exp(-\beta_1-\beta_2x)$ имеет одноиндексную форму, поэтому, если дополнительный регрессор $z$ с коэффициентом $\beta_3$ будет включён, то предельный эффект изменения на одну единицу $z$ будет в $\hat{\beta}_3/\hat{\beta}_2$ раза больше, чем изменение на одну единицу $x$ (см. п. 5.2.4).

Условное математическое ожидание монотонно убывает по $x$, поэтому знак $\hat{\beta}_2$ противоположный знаку предельного эффекта (см. раздел 5.2.4). Здесь предельный эффект увеличения $x$ является увеличением условного математического ожидания, поскольку $\hat{\beta}_2$ отрицательная.

Рассмотрим теперь величину предельного эффекта изменений в $x$ с использованием численных методов. Здесь $\partial \E[y|x]/\partial x=-\beta_2 \exp(-x'\beta)$ варьируется в зависимости от точки $x$ и принимает значения от $0.01$ до $19.09$ в выборке. Выборочное среднее предельного эффекта равно $0.99 N^{-1} \sum_i \exp(x'_i\hat{\beta})=0.61$. Предельный эффект для среднего  $x$ равен $0.99 \exp(\bar{x'}\hat{\beta})=0.37$, значительно меньше. Поскольку $\partial \E[y|x]/\partial x=-\beta_2 \E[y|x]$, получаем ещё одну оценку предельного эффекта, $0.99\bar{y}=0.61$.

Взятие разности вместо производной приводит к другим оценкам предельного эффекта. Для $\Delta x=1$ мы получаем $\Delta \E[y|x]=(e^{\beta_2}-1)\exp(-x'\beta)$ (см. Раздел 5.2.4) Получаем средний эффект для выборки $1.04$, а не $0.61$. Взятие разности и производной дают одинаковый предельный эффект, если $x$ мало.

Предыдущие предельные эффекты можно складывать. Для экспоненциального условного математического ожидания можно рассмотреть и мультипликативный или пропорциональной предельный эффект (см. п.5.2.4). Например, изменение $x$ на $0.1$, по прогнозам, приведёт к пропорциональному росту $\E[y|x]$ на $0.1 \times 0.99$ или к росту $9.9\%$. Опять взятие разности даст другую оценку.

Какая из этих мер наиболее полезна? Ограничение на одноиндексную форму очень полезно, поскольку относительное влияние регрессоров можно вычислить сразу. Что касается величины ответной реакции, наиболее точным является вычисление средней реакции по выборке, используя подсчет разности при изменении регрессора на $c$ единиц, где величина $c$ --- это осмысленное число, например,  одно стандартное отклонение $x$.

Аналогичные расчёты можно сделать для  оценок НМНК, ВНМНК и ДОНМНК с аналогичными результатами. Для МНК-оценки обратим внимание, что коэффициент при $x$ может быть интерпретирован как выборочный среднее предельного эффекта изменения в $x$ (см. Раздел 4.7.2). Здесь оценка МНК $\hat{\beta}_2=0.61$ равна до двух десятичных знаков среднему предельному эффекту по выборке, вычисленному ранее с использованием экспоненциальной оценки ММП. Здесь МНК обеспечивает хорошую оценку выборочного среднего предельной реакции, хотя он может обеспечить очень плохую оценку предельной реакции для любого конкретного значения $x$.

\subsection{Практические соображения}

Большинство эконометрических пакетов предоставляют простые команды для получения оценок максимального правдоподобия для стандартных моделей, введённых в Разделе 5.6.1. Для других плотностей многие пакеты обеспечивают ММП оценивание, для которого пользователь записывает уравнение плотности и, возможно, первые производные или даже вторые производные. Аналогично, для НМНК записывается уравнение условного математического ожидания. Для некоторых нелинейных моделей и наборов данных ММП и НМНК оценивание, реализованное в пакетах, может  быть связано c вычислительными трудностями при получении оценок. В таких случаях может быть необходимо использовать более надёжные оптимизационные процедуры, которые предоставляются в качестве дополнительных модулей к Gauss, Matlab и OX. Gauss, Matlab и OX хорошо подходят для нелинейного моделирования, но требуют более высокой подготовки.

Для пространственных данных становится распространенным использование стандартных ошибок, основанных на сэндвич-форме ковариационной матрицы. Возможность использовать сэндвич-форму часто предоставляется в виде опции при оценивании. Для оценок МНК это даёт устойчивые к гетероскедастичности стандартные ошибки. Для максимального правдоподобия необходимо знать, что неправильная спецификация плотности может привести к несостоятельности помимо  необходимости использовать сэндвич ошибки.

Параметры нелинейных моделей, как правило, не могут быть напрямую интерпретируемы, и хорошо дополнительно вычислить подразумеваемые предельные эффекты, вызванные изменениями в регрессорах (см. Раздел 5.2.4). Некоторые пакеты делают это автоматически, для других может быть необходимо несколько строчек кода после оценки, используя сохранённые коэффициенты регрессии.

\subsection{Библиографические примечания}

Краткая история развития теории асимптотических результатов экстремальных оценок приведена в Ньюи и МакФаддене (1994, с. 2115). Значительный прорыв вперёд в эконометрике был сделан Амэмия (1973), который разработал довольно общие теоремы, которые были применены к тобит модели оценки ММП. Полезные книги --- это книги Галанта (1987), Галанта и Уайта (1987), Байеренса (1993), Уайта (1994, 2001а). Основы по теории статистики приведены во многих книгах, в том числе в книгах Амэмия (1985, глава 3), Дэвидсона и МакКиннона (1993, глава 4), Грина (2003, Приложение D), Дэвидсона (1994), и Замана (1996).

\begin{enumerate}
\item [$5.3$] Презентация общих результатов для экстремальной оценки во многом основывается на книге Амэмия (1985, глава 4), и в меньшей степени на книге Ньюи и МакФаддена (1994). Последний источник имеет очень широкий охват материала.
\item [$5.4$] Подход оценочных уравнений используется в литературе об обобщённых линейных моделях (см. Маккалоу и Нельдер, 1989). Эконометристы обобщили его до обобщённого метода моментов (см. Главу 6).
\item [$5.5$] Статистические выводы подробно изложены в главе 7.
\item [$5.6$] См. новаторскую статью Фишера (1922) для общих результатов по оценке ММП, в том числе для результатов по эффективности, а также для сравнения подхода правдоподобия с обратными вероятностями или Байесовским подходом и с методом моментов.
\item [$5.7$] Современные приложения часто используют квази-ММП и сэндвич-оценки ковариационной матрицы (см. Уайт, 1982, 1994). В статистике этот подход называется обобщёнными линейными моделями,  см. Маккалоу и Нельдер (1989).
\item [$5.8$] Аналогично и в случае НМНК используются сэндвич-оценки ковариационной матрицы, при этом накладываются относительно слабые ограничения на ошибки. Работа Уайта (1980a, c) оказала большое влияние на статистические выводы в эконометрике. Обобщение и подробный обзор асимптотической теории представлены у Уайта и Домовитца (1984). У Амэмия (1983) содержится широко используемы методы нелинейных регрессий.
\end{enumerate}

\begin{center}
Упражнения
\end{center}

\begin{enumerate}
\item [$5-1$] Предположим, мы получили оценки модели, которые дали следующее условное математическое ожидание: $\hat{\E}[y|x]=\exp(1+0.01x)/[1+\exp(1+0.01x)]$. Предположим, что выборка имеет размер 100 и $x$ принимает целые значения $1,2,\dots,100$. Найдите следующие оценки оцениваемого предельного эффекта $\partial \hat{\E}[y|x]/\partial x$.
\begin{enumerate}
\item Средний предельный эффект по всем наблюдениям.
\item Предельный эффект для среднего наблюдения.
\item Предельный эффект при $x=90$.
\item Предельный эффект  изменения на одну единицу, когда $x=90$, вычислив его посчитав разницу.
\end{enumerate}

\item [$5-2$] Рассмотрим следующий частный однопараметрический случай гамма-распределения, 

$f(y/\lambda^2)\exp(-y/\lambda),y>0,\lambda>0$. Для этого распределения можно показать, что $\E[y]=2\lambda$ и $\Var[y]=2\lambda^2$. Здесь мы вводим регрессоры и предположим, что в истинной модели параметр $\lambda$ зависит от регрессоров в соответствии с $\lambda_i=\exp(x'_i\beta)/2$. Таким образом, $\E[y_i|x_i]=\exp(x'_i\beta)$ и $\Var[y_i|x_i]=[\exp(x'_i\beta)]^{2}/2$. Предположим, что данные являются независимыми по $i$ и $x_i$ являются нестохастическими и $\beta=\beta_0$ в процессе порождающем данные.
\begin{enumerate}
\item Покажите, что логарифмическая функция правдоподобия (домножаемая на $N^{-1}$) для этой гамма модели равна $\mathcal{Q}_{N}(\beta)=N^{-1} \sum_i\{\ln y_i -2 x_i'\beta + 2\ln2 -2 y_i \exp(-x'_i \beta) \}$.
\item Найдите $\plim \mathcal{Q}_{N}(\beta)$. Можно предполагать, что условия любого ЗБЧ удовлетворены. [Подсказка: $\E[\ln y_i]$ зависит от $\beta_0$, а не от $\beta$.]
\item Докажите, что оценка $\hat{\beta}$, являющаяся локальным максимумом $\mathcal{Q}_{N}(\beta)$, состоятельна для $\beta_0$. Запишите сделанные предположения.
\item Укажите, какой ЗБЧ нужно использовать для проверки пункта (b) и какая необходима дополнительная информация, если таковая необходима, чтобы применить этот закон. Подойдёт краткий ответ. Нет необходимости формального доказательства.
\end{enumerate}

\item [$5-3$] Продолжим работу с гамма моделью из упражнения 5-2.
\begin{enumerate}
\item Покажите, что $\partial \mathcal{Q}_{N}(\beta) / \partial \beta= N^{-1} \sum_i 2[(y_i-\exp(x'_i\beta))/\exp(x'_i\beta)]x_i$.
\item Какое из условий первого порядка, является существенным, чтобы $\hat{\beta}$ была состоятельной? 
\item Примените центральную предельную теорему для получения предельного распределения $\sqrt{N}\partial \mathcal{Q}_{N}/\partial \beta|_{\beta_0}$. Здесь вы можете предположить, что условия ЦПТ, удовлетворены.
\item Напишите, какую ЦПТ используете, чтобы использовать для проверки пункта $c$ и какая необходима дополнительная информация, если таковая необходима, чтобы применить этот закон. Подойдёт краткий ответ. Нет необходимости формального доказательства.
\item Найдите предел по вероятности от $\partial^2 \mathcal{Q}_{N}/\partial \beta \partial \beta'|_{\beta_0}$.
\item Объедините предыдущие результаты, чтобы получить предельное распределение $\sqrt{N}(\hat{\beta}-\beta_0)$.
\item С учётом пункта (f), укажите, как проверить $H_0: \beta_{0j} \geq {\beta_j}^*$ против $H_a: \beta_{0j} < {\beta_j}^*$ на уровне значимости 0.05, где $\beta_j$ --- $j$-ая компонента $\beta$.
\end{enumerate}

\item [$5-4$] Неотрицательная целая переменная $y$, которая распределена по геометрическому закону, описывается функцией вероятности $f(y)=(y+1)(2\lambda)^{y}(1+2\lambda)^{-(y+0.5)},y=0,1,2,\dots,\lambda>0$. Тогда $\E[y]=\lambda$ и $\Var[y]=\lambda(1+2\lambda)$. Введите регрессоры, и пусть $\gamma_i=\exp(x'_i\beta)$. Предположите, что данные независимы по $i$, $x_i$ является нестохастическим и $\beta=\beta_0$ в процессе порождающем данные.
\begin{enumerate}
\item Повторите упражнение 5-2 для этой модели. 
\item Повторите упражнение 5-3 для этой модели.
\end{enumerate}

\item [$5-5$] Предположим, выборка даёт оценки $\hat{\theta}_1=5,\hat{\theta}_2=3,se[\hat{\theta}_1]=2$, а $se[\hat{\theta}_2]=1$, а коэффициент корреляции между $\hat{\theta}_1$ и $\hat{\theta}_2$ равен 0.5. Проведите следующие тесты на уровне значимости 0.05, предполагая асимптотическую нормальность оценок параметров. 
\begin{enumerate}
\item Протестируйте $H_0: \theta_1=0$ против $H_a: \theta_1 \not= 0$.
\item Протестируйте $H_0: \theta_1=2\theta_2$ против $H_a: \theta_1 \not =2\theta_2$.
\item Протестируйте $H_0: \theta_1=0, \theta_2=0$ против $H_a:$ по меньшей мере один из $\theta_1,\theta_2 \not= 0$.
\end{enumerate}

\item [$5-6$] Рассмотрите нелинейную модель регрессии $y=\exp(x'\beta)/[1+\exp(x'\beta)]+u$, где ошибки, возможно, гетероскедастичны.
\begin{enumerate}
\item При таком ограничении в каком диапазоне должно лежать $\E[y|x]$?
\item Запишите условия первого порядка для оценки НМНК.
\item Получите асимптотическое распределение НМНК оценки, используя результат (5.77).
\end{enumerate}

\item [$5-7$] Этот вопрос предполагает доступ к программному обеспечению, которое позволяет получить оценки НМНК и ММП. Рассмотрим гамма регрессионную модель из упражнения 5-2. Соответствующую переменную гамма можно сгенерировать с помощью $y=-\lambda \ln r_1 - \lambda \ln r_2$, где $\lambda=\exp(x'\beta)/2$, $r_1$ и $r_2$ являются случайными выборками из равномерного распределения $[0,1]$. Пусть $x'\beta=\beta_1+\beta_2 x$. Сгенерируйте выборку размером 1 000, где $\beta_1=-1.0$ и $\beta_2=1$ и $x \sim \mathcal{N}[0,1]$.
\begin{enumerate}
\item  Получите оценки $\beta_1$ и $\beta_2$ из регрессии НМНК $y$ на $\exp(\beta_1+\beta_2 x)$. 
\item Нужно ли здесь использовать сэндвич стандартные ошибки?
\item Получите ММП оценки $\beta_1$ и $\beta_2$ из  НМНК регрессии $y$ на $\exp(\beta_1+\beta_2 x)$. 
\item Нужно ли здесь использовать сэндвич стандартные ошибки?
\end{enumerate}
\end{enumerate}

