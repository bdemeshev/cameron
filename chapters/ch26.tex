

\chapter{Модели ошибок измерения}
%%%% Comments %%%%

\section{Введение}
Проблемы, связанные с ошибками измерений в эконометрике встречаются повсеместно. В микроэконометрике основным источником проблем ошибок измерения являются неправильные \emph{(некорректные)} ответы на вопросы анкет, неправильное кодирование правильных ответов и использование правильно измеренных переменных в качестве прокси для теоретически подходящей, но ненаблюдаемой переменной (например, использование наблюдаемого дохода как прокси для «нармального дохода» \emph{(normal income)}).  Вопросы, связанные с конфиденциальной информацией, могут привести к неполным или неверным ответам. Иными словами, ошибки измерений порождаются ненаблюдаемыми \emph{(или латентными)} переменными, когда  они заменяются прокси-переменными.

Приведём несколько примеров. Рассмотрим задачу тестирования наличия дискриминации по половому признаку в рамках исследования доходов. Очевидный подход --- построить регрессию переменной доходов на категориальную переменную пола индивида, контролируя на квалификацию, возраст, опыт работы и т.д. Однако наиболее подходящей переменной может быть индивидуальная производительность труда на рабочем месте, которая не может быть измерена напрямую и вместо которой может быть использована прокси. Таким образом, влияние ошибки измерений на выводы относительно гендерной дискриминации, является важной проблемой. Исследования индивидуального спроса на товары и услуги используют такие понятия, как «экономические издержки» \emph{(economic cost)} и «полная цена услуги» \emph{(full price of e service)}. Тем не менее, такие переменные редко напрямую могут быть измерены и опубликованы, так то для оценивания моделей должны быть сформулированы априорные предположения.  Неизбежно это становится источником ошибок.

В этой книге нет фактически ни одной модели, защищённой от проблем ошибок измерений. Бинарные исходы эндогенных и экзогенных переменных являются потенциально источником ошибок классификации; данные по переходам из одного состояния в другое и дискретные данные, взятые из ретроспективных обследований подвержены ошибкам памяти; данные по относительно однозначным переменным, таким как почасовая заработная плата и расходы домохозяйств, могут искажаться из-за преднамеренного завышения и/или неверно сообщенной информации. В отличие от агрегированных данных, когда агрегирование может нивелировать некоторые ошибки измерений, данные на уровне отдельных индивидов эти ошибки сохраняют.

В первой части этой Главы изучаются последствия ошибок измерений и стратегии оценивания для ликвидации этих последствий. Рассматриваются как линейные, так и нелинейные модели. Хотя более реалистичным было бы решать проблему ошибок измерений, принимая во внимание, что она возникает одновременно с рядом других, более удобно для представления предположить, что это единственная проблема, которая стоит перед эконометристом.
Вообще говоря, последствия ошибок измеренйя заключаются в неправильной идентификации интересующих параметров. Проблема решения такой задачи --- комплексная. Можно просто не включать действительно подходящую переменную, можно заменить её прокси для истинного значения. Существуют, по крайней мере, две причины, по которым не стоит впадать в крайности. Первая заключается в том, что невключение в модель основной интересующей переменной ведёт к серьёзному смещению, что является замещением одной проблемы другой при той же невозможности идентификации параметров. Вторая причина такова: в линейной регрессии использование прокси вместо латентной переменной даёт меньшее асимптотическое смещение, чем невключение переменной в модель, что делает ошибку наблюдения случайной и независимой от истинных регрессоров (McCallum, 1972). Игнорирование переменной даёт оценки низкого качества.  Тем не менее, использование прокси не решает проблему несмещенности, хотя и приводит к меньшему смещению.
Основная идея, лежащая в основе решения задачи борьбы с ошибками измерений, --- не иметь дела с параметрами, а идентифицировать саму модель; необходимо располагать внешней информацией в виде дополнительных предположений об ошибках измерений или получить дополнительные данные и использовать эту информацию после введения правдоподобных допущений. Такой подход является довольно известным. Тем не менее, когда дополнительная информация недоступна, эконометрические модели представляют собой хорошую альтернативу.

Ошибки измерений вызывают очень серьёзные последствия, так как во многих случаях они ведут к тому, что параметры регрессии не могут быть идентифицированы. Например, Кард (Card, 2001) проанализировал эмпирические исследования коэффициента при переменной образования в регрессии доходов и обнаружил, что, как правило, результаты смещены в сторону занижения на 25--35\%.  Конкретные масштабы последствий ошибок измерений зависят от функциональной формы модели, от того, какую форму принимают ошибки (аддитивную или мультипликативную) и от структуры анализируемых данных. Решение проблемы, возникшей по причине ошибок измерений, обычно требует введения в модель дополнительной информации, как в форме данных, так и в форме предположений.

Удобно построить рассмотрение ошибок измерений, разбив его на части: случаи линейных моделей, нелинейных моделей и специальные случаи. Секции 26.2 и 26.3 посвящены линейной регрессии. В Секции 26.4 речь идёт о нелинейной регрессии. В Секции 26.5 приводятся несколько примеров использования метода Монте-Карло.  Необходимые в анализе линейных регрессий моменты применимы и в случае нелинейных моделей. Заведомо более понятные результаты обычно доступны в моделях специального вида.

\section{Ошибки измерений в линейной регрессии}
Ошибки измерений регрессоров или {\bfошибки переменных} \emph{(error-in-variables)} --- важный предмет рассмотрения, так как это причина несостоятельности МНК-оценок, даже если ошибки измерений имеют нулевое математическое ожидание. Принято говорить, что ошибки измерений регрессоров ведут к смещению оценок, но мы пользуемся более сильным термином --- несостоятельность --- то есть смещение сохраняется даже при стремлении размера выборки к бесконечности.

Спектр моделей ошибок измерений широк, они описывают ситуации, когда ошибкам измерений подвержены объясняющие переменные --- регрессоры (regressors), зависимые переменные (outcome) или и те, и другие. В работе Хаусмана (Hausman, 2001) эти случаи названы «проблема справа» и «проблема слева». В случае зависимой переменной, когда мы имеем классическую модель ошибки переменной,  интерес представляет взаимосвязь регрессанта y и предикторов $(\mathbf{W}, \mathbf{X^*})$, где $\mathbf{W}$ измерена без ошибок, а $\mathbf{X^*}$ --- ненаблюдаемая, но прокси для неё, переменная $\mathbf{X}$, доступна. Вопрос исследования ставится тогда следующим образом: является ли оценка взаимосвязи $\mathbf{y}$ и $(\mathbf{W}, \mathbf{X})$ достаточным основанием для заключений относительно $\mathbf{X^*}$.
В статистической литературе  нет однозначного вывода относительно выбора между функциональным и структурным подходом к моделированию ошибок измерения. Если $\mathbf{X^*}$ отражает истинные ненаблюдаемые ковариации, то, согласно функциональному подходу, они буду представлены как неизвестные фиксированные константы (параметры) . В структурном подходе они будут считаться случайными. Коррол, Рупперт и Штефански (Carrol, Ruppert, Stefanski, 1995) конкретизировали подходы, разделив их на \emph{функциональное моделирование}, в котором относительно регрессоров X сделаны лишь минимальные предположения, вне зависимости от того, фиксированные они или случайные, и \emph{структурное моделирование}, которое предполагает допущения на основе распределения $\mathbf{X}$. Функциональные модели ошибок измерений являются примерами моделей с бесконечным количеством уточняющих параметров, что порождает известные сложности в применении метода максимального правдоподобия (которые обсуждались в Главе, посвященной анализу панельных данных). Этому моменту нечасто уделяется внимание в эконометрической литературе.

Масштабы несостоятельности на практике могут играть важную роль. В исследованиях детерминант доходов индивидов это чрезвычайно значимая тема для обсуждения в контексте проблемы ошибок измерений, как и методы её выявления.

\subsection{Классическая модель ошибок измерений} 
Стандартная модель ошибок измерений имеет непрерывную зависимую переменную $y$, которая является линейной функцией от $K$ истинных регрессоров $\mathbf{x^*}$. Аддитивная ошибка измерений y не порождает никаких проблем, если она не коррелирована с регрессорами, так как она может просто сливаться с ошибкой регрессии. Если $\mathbf{x^*}$ наблюдались, то параметры могут быть состоятельно оценены с помощью МНК-регрессии $y$ на $\mathbf{x^*}$,
\[
y_i=\mathbf{x^*}_i'+u_i,
\]
Где $u_i$ --- независимые одинаково распределённые случайные величины с нулевым математическим ожиданием и дисперсией (iid $[0,\sigma^2]$). Вместо этого имеем $\mathbf{x}\neq \mathbf{x^*}$ и регрессируем $y$ на $\mathbf{x}$, а не на $\mathbf{x^*}$. Взаимосвязь между истинными и реально наблюдаемыми регрессорами задаётся как
\begin{equation}
\mathbf{x}_i=\mathbf{x}_i^*+\mathbf{v}_i, i=1,…,N,
\end{equation}
где аддитивно заданная ошибка измерений предполагается распределённой следующим образом:
\begin{equation}
\mathbf{v}_i \sim [\mathbf{0},\mathbf{\Sigma_{vv}}].
\end{equation}
Для ненаблюдаемых истинных регрессоров предполагается нулевое математическое ожидание, так что наблюдения могут быть измерены как отклонения от среднего с матрицей вариаций
\begin{equation}
\Var\mathbf{([x^*_i]}=\mathbf{\Sigma_{x^*x^*}}.
\end{equation}
Отметим, что $\mathbf{x}$ есть несмещённая оценка $\mathbf{x^*}$, так как математическое ожидание ошибки измерений $\mathbf{v}$ предполагается равным нулю. Ошибка измерений, согласно предпосылкам модели, независима от $\mathbf{x^*}$ и ошибки регрессии $u$,
\begin{equation}
\Expect(\mathbf{v_i}|\mathbf{x^*_i})=\Expect[\mathbf{v_i}|u_i]=0.
\end{equation}

\subsection{Несостоятельность оценок метода наименьших квадратов}  
Чтобы рассмотреть последствия ошибок измерений, удобно представить процесс получения данных (data generating process) для классической модели ошибок измерений в матричном виде как 
\begin{align}
\mathbf{y}&=\mathbf{X^*}\beta+\mathbf{u}, \\
\mathbf{X}&=\mathbf{X^*}+\mathbf{V}, \notag
\end{align}
где $u$, ошибка регрессии, удовлетворяет условиям $\Expect(\mathbf{u}|\mathbf{X^*})=\mathbf{0}$ и $\Expect(\mathbf{uu'}|\mathbf{X^*})=\sigma^2\mathbf{I}_N$. Подставляя второе вырашение в первое, получаем
\begin{equation}
\mathbf{y}=\mathbf{X}\beta+(\mathbf{u}-\mathbf{V}\beta).
\end{equation}
МНК-регрессия $\mathbf{y}$ на $\mathbf{X}$ ведёт к получению несостоятельных оценок $\mathbf{\beta}$, так как ошибка $(\mathbf{u}-\mathbf{V\beta})$ коррелирована с регрессором $\mathbf{X}$ через ошибку измерений $\mathbf{V}$.
Формально мы имеем
\begin{align*}
plim N^{-1}\mathbf{X'}(\mathbf{u}-\mathbf{V\beta})&=plimN^{-1}(\mathbf{X^*}+\mathbf{V})'(\mathbf{u}-\mathbf{V\beta}) \\
&=-\mathbf{\Sigma_{vv}}\mathbf{\beta} \\
&\neq\mathbf{0},
\end{align*}
зная $N^{-1}\mathbf{V'V}=N^{-1}\sum\limits_i\mathbf{v}_i\mathbf{v}'_i$ и $\mathbf{v}_i$ iid $[\mathbf{0}, \mathbf{\Sigma_{vv}}]$. Это и есть основной источник несостоятельности. Теперь 
\begin{align*}
plimN^{-1}\mathbf{X'X}&=plimN^{-1}(\mathbf{X^*+V})'(\mathbf{X^*+V}) \\
&=\mathbf{\Sigma_{x^*x^*}}+\mathbf{\Sigma_{vv}},
\end{align*}
Где мы использовали требование о том, что $\mathbf{x_i^*}$ - независимые одинаково распределённые случайные величины с нулевым средним и ковариационной матрицей $\Var\mathbf{([x^*_i]}=\mathbf{\Sigma_{x^*x^*}}$. Также
\begin{align*}
plimN^{-1}\mathbf{X'y}&=plimN^{-1}(\mathbf{X^*+V})'(\mathbf{X^*\beta+u}) \\
&=\mathbf{\Sigma_{x^*x^*}}+\mathbf{\Sigma_{vv}},
\end{align*}
так что, согласно теореме Слуцкого (Приложение А, Теорема А.3.), мы получаем
\begin{align}
plim\mathbf{\widehat {\beta}}&={plimN^{-1}\mathbf{X'X}}^{-1}plimN^{-1}\mathbf{X'y} \\
&={\mathbf{\Sigma_{xx}}}^{-1}(\mathbf{\Sigma_{xx}}-\mathbf{\Sigma_{vv}})\mathbf{\beta} \notag \\
&=\mathbf{\beta}-{(\mathbf{\Sigma_{x^*x^*}}+\mathbf{\Sigma_{vv}})}^{-1}\mathbf{\Sigma_{vv}}\mathbf{\beta}. \notag 
\end{align}

Очевидно, что несостоятельность МНК появляется из-за ошибок измерений и предположения $\mathbf{\Sigma_{vv}}\neq\mathbf{0}$.

Следующее замечание состоит в том, что, если мы имеем состоятельную оценку ковариационной матрицы $\mathbf{\Sigma_{vv}}$, обозначенной как $\mathbf{S_{vv}}$, и если $(\mathbf{X'X}-\mathbf{S_{vv}})$ положительно определена, то можно вычислить оценку скорректированным МНК $\mathbf{\widehat {\beta_a}}={(\mathbf{X'X}-\mathbf{S_{vv}})}^{-1}\mathbf{X'y}$. Эта формула может быть использована для изучения влияния возможных значений дисперсии ошибки измерений на оценку метода наименьших квадратов.

\subsection{Ошибки измерений и скалярный регрессор}
Особый случай этой модели, который обычно рассматривается в учебниках, подразумевает наличие одного истинного или ненаблюдаемого регрессора $x^*$ с дисперсией $\sigma^2_x$, наблюдаемой переменной $x$, ошибки измерений $v$ с нулевым средним и дисперсией $\sigma^2_v$. Таким образом, мы имеем уравнение регрессии $y=\beta x^*+u$, где $\Expect[u|x^*]=0$, $\Var[u|x^*]=\sigma^2_u$,  и $\Cov[v,u]=0$, но вместо регрессии на $x^*$ в оценивании участвует наблюдаемая величина $x$. 
В этом случае (26.7) может быть представлено как
\begin{align}
plim\widehat {\beta}&=\frac{\sigma^2_{x^*}}{\sigma^2_{x^*}+\sigma^2_v}\beta \\
&=\frac{1}{1+\sigma^2_v / \sigma^2_{x^*}}\beta \notag \\
&=\beta[1-s/(1+s)], \notag 
\end{align}

где $s=\sigma^2_v / \sigma^2_{x^*}$, что носит название отношения {\bf шум-сигнал} \emph{(noise-to-signal ratio)}, а компонента ${(1+s)}^{-1}$ называется {\bf отношением стабильности} \emph{(reliability ratio)}. Оценка коэффициента $\widehat {\beta}$ асимптотически стремится к нулю, при этом смещение сокращается в зависимости непосредственно от отношения шум-сигнал. Эта величина ещё носит название {\bf смещения затухания} \emph{(attenuation bias)}. Такая терминология интуитивно понятна, если предположить, что оценки предельного воздействия изменения $\mathbf{x^*}$ на изменение $y$, полученные исследователем, затухают, уменьшаются из-за появления ошибки наблюдений регрессора $x^*$. 
  
Отметим также, что
\[
\Var[y|x]=\sigma^2_u+\frac{{\beta}^2\sigma^2_v\sigma^2_{x^*}}{\sigma^2_{x^*}+\sigma^2_v}\geqslant \sigma^2_u.
\]
Из этого следует, что ошибка измерений не только занижает оценки коэффициентов, но и увеличивает дисперсию ошибки регрессии. И уменьшение вариации ошибок измерений определённо приведёт к снижению вариации ошибок регрессии.
Если бы в описанное выше уравнение регрессии был включен свободный член, смещенной была бы МНК-оценка этого параметра $\bar{y} - \widehat{\beta}\bar{x}$,  где $(\bar y, \bar x)$ - выборочные средние значения, которые всё ещё представляют собой несмещенные оценки средних значений всей генеральной совокупности. Крэгг (Cragg, 1994) предложил термин {\bf «искажение»} \emph{(contamination bias)} для описания влияния ошибок измерения на другие параметры в регрессионном уравнении.
В качестве примера рассмотрим регрессию логарифма почасовой заработной платы на количество лет образования. Предположим, что продолжительность образования $x^*$ измерена с ошибкой, также предположим, что стандартное отклонение истинной величины продолжительности образования равно 2, а стандартное отклонение ошибок измерений равно 1, так что $\sigma^2_{x^*}=4$, $\sigma^2_v=1$, и $\sigma^2_x=5$. Тогда $plim\widehat{\beta}=0.8 \times \beta$. МНК-оценка коэффициента наклона 0.04 тогда означает, что один дополнительный год обучения в школе соответствует 5\%-ому увеличению заработной платы, а не 4\%-ому.

\subsection{Обобщение} 
При обобщении этого простого, но изящного результата, исследователи часто задаются вопросом, является ли затухание общей чертой всех моделей ошибок измерения и что если затуханию подвергаются все параметры? Хотя результат не обязательно распространять на более общие модели, он может послужить хорошей отправной точкой. Хаусман (Hausman, 2001) назвал затухание, вызванное ошибками измерения, «железным законом эконометрики». 
Если ошибки измерений предполагается некоррелированной с истинной ненаблюдаемой переменной, такие ошибки называются «классическими». Несмотря на удобство, эта предпосылка не выполняется. На самом деле, в некоторых случаях она и не может выполняться. Например, если $x$ является бинарной переменной, ошибки измерений будут ошибками классификации. Если в результате неправильной классификации 0 будет измерен как 1 и наоборот, то ошибки измерений должны будут коррелировать с истинными значениями.
Когда в модель включается больше, чем 1 регрессор, обозначим $\mathbf{X^*}=[\mathbf{x^* Z}]$, где, как и в предыдущем случае, мы предполагаем, что только один регрессор наблюдается с ошибкой измерения , то есть $x=x^*+v$. Тогда выражение для оценки коэффициента при $x$ методом наименьших квадратов будет следующим:
\begin{equation}
plim\widehat{\beta}_{x|\mathbf{Z}}=\beta \left[ 1-\frac{\sigma^2_v}{\sigma^2_{x^*}(1-R^2_{x^*,\mathbf{Z}})+\sigma^2_v}\right],
\end{equation}
где $R^2_{x^*,\mathbf{Z}}$ представляет собой $R^2$ во вспомогательной регрессии $\mathbf{x^*}$ на $\mathbf{Z}$. Формула (26.9) представляет собой другой вариант интерпретации  дисперсии $x^*$ как вариации после контролирования на или исключения влияния $\mathbf{Z}$ на $\mathbf{x^*}$. И снова несостоятельность МНК-оценки стремиться к нулю, хотя и с меньшей скоростью, чем в случае одного регрессора. Коэффициенты при регрессорах, измеренных без ошибок, также несостоятельны, при этом направление смещения зависит от $\mathbf{\Sigma_{x^*x^*}}$ (Levi, 1973). Этот эффект может быть снова отнесён к «искажению». Затухание, которое демонстрируется в рассмотренном случае, сильно зависит от предпосылки об аддитивности ошибок измерений. 
Когда более чем один регрессор измерен с ошибками, общие результаты, связанные с несостоятельностью, больше неверны, хотя в любой рассматриваемой задаче они могут быть выведены, исходя из знания о $\mathbf{\Sigma_{x^*x^*}}$ и $\mathbf{\Sigma_{vv}}$. В большинстве работ предполагается, что ошибки измерения существуют только для одного регрессора, в этом случае несостоятельность стремиться к нулю. Интуиция, основанная на рассмотренных выше примерах, позволяет заключить, что ошибки измерений разных регрессоров независимы, тогда ошибки любого будут способствовать затуханию собственного коэффициента регрессии, что в итоге ведёт к росту условной дисперсии. Крэгг (Cragg, 1994) анализировал множественную регрессионную модель с ошибками измерений и продемонстрировал взаимосвязь между смещениями разной природы.

\subsection{Ошибки измерений в линейных моделях панельных данных}
 
Эффект от ошибок измерений в регрессорах становится более сложным в случае панельных данных.
Рассмотрим pool-модель панельных данных $y_{it}=\beta x^*_{it}+u_{it}$, где мы наблюдаем $x_{it}=x^*_{it}+v_{it}$, при этом для упрощения рассмотрим скалярный регрессор. Полученные ранее результаты по-прежнему верны, если мы рассматриваем пространственный срез данных. Тем не менее, если мы оцениваем модель, используя наблюдения за несколько лет, нам необходимо скорректировать  предыдущие результаты, так как регрессор $x^*_{it}$ будет скорее всего положительно коррелирован, нежели ион будет иметь независимые наблюдения по $t$ для определённого $i$. Например, если мы строим регрессию в первых разностях
\begin{align*}
\Delta y_{it}&=\beta \Delta x^*_{it}+ \Delta u_{it} \\
&= \beta \Delta x_{it}+\Delta u_{it}-\beta \Delta v_{it}
\end{align*}
(см. Главу 21.6) и определяем параметр $\rho = Cor[x^*_{it}, x^*_{i,t-1}]$, то
\begin{align*}
plim\widehat{\beta}&=\beta + \left( plim \frac{1}{N} \sum \limits^{N}_{i=1}(\Delta x_{it})^2 \right)^{-1} \left( plim \frac{1}{N} \sum \limits^{N}_{i=1}(\Delta x_{it} \Delta u_{it}-\beta \Delta x_{it} \Delta v_{it}) \right) \\
&= \beta - \frac{2\beta \sigma^2_v}{2(1-\rho)\sigma^2_{x^*}+2\sigma^2_v} \\
&=\beta-\frac{\beta \sigma^2_v}{(1-\rho)\sigma^2_{x^*}+\sigma^2_v},
\end{align*}
при использовании $\Var[\Delta v_{it}]=2\Var[v_{it}]$ и $\Var[\Delta x^*_{it}]=2(1-\rho)\Var[x^*_{it}]$.

Масштаб несостоятельности увеличивается по сравнению с cross-section случаем, если $\rho>0$. Более того, при $\rho\rightarrow 1$, что вероятно в случае панельных данных, несостоятельность становится очень сильной. Она может быть снижена путём взятия разностей, которые покрывают $m>1$ лагов, так как $\Corr[x^*_{it}, x^*_{i,t-m}]$ будет убывать с ростом $m$.

\section{Стратегии идентификации} 

Принято считать, что модели ошибок переменных не могут быть идентифицированы без введения дополнительных предпосылок. Это утверждение может быть интерпретировано в контексте особого случая – парной регрессии – следующим образом. Оценка значения параметра $\widehat{\beta}$, или точнее, его предел по вероятности, будет состоятельной при бесконечно большом количестве комбинаций $\beta$ и $s$, отношений шум-сигнал. Хотя, если дополнительные предпосылки или информация могут быть введены для решения задачи, возможным будет определить некоторые комбинации упомянутых параметров, которые будут состоятельными в условиях распределения имеющихся данных. Если дополнительные ограничения вводятся только для достижения единственности решения, модель считается точно идентифицированной. Если введённых ограничений больше, чем достаточно, чтобы однозначно идентифицировать параметры, модель будет переидентифицированной.

Общая стратегия идентификации моделей ошибок измерений заключается в получении границ интервалов, а не точечных оценок интересующих параметров, если нет более полной априорной информации о данных. Если доступны дополнительные данные или информация об ошибках измерений, то осуществимыми становятся и другие стратегии идентификации, такие как метод инструментальных переменных или метод моментов. Наличие дополнительной информации об ошибках измерения определяет общий подход ,который включает одну из самых старых стратегий идентификации, которая подразумевает использование инструментальных переменных, которые выступают связующим звеном между истинными ненаблюдаемыми переменными и соответствующие им наблюдениями. Например, использование дополнительной информации может привести к состоятельным оценкам при затухании, $\frac{\sigma^2_{x^*}}{\sigma^2_{x^*}+\sigma^2_{v}}$, способствуя возможности скорректировать несостоятельные оценки на это смещение. Наконец, дублированные данные или контрольные данные могут быть доступны, и использованы как информация о моментах ошибок измерений. Эти возможности будут подробнее разобраны далее.

\subsection{Ограничения на параметры регрессии}
 
Обратимся снова к множественной регрессии из Главы 26.2. Рассмотренная в том случае модель построена в соответствии с требованиями положительной полуопределённости матриц $\mathbf{\Sigma_{x^*x^*}}$, $\mathbf{\Sigma_{vv}}$ и $\sigma^2$. Вместе с условиями ортогональности они могут быть использованы для построения ограничений на интервал, в котором должны лежать коэффициенты. Клеппер и Лимер (Keppler and Leamer, 1984), а также Вансбик и Мейер (Wansbeek and Meijer, 2000) рассматривали эту проблему в общей её постановке. Наиболее доступный случай интервального подхода – построение обратной регрессии, описанное далее.

\subsection*{Обратная регрессия} 

В простой парной регрессии с переменными $(x, y)$ {\bf прямой} называют регрессию y на x, тогда как {\bf обратной} – регрессию $x$ на $y$. В общем случае множественной регрессии с $K$ независимыми переменными существует единственная прямая регрессия и $K$ обратных. Каждая обратная регрессия содержит в левой части экзогенную переменную, измеренную с ошибками, а в правой части - другие экзогенные переменные и $y$. В случае парной регрессии с ошибками измерений легко показать, что оценки коэффициентов наклона в прямой и обратной постановке определяют верхнюю и нижнюю границы для истинного значения коэффициента. Этот результат может быть использован при анализе эффектов ошибок измерения. Лимер (Leamer, 1978) приводит отличное объяснение логики использования обратной регрессии.
 
Для начала приведём обоснование использования обратной регрессии в приложении к простой парной регрессии с ошибками измерения:
\begin{align}
y&=\beta x^*+u, \\
x&=x^*+v, \notag
\end{align}
где $u$ --– ошибка регрессии  и $v$ --– ошибка измерений, которая представляет собой отклонение наблюдаемых значений x от истинных значений переменной $x^*$, которая включена в регрессию. Будем предполагать, что $ [0,\sigma^2_u]$ и $ [0,\sigma^2_v$.

В соответствии со структурным подходом Солари (Solari, 1969) и Лимера (and Leamer, 1978), положим $x^*$ неизвестными параметроми в функции правдоподобия. Общая функция правдоподобия для имеющихся данных $(\mathbf{y}, \mathbf{x})$ выглядит как
\begin{align}
L(\mathbf{x^*}, \beta, \sigma^2_v)\propto ( \sigma^2_u)^{N/2} \exp \left[ -\frac{1}{2\sigma^2_u}(\mathbf{y-\beta x})'(\mathbf{y-\beta x}) \right] \notag \\
\times ( \sigma^2_v)^{N/2} \exp \left[ -\frac{1}{2\sigma^2_v}(\mathbf{x^*-\beta x})'(\mathbf{x^*-\beta x}) \right].
\end{align}
Эта функция не определена в точках, удовлетворяющим условиям $\sigma^2_u=0$ и $\mathbf{x^*}=\mathbf{x}$, или условиям $\sigma^2_v=0$ и $\mathbf{y}=\mathbf{\beta x^*}$. Если мы просто минимизируем хорошо определённые части функции правдоподобия в соответствии с ограничениями, мы получи два скалярных параметра регрессии, $\widehat{\beta}_D=\mathbf{y'x/x'x}$ для прямой регрессии и $\widehat{\beta}_R=\mathbf{y'x/y'y}$ для обратной. В помощь интуиции отметим, что если $\mathbf{x}$ измерен без ошибок, то $\mathbf{y}$ случаен, а $\mathbf{x}$ --- нет, тогда прямая регрессия имеет интерпретацию осмысленных условных ожиданий, и если только $\mathbf{x}$ --- стохастический (измерен без ошибок), то условное ожидание $\Expect[\mathbf{x|y}]$ имеет смысл, та как система из двух уравнений сводится к уравнению $x=(1/\beta)y-u/\beta+v$. Таким образом, обратная регрессия даёт МНК-оценку $\widehat{1/\beta}$. Из этого напрямую можно проверить, что
\begin{align}
r^2_{xy} \widehat{\beta}_R&=\widehat{\beta}_D, \\
\widehat{\beta}_D &< \beta < \widehat{\beta}_R, \notag
\end{align}
где $r^2_{xy}$ --- простой квадрат корреляции между $x$ и $y$; ограничения свидетельствуют о том, что $\widehat{\beta}_D$ --- заниженная оценка $\beta$ и $\widehat{\beta}_R$ --- завышенная оценка. Отметим, что этот интервал может быть очень широким для микроэкономических данных, где почти всегда $r^2_{xy}<0.5$ и даже $r^2_{xy}<0.5$ довольно распространено.

Лимер (Leamer, 1978) предложил модель, в которой $(y, x*)$ имеют двумерное нормальное распределение со средним $(\beta \bar{x}^*, \bar{x}^*)$ и ковариационной матрицей 
\begin{equation}
\mathbf{\Sigma}=
\begin{bmatrix}
\sigma^2_u+\beta^2\sigma^2_{x^*} & \beta\sigma^2_{x^*} \\ \beta\sigma^2_{x^*} & \sigma^2_{x^*}+\sigma^2_v
\end{bmatrix} .
\end{equation}
Он показал (Leamer, 1978, pp. 239--240), что для этой модели функция правдоподобия достигает максимума при любом значении $\widehat{\beta}_D$ из промежутка от оценки из прямой регрессии $\beta$ до оценки из обратной регрессии $\widehat{\beta}_R$.

Анализ, о котором пойдёт речь далее, предполагает, что хотя $\beta$ и не идентифицируется, можно всё же получить состоятельные оценки интервала для искомого значения. Здесь очень полезным является {\bf метод идентификации интервалов для коэффициентов}. Этот результат может быть быть непосредственно распространён на случай множественной регрессии, а которой только один регрессор измерен с ошибками (Bollinger, 2003). Клеппер и Лимер (Klepper and Leamer, 1984) рассмотрели обобщение случая множественной регрессии с $K$ независимыми переменными, предположив, что все они могут иметь ошибки измерений. В этом случае будет одна прямая регрессия и $K$ обратных. После оценивания каждая обратная регрессия перенормируется с учетом одинакового коэффициента при $y$ в левой части. Тогда $\mathbf{\widehat{\beta}_{D}}$ --- вектор оценок из прямой регрессии и $\mathbf{\widehat{\beta}_{R,j}}$ --- вектор оценок из $j$ой обратной регрессии. Согласно результатам Клеппера и Лимера (Klepper ang Leamer, 1984), если коэффициенты из прямой и обратных регрессий лежат в одном ортанте, то набор доступных значений $\mathbf{\beta}$ --- это выпуклая оболочка множества точек для прямой и обратной регрессии; тогда $\mathbf{\beta} \in \{ \mathbf{\widehat{\beta}|\widehat{\beta}=\lambda_D \widehat{\beta}_D}+\lambda_1 \widehat{\beta}_{R,1}+ \dots \lambda_k \widehat{\beta}_{R,K} \}$, где $\lambda$ --- неотрицательные веса, в сумме дающие единицу. Наименьший коэффициент из прямой и обратных регрессий является нижней границей интервала, а наибольшие --- верхней границей. Таких границе не существует, если значения коэффициента меняют знак.

В дополнение к работе Клеппера и Лимера (Klepper and Leamer, 1984) можно упомянуть несколько работ, в которых этот подход применятся на практике. Грин (Green, 1983) и Голдбергер (Goldberger) использовали метод обратных регрессий для измерения масштабов дискриминации в контексте размера заработной платы. Боллингер (Bollinger, 2003) измерял разницу в доходах белого и черного населения на основе модели зарплаты и человеческого капитала. Боллингер (Bollinger, 1996) обратился к интервальному подходу в случае регрессии на категориальную дамми-переменную, при этом имели место шибки классификации.

\subsection{Идентификация с помощью инструментальных переменных} 
Одним решением проблемы идентификации можно назвать введение одного или более моментных ограничений, которые формируют идентификационную информацию. Моментное условие, как правило, состоит в том, что есть некоторая инструментальная переменная коррелирует или имеет причинно-следственную связь с ошибочно измеренной переменной. Более того, она не коррелирует и не имеет причинно-следственных связей с зависимой переменной модели. Добавляя это ограничение в исходную модель, мы теоретически получаем решение проблемы идентификации.

Исторически, оценивание методом инструментальных переменных считается возможным путём решения проблемы ошибок измерения в линейных моделях (Reiersol, 1941; Durbin, 1954). Подход с использованием инструментальных переменных также обоснован, когда одна или более переменная в правой части уравнения является эндогенной, что ведёт к коррелированности с ошибкой регрессии. Модель линейных одновременных уравнений и линейная модель ошибок измерений сходны по структуре, и, таким образом, использование оценок метода инструментальных переменных в случае возникновения ошибок измерений вполне естественно.

Снова рассмотрим постановку линейной модели инструментальных переменных из Главы 4.8 и 6.4, где $\mathbf{y=X\beta+u}$ и $\Expect[\mathbf{u|X}] \neq \mathbf{0}$, для них мы можем использовать 2SLS оценивание, если доступен набор валидных инструментов $\mathbf{Z}$, таких что $\dim[\mathbf{Z}] \geqslant \dim[\mathbf{X}]$.

С помощью теста Хаусмана на эндогенность в регрессорах можно проверить наличие ошибок измерений, см. Главу 8.3. Возможны несколько вариантов теста, один из которых подробнее рассмотрен в Главе 8.4.

Основная проблема применения оценок метода инструментальных переменных заключается в сложности нахождения валидных инструментов на практике. Хорошие инструменты удовлетворяют двум требованиям: некоррелированность с ошибкой регрессии (для состоятельности) и высокая корреляция с инструментируемыми переменными (для эффективности). Такие инструменты не так просто найти. Хотя в идеале что-то должно в явном виде давать валидные инструменты из подробно проработанной спецификации взаимосвязей регрессоров и регрессантов, как правило, обычно на практике используются ad-hoc методы. В отличие от подхода, подразумевающего полное описание системы, ad-hoc метод проще и требуется для него меньше. Отметим, что требования к валидности инструментов не порождают никакой процедуры отбора. Технические требования могут быть удовлетворены, если переменная не имеет причинно-следственной связи с изучаемым явлением. Нужно найти переменную, сильно коррелированную с регрессором(-ами) и некоррелированную с ошибкой регрессии. Несколько интересных примеров применения этой идеи можно встретить в литературе; см., например, работу Ангриста (Angrist, 1990). Если такая переменная найдена, её использование может быть спорным и противоречивым.

В качестве примера мы предполагаем, что у нас имеется несколько возможных инструментов для cross-section регрессии доходов на образование. Во-первых, если доступны данные о родных братья или сёстрах, то их уровень образования может быть использован как инструмент, так как уровни образования братьев и сестёр предполагаются коррелированными. Состоятельность оценок метода инструментальных переменных тогда основывается на коррелированности ошибки измерений и любой ошибкой измерений уровня образования родных братьев и сестёр. Во-вторых, как более общие, могут быть использованы такие переменные, связанные с уровнем образования, как уровень образования родителей или их доход. Определение границ интервалов с помощью инструментов, тем не менее, может быть связано с риском использования слабо коррелирующих с $x$ переменных, что ведёт к неточности и возможно плохим оценкам. В-третьих, более, чем один вопрос об уровне образования может быть задан респонденту в рамках опроса, или же информация об образовании может быть доступна из опросов прошлых лет, если речь идёт об исследовании данных панельной структуры. Такие инструменты, скорее всего, будут сально коррелировать с $x$, но в этом случае сложно поверить в выполнение предпосылки о слабой коррелированности между ошибками измерений в $x$ и $z$.
 
Лаги переменных часто используются как инструменты, но и они будут иметь ошибки измерений, так что использование метода инструментальных переменных оправдано только в случае, если отсутствует серийная корреляция между ошибками измерений.

Последствия от ошибок измерений могут иметь больший размах в случае анализа панельных данных. Так как в этом случае измерения $x^*_{it}$ берутся за несколько периодов, то оценивание методом инструментальных переменных может быть использовано для получения состоятельных оценок параметров в предположении о некоррелированности ошибок измерений во времени. См. Хсяо (Hsiao, 1986, pp. 63-65).

\subsection{Идентификация с помощью дополнительных моментных ограничений} 
Предположения о распределении ошибок регрессии и ошибок измерения $(u, v)$ могут обеспечить возможность идентификации модели. Есть один важный случай, в котором использование информации или допущений относительно распределения ненаблюдаемого истинного значения измеряемой переменной помогает при идентификации. Предположения о нормальности совместного многомерного закона распределения $(y, x, x^*)$ вместе с предположением о то, что ошибки регрессии и ошибки измерений являются независимыми одинаково распределёнными нормальными случайными величинами iid $\mathcal{N}[0, \, \sigma^2_v]$ и  iid $\mathcal{N}[0, \, \sigma^2_u]$, недостаточно для построения модели ошибок измерения. Тем не менее, предпосылка о том, что первые четыре момента $(x^*, u, v)$ существуют и первые три момента каждой величины и перекрёстные моменты третьего порядка --- ненулевые, на основе нормальности, дают достаточную базу для идентификации, что будет продемонстрировано ниже.

Снова построим модель вида (26.10)
\begin{align*}
y&=\beta x^* + u, \\
x&=x^*+v,
\end{align*}
сокращенная форма которой выглядит как $y=\beta x+ \epsilon$, где $\epsilon = u- \beta v$, и которая оценивается с помощью инструментальных переменных. Однако теперь мы добавляем новую информацию: распределение $x^*$ ненормально в том смысле, что асимметрия и эксцесс отличны от нормального (Cragg,1997; Dagenais and Dagenais, 1997; Wansbeek and Mejer, 2000). Эти предпосылки отражены в слудующих шести условиях:
\begin{align*}
&\Expect [(xy)x]=\beta \Expect [x^{*3}], &\ &\Expect [(xy)u]=0, \\
&\Expect [(x^2)x]=\Expect [x^{*3}]+\Expect [v^3],&\ &\Expect [(x^2)u]=-\beta \Expect [v^3], \\
&\Expect [(y^2)x]=\beta^2 \Expect [x^{*3}],&\ &\Expect [(y^2)u]=-\beta \Expect [\epsilon^3],
\end{align*}
Первая строчка отражает, что искусственно созданная переменная $x_i y_i$ будет являться валидным инструментом, если $\Expect[x^{*3}_i] \neq 0$.  Из второй строчки видно, что $x^2_i$ будет валидным инструментом, если  $\Expect[x^{*3}_i] \neq 0$, но  $\Expect[v^3_i] = 0$, то есть, если $x^*$ не распределён нормально, но v имеет симметричное распределение. Действительно, чем больше коэффициент асимметрии, тем лучше инструмент. Тем не менее, так как переменная $x^*$ - ненаблюдаемая, то любые заключения относительно неё должны базироваться на $x$. В последней строчке содержится утверждение, что $y_i^2$ будет валидным инструментом, если третий момент $x^*$ не будет равным нулю при том, что третий момент $\epsilon$ будет нулевым.

При использовании этих моментных условий метод инструментальных переменных может применяться для получения состоятельных оценок параметров модели. Этот пример показывает, как дополнительные предположения о моментах распределения могут помочь для создания хороших инструментов даже в условиях, когда кроме $(y_i, x_i)$ нет доступных данных.

\subsection{Дублированные данные} 
Альтернативное решение возможно, если может быть оценена вариация ошибок измерений. В этом случае идея состоит в том, что мы можем скорректировать матрицу выборочных вторых моментов регрессоров $\mathbf{X'X}$ на число, зависящее от дисперсии и ковариаций ошибок измерений. Обратим внимание, что мы не пытаемся скорректировать сами наблюдения. Вместо этого корректируются выборочные моменты, так как оценка сама по себе является функцией от выборочный моментов. Основная идея также обобщается до более сложных моделей.

Когда ковариационная матрица ошибок измерений $\mathbf{\Sigma_{vv}}$ известна, состоятельная оценка коэффициента $\mathbf{\beta}$ может быть получена с помощью формулы
\begin{equation}
\mathbf{\tilde{\beta}}=(\mathbf{X'X}-N \mathbf{\Sigma_{vv}})^{-1} \mathbf{X'y},
\end{equation}
где $N$ – размер выборки. Эта оценка состоятельна в силу того, что
\begin{align*}
\mathbf{\tilde{\beta}}&=plim(N^{-1}\mathbf{X'X}-N \mathbf{\Sigma_{vv}})^{-1} plim N^{-1} \mathbf{X'y}  \\
&= (\mathbf{\Sigma_{x^*x^*}} + \mathbf{\Sigma_{vv}} - \mathbf{\Sigma_{vv}})^{-1} \mathbf{\Sigma_{x^*x^*} \beta} \\
&= \mathbf{\beta},
\end{align*}
где $plim N^{-1} \mathbf{X'y}=\mathbf{\Sigma_{x^*x^*} \beta}$ получено с использованием $\mathbf{X}=\mathbf{X^*}+\mathbf{V}$ и $\mathbf{y}=\mathbf{X} \beta + (\mathbf{u}-\mathbf{V \beta})$. Для подробного описания способов оценивания $\mathbf{\Sigma}_{vv}$ в практических приложениях можно обратиться к работе Крашинского (Krashinsky, 2004).

{\bf Дублирование (повторение, реплицирование) данных} –-- это ситуация, в которой доступна несмещенная оценка ненаблюдаемой переменной $\mathbf{X^*}$. Рассмотрим случай, когда ошибка измерений аддитивна и имеется наблюдаемая величина $\mathbf{X}$:
\[
\mathbf{X}=\mathbf{X^*}+\mathbf{V}.
\]
Если $\mathbf{X}$ является несмущенной оценкой $\mathbf{X^*}$, то $\Expect[\mathbf{V|X^*}]=\mathbf{0}$. Если данные дублированы, то это означает, что мы имеем, по крайней мере, два доступных наблюдения $\mathbf{X}$. Также это означает, что с использованием множественных измерений $\mathbf{X}$ мы может получить оценки моментов $\mathbf{V}$, предполагая, что ошибки измерений в случае многомерности не коррелированы.

Предположим, что в нашем распоряжении две скалярных величины (реплики) $X(1)$ и $X(2)$, такие что $X_{(j)}=X^*+V_{(j)}, \, j=1, \, 2$. Тогда $\Var[V_{(j)}]=\Expect[X^2_{(j)}]-\Expect[X_{(1)}X_{(2)}]$, которая может быть оценена, исходя из выборочного среднего, как $N^{-1} \Sigma_i [X^2_{(j),i}-X_{(1),i}X_{(2),i}]$. В этом случае параметра регрессии могут быть оценены  по формуле (26.14). 

В качестве примера предположим, что мы хотим предсказать средний академический балл (GPA) за первый год обучения в колледже, используя результаты за экзамен SAT в старших классах (тест на проверку академических способностей). Известно, что наблюдаемые баллы за SAT для одного и того же человека в разных попытках сдачи экзамена. Положим, что $x^*$ отражает истинный балл за SAT, а $x_1$ и $x_2$ отражают наблюдаемые результаты двух разных экзаменов. Тогда $x_1=x^*+v_1$, $x_2=x^*+v_2$, и предполагается, что $v_1$ и $v_2$ независимы и имеют одинаковую дисперсию $\sigma^2_v$. Из этого следует, что $\Cov[x_1, x_2]=\sigma^2_{x^*}$, $\Var[x_1]=\Var[x_2]=\sigma^2_{x^*}+\sigma^2_v$ и $\Corr^2[x_1,x_2]=\sigma^2_{x^*} / (\sigma^2_{x^*}+\sigma^2_v)$. Согласно исследованиям, теста имеют надёжность 0.9, что означает, что корреляция между результатами двух сдач теста равна 0.9 и квадрат корреляции равен 0.81. Таким образом, $\sigma^2_{x^*} / (\sigma^2_{x^*}+\sigma^2_v)=0.81$. Из (26.8) следует, что $plim \widehat{\beta}=0.82 \times \beta$, то есть, так как ошибки измерений балла за SAT помогают лучше предсказать результаты первого года обучения (GPA), чем МНК.

\subsection{Контрольные данные} 
Иногда контрольная выборка формируется также для дополнительной проверки исходных ответов. Хотя {\bf контрольная выборк}а и принадлежит исследуемой совокупности, она может быть получена из другого независимого источника. Например, пациенты могут заполнять анкету об оказанных им медицинских услугах, и те, кто оказывают эти услуги, могут ответить на вопросы в качестве контрольной информации. Другой пример касается работников, которые могут предоставить информацию о чем-то, тогда как проконтролирована она может быть на основе информации, полученной от работодателей. Один из лучших примеров в экономике –-- контрольное исследование PSID (Bound et al., 1994).

Пусть $\mathbf{X}$ – матрица размерности $N \times K$ наблюдений регрессоров, измеренных с ошибками, и пусть $X_v$ – матрица контрольных измерений размерности $M \times K$. Мы можем использовать контрольные данные, построив регрессию $X_v$ на $\mathbf{X}$ и сгенерировав «предсказанные» значения $\mathbf{X[X'X]}^{-1} \mathbf{X'X}_v$, которые послужат заменой искаженной матрице $\mathbf{X}$. Для нелинейных моделей используется более сложный подход (см. Lee and Sepanski, 1995).

Использование сгенерированных регрессоров вместо интересующих может быть стратегией, полезной на практике, если предсказания взяты из хорошо подогнанной регрессии. Сгенерированные регрессоры являются оценками истинных значений и, таким образом, связаны с неопределённостью. Эта неопределённость будет учтена при оценивании дисперсии оценок коэффициентов. Соответствующая теория описана в главе 6.8.

\section{Ошибки измерений в нелинейных моделях} 
Нелинейные модели, что должно быть достаточно понятно, включают в себя ряд моделей, которые требуют над собой размышлений. Получение общих результатов, таких как эффект затухания, которые применимы к широкому классу моделей, представляет собой важную задачу. Нечасто общие результаты получают при упрощении предположений, тогда как в для получения специфических внимание уделяется усложнению и уточнению определённых связанных с данными случаев. Таким образом, неудивительно, что развитие одной темы в литературе порождает много процедур и подходов, специфических и применимых к определённого рода моделям. Например, в случае моделей бинарного выбора с ошибками измерений в левой части естественным считается обращать внимание на проблему неверной классификации; в случае дискретных моделей также с ошибками измерений в левой части это в той же мере естественно --– концентрироваться на проблеме занижения и завышения показателей. Мотивированный такими сложностями, Хсяо (Hsiao, 1992) предложил переключить внимание от поиска решений для общего вида моделей на более специальные типы задач. Однако в поиске специфических результатов есть опасность узости и потери части общих результатов. Таким образом, мы начинаем рассмотрение с некоторых общих результатов.

\subsection{Идентификация с помощью инструментальных переменных} 
Общая техника анализа линейных моделей ошибок переменных --– метод инструментальных переменных. Для нелинейной (по независимым переменным) регрессионной модели было показано (Y.Amemiya, 1985), что оценка метода инструментальных переменных в общем случае несостоятельна, кроме случая, когда выполнена предпосылка об упрощающейся (shrinking) ковариационной матрице.

Простое представление описанной выше идеи основано на регрессионном уравнении
\begin{equation}
y=\beta_0 +\beta_1 f(x^*)+ \epsilon,
\end{equation}
где $f(x^*)$ --- гладкая, дифференцируемая ограниченная функция скалярного регрессора $x^*$, измеренного без ошибок. Наблюдаемая переменная определяется как $x=x^*+v$, где $v$ --– ошибка измерений. Заменяя переменную $x^*$ и используя разложение в ряд Тейлора функции $f(x-v)$ в окрестности точки $x$, получаем
\begin{equation}
y=\beta_0 +\beta_1 f(x)+ \epsilon - \beta_1 f^{(1)}(x)v + \beta_1 \sum \limits^{\infty}_{j=2} f^{(j)}(x)(=v)^j/j!,
\end{equation}
где $f^{(j)}(\cdot)$ - производная $j$ого порядка функции $f(\cdot)$. Рассмотрим случая квадратичной функции $f(x)=x^2+\gamma x$, так что $f^{(1)}(x)=2x+ \gamma$, $f^{(1)}(x)=2$ и $f^{(j)}(x)=0, \, j>2$. Тогда
\begin{align}
y&=\beta_0 + \beta_1 (x^2+ \gamma x) + \epsilon - \beta_1 (2x+ \gamma)v + \beta_1 2v^1 /2 \notag \\
&= \beta_0 + \beta_1 x^2 + \beta_1 \gamma x + (\epsilon- \beta_1 xv - \beta_1 \gamma v + \beta_1 v^2),
\end{align}
Следовательно, переменные, которые будут валидными инструментами, должны будут коррелировать с $x^2$ и $x$, но не коррелировать с $u=(\epsilon- \beta_1 xv - \beta_1 \gamma v + \beta_1 v^2)$. Понятно, что недостаточно, чтобы $v$ и $\epsilon$ каждая не коррелировали с инструментами. Это означает, что инструментальная переменная для $f(x)$ должна удовлетворять более жестким требованиям, что в линейном случае.

В более общем случае было показано (Y. Amemiya), используя аппроксимацию Тейлора, что инструментальные переменные не дают состоятельных оценок в нелинейных моделях ошибок переменных, так как остаточный член включает в себя как ошибку измерений, так и наблюдаемую искаженную ошибками переменную. Таким образом, невозможно найти инструментальную переменную, которая бы сильно коррелировала с наблюдаемой переменной и не коррелировала с остаточным членом. Более того, с практической точки зрения, непросто найти подтверждение валидности инструмента при оценивании из-за ограниченности информации о латентной переменной $(x^*)$ и ошибках измерений.
 
\subsection{Идентификация с помощью дублированных данных} 
Имея в виду сложности применения метода инструментальных переменных, приведём ещё два существующих альтернативных подхода.

Первый подразумевает введение очень жестких предположений об условном распределении ненаблюдаемой переменной $x^*$ при условии знания о наблюдаемом $x$. Такие предположения, усиленные техническими условиями, позволяют идентифицировать параметры модели. Такой подход описан, среди прочих, в ряде работ (Y. Amemiya, 1985 and Hsiao, 1989).

Второй подход предусматривает возможность получения большого количества измерений каждого ненаблюдаемого $x^*$, обозначенного как $x_{(j)}$. Тогда среднее по реплицированных измерениям для каждого $x^*$ служит заменой для ненаблюдаемого регрессора. Мы получаем состоятельные оценки нелинейной модели, так как ковариационная матрица ошибок измерений сокращается до нуля при увеличении числа реплик (Y.Amemiya, 1985). К сожалению, в эконометрике этот способ редко воплотим.

Так как не существует информации об обычной структуре нелинейной модели ошибок измерения, которая может быть использована для идентификации и оценивания регрессионной модели, рассмотрим несколько специальных нелинейных регрессий.

Хаусман, Ньюи и Поуэл (Hausman, Newey and Powell, 1995) анализировали полиномиальные кривые Энгла на основе данных обследования расходов домохозяйств. Их полиномиальная функция была линейна по параметрам. Они доказали, что, при условиях регулярности, и инструментальные переменные, и дополнительные измерения могут быть использованы для получения состоятельных и асимптотически нормальных оценок. Применительно к этому случаю, четверть была взята как дублированная информация и как инструментальные переменные. Далее они предположили, что нелинейная функция в общем может быть аппроксимирована полиномиальной функцией. Тем не менее, они признали, что инструментальные переменные не могут быть введены в этом случае, так что требуются дополнительные измерения истинных регрессоров.

Ли (Li, 2002)предложил общий двухшаговый подход к проблеме нелинейных моделей ошибок переменных, который основан на дублированных измерениях. В качестве первого шага, на базе эмпирических характеристик функции и обратного преобразования Фурье, получают непараметрические оценки для условной функции плотности латентных переменных. При доступности этого метода строятся полупараметрические оценки нелинейного метода наименьших квадратов с использованием критерия минимального расстояния. Он обосновал состоятельность этих оценок. Эти оценки также робастны в том смысле, что не требуют никакого знания о функциональной форме латентных переменных. Подход Ли может быть применён в ситуации любых нелинейных моделей ошибок переменных, если доступны дублированные измерения. Тем не менее, асимптотическое распределение оценки не установлено.

\subsection{Ошибки измерений в зависимых переменных} 
В линейной регрессионной модели ошибки измерений в зависимой переменной увеличивают стандартные ошибки параметров регрессии, но не ведут к несостоятельности оценок. В нелинейной модели возникают ещё ряд последствий.

В одном из классов моделей предполагается наличие неверной классификации наблюдений в моделях качественного выбора. Далее приведено обобщение литературы об анализируемых ошибках.

\subsection*{Модели дискретного выбора} 
Потерба и Саммерз (Poterba and Summers, 1995) в своём исследовании влияния страхования от безработицы на длительность безработицы с использованием данных CPS обобщили вероятностные модели, чтобы учесть неправильную классификацию индивидов по смете статуса на рынке труда. Конкретнее, они фокусировались на возможных ошибках отнесения к одному из трёх классов: занятные, безработные и не входящие в рабочую силу. Они анализировали множественную логит-модель с учетом некоторых особенностей данных: предполагалось, что все индивиды правильно ответили на вопрос о статусе занятости в первый месяц исследования. Их результаты показали, что страхование от безработицы увеличивает длительность безработицы, и что эта взаимосвязь для неправильного отнесения в группы по статусу на рынке труда усиливает наблюдаемое влияние страхования безработицы на её длительность. Тем не менее, их модель основана на предпосылке о том, что вероятность предоставления неправильных ответовфиксирована и не коррелирует с индивидуальными характеристиками, которая, с чем согласны сами авторы, на практике маловероятно выполняется. Хотя авторы утверждают, что оценки параметров состоятельные, другие спорят с ними, утверждая, что стандартные ошибки несостоятельно оценены из-за игнорирования вариабельности по выборке вероятности ошибки и не блочно-диагональной формы информационной матрицы.

Хаусман с коллегами (Hausman et al., 1998) предложили параметрический метод оценивания модели бинарного выбора с ошибками неверной классификации. Тем не менее, их параметрический метод требует знания о законе распределения ошибок. Они делают  акцент на том, что оценки параметром могут быть несостоятельными, если предпосылка о типе распределения не выполняется. Также они предлагают двухшаговый полупараметрический метод. Основное условие для идентификации в этой модели, которое, как они показали, слабее условия для параметрического метода, состоит в том, что ожидаемое значение наблюдаемой зависимой переменной есть возрастающая функция от некоторого индекса. В сравнении с подходом Потерба и Саммерза (Poterba and Summers, 1995), их подход робастен в том смысле, что вероятность неверной классификации является функцией от индивидуальных характеристик. Используя CPS и PSID, они показали, что имеет место серьезные ошибки классификации в переменной смены работы.

Клейн и Шерман (Klein and Sherman, 1997) развили {\bf модель «Орбит»} (с чертами ордерд-модели упорядоченного выбора и Тобит-модели) для оценки возможного спроса на новый видео продукт. Они выявили, что потенциальные потребители завышают спрос. Модель Орбит представляет собой двухшаговую процедуру с оцениванием параметров стандартной Тобит-модели для реального будущего спроса на первом шаге и с оцениванием функции отображения между текущим предполагаемым спросом и реальным будущим спросом. Далее они доказывают состоятельность и асимптотическую нормальность оценок Орбит-модели. Несмотря на это, идентификация модели требует выполнения предпосылки о том, что предполагаемый нулевой спрос действительно будет нулевым в будущем. Это может быть достаточно сильным предположением.
 
Хсяо и Сан (Hsiao and Sun, 1999) использовали данные рыночного обследования спроса на продвинутую электронику. Они показывали, что респонденты могут давать смещенную информацию относительно их спроса. Они предложили модель случайных ответов и модель одностороннего смещения для преувеличений, в которой различные параметрические вероятности расценивались, как истина или нет в виде логит или пробит функции распределения для верного отражения действительности. Они определили, что «имеет место значительное смещение в ответах, и на анализируемом рынке ставки и эластичности цен являются более оправданными, чем оценки, полученные на основе  предположений о сообщении своих истинных предпочтений потребителями».

\subsection*{Регрессии дискретных переменных} 
В случае построения нелинейной регрессии дискретных \emph{(count)} переменных Камерон и Триведи (Cameron and Trivedi, 1998) предложили подход для моделирования в условиях занижения вероятности. Этот подход обобщает комбинацию модели Пуассона и отрицательной биномиальной модели для дискретных данных, допуская бинарную форму зависимой переменной.  Говоря точнее, каждое наступление события рассматривается как испытание Бернулли, чтобы отменить, было ли отмечено явление. В предположении о положительной вероятности, что явление может быть неотмеченным, распределение отмеченных явлений будет иметь меньшее среднее и дисперсию, чем распределение действительно наступавших событий. Далее они исследуют оценку модели, полученную методом максимального правдоподобия, псевдо обобщенным методом максимального правдоподобия и методом моментов. Основываясь на методе Монте-Карло, они определили, что использование метода максимального правдоподобия хорошо в случаях выборок, превосходящих 500 наблюдений.

Джордан с коллегами (Jordan et al., 1997) применили метод ошибок в переменных в модели Пуассоновской регрессии. В исследовании смертности от рака желудка в пяти регионах Японии они отметили, что регрессор (например, уровень ликопена в плазме) неизвестен и оценивается по случайной совокупности, и, таким образом, порождает ошибки. Принимая предпосылку о том, что ошибка измерений нормально распределена, они применяют Байесовскую технику, получив апостериорное распределение параметров, используя отбор Гиббса \emph{(Gibbs sampling)}. Результаты говорят о том, что скорректированная модель даёт более точные оценки параметров, даже когда выборка мала.

\subsection{Пуассоновская регрессия с ошибками измерений в независимых переменных} 
Теперь мы более детально рассмотрим один специальный пример нелинейной регрессионной модели с аддитивными ошибками измерений в независимых переменных. Этот пример иллюстрирует и последствия от такого рода ошибок измерений, и доступные стратегии оценивания.

Гуо и Ли (Guo and Li, 2002) показали, что ошибки измерений в регрессорах в общем случае ведут к завышенной дисперсии наблюдаемых данных. Они также показали с использованием Монте-Карло симуляций, что смещение возникает в случае, если завышенная дисперсия, вызванная ошибками измерений, неверно смоделирована как возникшая вследствие ненаблюдаемой неоднородности. Таким образом, из наличия завышенной дисперсии не обязательно следует вывод о ненаблюдаемой гетерогенности.

Штефански (Stefanski, 1989) и Накамура (Nakamura, 1990) предложили {\bf скорректированную оценку счета} \emph{(corrected score estimator)}, которая является состоятельной в случае наличия ошибок измерений. В частности, Накамура (Nakamura, 1990) построил конечный вид функции счета \emph{(score function)} для случая, когда ошибки измерений распределены нормально и доступны дублированные наблюдения. Гуо и Ли (Guo and Li, 2002) же обобщили результаты Накамуры (Nakamura, 1990).

\subsection*{Ошибки наблюдений и завышенная дисперсия} 
В этой Секции мы рассмотрим Пуассоновскую регрессионную модель, в которой дискретная случайная переменная $y$ распределена на закону Пуассона с параметром $\mu = exp(\mathbf{x^{*\prime} \beta})$, где $\mathbf{\beta}$ --- вектор параметров размерности $K \times 1$. Как известно, Пуассоновская регрессионная модель накладывает требование на дисперсию:
\begin{equation}
\Expect[y| \mathbf{x^*}]=\Var[y \mid  \mathbf{x^*}].
\end{equation} 
Если ошибка измерений имеет аддитивную форму, то
\[
\mathbf{x}=\mathbf{x^*}+ \epsilon,
\]
где $\epsilon$ предполагается независимой от ненаблюдаемой латентной переменной $\mathbf{x^*}$, обладающей нулевым средним и ковариационной матрицей $\mathbf{\Sigma_{\epsilon}}$. Это замечание включает в рассмотрение и случай, когда все или некоторые объясняющие переменные измерены с ошибками.

Ошибки измерений увеличивают дисперсию (см. Chesher, 1991). Это применимо к Пуассоновской регрессии в том смысле, что, хотя (26.18) выполняется, условное математическое ожидание и дисперсия $y$ при заданном $\mathbf{x^*}$ меняются при подстановки в условие известного $\mathbf{x}$. Вместо этого мы получаем $\Expect[y| \mathbf{x}]<\Var[y| \mathbf{x^*}]$, в частности,$\Expect[y| \mathbf{x^*}] \neq \Expect[y| \mathbf{x}]$  и $\Var[y| \mathbf{x^*} \neq \Var[y| \mathbf{x^*}]$.

Если $g(\mathbf{x^*|x})$ определяет условную плотность $\mathbf{x^*}$ при заданном $\mathbf{x}$, тогда Гуо и Ли (Guo and Li) показали, что
\begin{align}
\Expect[y| \mathbf{x}] &= \int \Expect[y| \mathbf{x^*}]g(\mathbf{x^*|x})\, d\mathbf{x^*} \notag \\
&= \int \Expect[y^2| \mathbf{x^*}]g(\mathbf{x^*|x})\, d\mathbf{x^*} - \int (\Expect[y| \mathbf{x^*}])^2 g(\mathbf{x^*|x})\, d\mathbf{x^*},
\end{align}
И, используя (26.18), условная дисперсия $y$ при заданном $\mathbf{x}$ будет следующей:
\begin{equation}
\Var[y| \mathbf{x^*}]= \int \Expect[y^2| \mathbf{x^*}]g(\mathbf{x^*|x})\, d\mathbf{x^*} -  \left[ \int \Expect[y| \mathbf{x^*}]g(\mathbf{x^*|x})\, d\mathbf{x^*} \right]^2.
\end{equation}

Сравнение (26.19) и (26.20) демонстрирует, что первая компонента в скобках в (26.19) совпадает с первой компонентой в (26.20). Используя этот факт, Гуо и Ли (Guo and Li) показали, что
\begin{equation}
 \left[ \int \Expect[y| \mathbf{x^*}]g(\mathbf{x^*|x})\, d\mathbf{x^*} \right]^2 \leqslant \int (\Expect[y| \mathbf{x^*}])^2 g(\mathbf{x^*|x})\, d\mathbf{x^*},
\end{equation}
что может быть интерпретировано, как завышение дисперсии вследствие наличия ошибок измерений.

\subsection*{Оценивание модели ошибок в переменных} 
Когда $\mathbf{x}$ искажен ошибками измерений, оценивание методом максимального правдоподобия или нелинейным методом наименьших квадратов, основанными на наблюдениях $(y, \mathbf{x})$ на дают состоятельных оценок. Замена  регрессора $\mathbf{x^*}$ на x в оценивании относится к «наивным» моделям.

Есть две темы для размышлений. Во-первых, почему ММП даёт несостоятельные оценки, когда появляются ошибки измерений? Во-вторых, возможно ли вообще получить состоятельные оценки? Ответ на второй вопрос – «да», если мы применяем {\bf метод corrected score оценивания} для обобщенных линейных моделей, согласно работам Штефански (Stefanski, 1989) и Накамуры (Nakamura, 1990). 

Идея, лежащая в основе метода corrected score оценивания заключается в том, что условное распределение скорректированной оценки относительно $\mathbf{x}$ при условии знания истинной независимой переменной $\mathbf{x^*}$ и зависимой переменной $y$, центрировано относительно ММП-оценки, которая обеспечивает состоятельность оценки истинного значения интересующего параметра.

\subsection*{Несостоятельные и состоятельные оценки} 
Пусть есть выборка $N$ наблюдений $(y_i, \mathbf{x}^*_i), \, i=1, \dots,N$ , которые имеют распределение Пуассона с функцией распределения масс
\[
\Prob[Y_i=y_i|\mathbf{x}^*_i]=\frac{e^{-\mu_i(\mathbf{\beta_0})}\mu_i(\mathbf{\beta_0})^{y_i}}{y_i!},
\]
где $\mu_i(\beta_0)=\exp(\mathbf{x}^{*\prime}_i \mathbf{\beta_0})$. При заданных наблюдениях $(y_i, \mathbf{x}^*_i), \, i=1, \dots,N$ ММП-оценки коэффициентов $\mathbf{\widehat{\beta}}$ состоятельные, если предел по вероятности среднего значения логарифма функции правдоподобия
\begin{align}
plim N^{-1}\ln L(\mathbf{\beta})&=N^{-1} \sum \limits_i \{ -e^{\mathbf{x}^{*\prime}_i \mathbf{\beta}} + y_i \mathbf{x}^{*\prime}_i \mathbf{\beta} - \ln y_i! \} \notag \\
&= \Expect_{y,\mathbf{x^*}}[-e^{\mathbf{x^{*\prime}} \mathbf{\beta}} + y \mathbf{x^{*\prime}} \mathbf{\beta} - \ln y! ]
\end{align}
достигает максимума при $\mathbf{\beta} = \mathbf{\beta_0}$.

Предположим, что мы наблюдаем $\mathbf{x}_i$ вместо $\mathbf{x}^*_i$, где $\mathbf{x}_i=\mathbf{x}^*_i+\mathbf{\epsilon}_i$ и $\epsilon_i \sim \mathcal{N}[\mathbf{0}, \, \mathbf{\Sigma_{\epsilon}}]$ независимы от $\mathbf{x}^*_i$. Тогда $y_i| \mathbf{x}_i$ не распределено по Пуассону. Если, несмотря на это, использовать {\bf  «наивную» модель Пуассона}, полученная оценка $\mathbf{\tilde{\beta}}$ максимизирует
\begin{equation}
Q(\mathbf{\beta})= N^{-1} \sum \limits_i \{ -e^{\mathbf{x}^{*\prime}_i \mathbf{\beta}} + y_i \mathbf{x}^{*\prime}_i \mathbf{\beta} - \ln y_i! \}.
\end{equation}
Этот логарифм неверно специфицированной функции правдоподобия сходится к
\begin{equation}
plim Q(\mathbf{\beta})=\Expect_{y,\mathbf{x}^*}[-e^{\mathbf{x}^{*\prime} \mathbf{\beta}} + y \mathbf{x^{*\prime}} \mathbf{\beta} - \ln y! ] +  \Expect_{\mathbf{x^*}} [-e^{\mathbf{x^{*\prime}} \mathbf{\beta}}] (\Expect_{\epsilon}[e^{\mathbf{\epsilon}' \mathbf{\beta}}]-1),
\end{equation}
что, в общем, не достигает максимума при $\mathbf{\beta}= \mathbf{\beta_0}$. Так что $\mathbf{\tilde{\beta}}$ не является состоятельной оценкой $\mathbf{\beta}_0$.

Возможная модификация этой целевой функции обеспечивает состоятельные оценки. Уравнения (26.22) и (26.24) дают
\[
\{ plim Q(\mathbf{\beta})-\Expect_{\mathbf{x^*}} [-e^{\mathbf{x^*}' \mathbf{\beta}}](\Expect_{\epsilon}[e^{\mathbf{\epsilon}' \mathbf{\beta}}]-1) \} = plim N^{-1}\ln L(\mathbf{\beta}).
\]
Это позволяет максимизировать целевую функцию
\[
Q^+(\mathbf{\beta})=N^{-1} \sum \limits_i \{ -e^{\mathbf{x}^{*\prime}_i \mathbf{\beta}} + y_i \mathbf{x}^{*\prime}_i \mathbf{\beta} - \ln y_i! \} - \Expect_{\mathbf{x^*}} [-e^{\mathbf{x^{*\prime}} \mathbf{\beta}}](\Expect_{\epsilon}[e^{\mathbf{\epsilon}' \mathbf{\beta}}]-1),
\]
так как $Q^+(\mathbf{\beta})$ стремится к $-N^{-1} \sum \limits_i \ln L(\mathbf{\beta})$. Теперь, учитывая независимость $x^*$ и $\mathbf{\epsilon}$,
\[
\Expect_{\mathbf{x^*}}[ -e^{\mathbf{x^*}'_i \mathbf{\beta}}]\Expect_{\epsilon}[e^{\mathbf{\epsilon}' \mathbf{\beta}}] = \Expect_{\mathbf{x^*},\epsilon}[e^{(\mathbf{x^*}+\mathbf{\epsilon})' \mathbf{\beta}}]=-\Expect_{\mathbf{x}}[e^{\mathbf{x}'_i \mathbf{\beta}}],
\]
что состоятельно оценивается как $-N^{-1} \sum \limits_i -e^{\mathbf{x}_i' \mathbf{\beta}}$. Из некоторых исключений следует, что максимизация $$Q^+(\mathbf{\beta})$$ эквивалентна максимизации
\begin{equation}
Q^{++}(\mathbf{\beta})=N^{-1} \sum \limits_i \{ y_i \mathbf{x}^*_i \mathbf{\beta} - \ln y! \} - \Expect_{\mathbf{x^*}} [e^{\mathbf{x^*}' \mathbf{\beta}}].
\end{equation}
Это даёт состоятельную оценку $\mathbf{\beta}_0$ использование этого метода требует пригодной оценки $\Expect_{\mathbf{x}^*}[ e^{\mathbf{x}^{*\prime} \mathbf{\beta}}]$, которая возможна, если доступны дублированные данные. Если распределение объясняющих переменных специфицировано относительно неизвестных параметров, то эти параметры могут быть оценены с помощью этих реплицированных измерений. Таким образом, можно получить оценку $\Expect_{\mathbf{x}^*}[ e^{\mathbf{x}^{*\prime} \mathbf{\beta}}]$.

Оценка $\mathbf{\beta_C}$, которая максимизирует (26.25) была названа {\bf corrected score оценкой} Гуо и Ли (Guo and Li, 2002), так как это корень уравнения corrected score функции $\sum ]limits_i (y_i \mathbf{x}_i- \Expect_{\mathbf{x^*}}[\mathbf{x^*}e^{\mathbf{x^*}'\mathbf{\beta}}])=\mathbf{0}$. Гуо и Ли также показали асимптотическую нормальность этой оценки. Асимптотическая оценка ковариационной матрицы тогда $\widehat{\Var}[\widehat{\mathbf{\beta}}]=N^{-1} \widehat{\mathbf{A}}^{-1}\widehat{\mathbf{B}}\widehat{\mathbf{A}}^{-1}$, где 
\begin{align*}
\widehat{\mathbf{A}}&=\Expect_{\mathbf{x^*}}[e^{\mathbf{x}^{*\prime} \widehat{\mathbf{\beta_C}}} \mathbf{x^*} \mathbf{x^*}'], \\
\widehat{\mathbf{B}}&=N^{-1} \sum \limits_i (y_i \mathbf{x}_i - \Expect_{\mathbf{x^*}}[e^{\mathbf{x}^{*\prime} \widehat{\mathbf{\beta_C}}} \mathbf{x^*}]) (y_i \mathbf{x}_i - \Expect_{\mathbf{x^*}}[e^{\mathbf{x}^{*\prime} \widehat{\mathbf{\beta_C}}} \mathbf{x^*}])'.
\end{align*}

Накамура (Nakamura, 1990) ввёл сильную предпосылку о том, что ошибки измерений … имеют нормальное распределение $\mathcal{N}[\mathbf{0}, \, \mathbf{\Omega}]$. Тогда
\[
\exp (\mathbf{x^*}' \mathbf{\beta}) = \Expect_{\mathbf{x|x^*}} \left[ \exp \left(\mathbf{x}' \mathbf{\beta} - (\mathbf{\beta' \Omega \beta} /2) \right) \right].
\]
Согласно закону итерационных ожиданий,
\[
\Expect_{\mathbf{x^*}} [\exp (\mathbf{x^*}' \mathbf{\beta})] = \Expect_{\mathbf{x}} \left[ \exp \left(\mathbf{x}' \mathbf{\beta} - (\mathbf{\beta' \Omega \beta} /2) \right) \right],
\]
Что может быть состоятельно оценено, как $N^{-1} \sum \limits_i [ \exp (\mathbf{x}' \mathbf{\beta} - (\mathbf{\beta' \Omega \beta} /2) )]$. Следовательно, для $Q(\mathbf{\beta})$ в (26.23) предел по вероятности, приведённый в (26.24), сводится к
\[
plim Q(\mathbf{\beta}) = N^{-1} \sum \limits_i \left[ y_i \mathbf{x}'_i \mathbf{\beta} - \ln y_! - \exp \left(\mathbf{x}' \mathbf{\beta} - (\mathbf{\beta' \Omega \beta} /2) \right) \right].
\]
Это логарифм {\bf скорректированной функции правдоподобия}, предложенный Накамурой (Nakamura, 1990). Максимизация по $\mathbf{\beta}$ позволяет получить состоятельные оценки $\mathbf{\beta_0}$.

Подход Накамуры (Nakamura) напоминает один из способов оценивания линейной регрессии с ошибками измерений (см. (26.14)) при данной оценке ковариационной матрицы ошибок измерений. Как и в том случае, для максимизации corrected score функции Накамуры требуется знание о $\mathbf{\Omega}$, ковариационной матрице ошибок измерений. Она может быть получена из дублированных данных. Тем не менее, регрессоры преимущественно дискретны, так что предпосылка о нормальности ошибок измерений неправдоподобна. В таких случаях подход Гуо и Ли выглядит более привлекательным.

Для случая множественности регрессоров $\mathbf{x^*}$, вычисление $\Expect [\exp (\mathbf{x^*}' \mathbf{\beta})]$ непросто, даже если известно распределение $\mathbf{x^*}$, так как требуется работа с многомерными интегралами. Методы, основанные на симуляции (Li, 2002) обеспечивают возможное разрешение этой проблемы.

Применение некоторых других нелинейных моделей ошибок в переменных также требует реплицированных наблюдений; например, работы Хсяо (Hsiao, 1992) и Хаусмана, Ньюи и Поуэла (Hausman, Newey and Powell, 1995). Панельные данные могут обеспечить дублированные наблюдения на уровне индивидов. Например, рассмотрим случай, когда имеется скалярный регрессор $x^*$, для которого имеются две реплики измерений $x$, так как $x_{ij} = x_i + \epsilon_{ij}$ для $i=1,\dots,N$ и $j=1, \, 2$. Тогда оценка $\sigma^2_{\epsilon}$, полученная методом моментов, будет равна $\widehat{\sigma}^2_{\epsilon} = \sum \limits_i (x^2_{i1}+x^2_{i2} - 2x_{i1}x_{i2})/2N$. Таким образом, и среднее значение, и дисперсия $\mathbf{x^*}$ может быть оценена.
 
\section{Пример симуляции смещения затухания} 
Аналитические результаты для линейной модели приведены в Секции 26.2, но результаты в нелинейных моделях получить намного сложнее. Здесь мы приведём два примера симуляции, один из которых --- для логит модели, другой --- для линейной в логарифмах модели, которые иллюстрируют затухание в нелинейной регрессии с ошибками измерений в регрессорах.

В первом примере данные устроены как логит модель, где
\begin{align*}
y^* &=\alpha^* + \beta^* + \epsilon, \\
x^* &\sim \mathcal{U}[0, \, 1], \, \epsilon \sim \text{logistic}, \\
y &= 
\begin{cases}
0, \text{если} y^* &\leqslant0 ,\\
1, \text{если} y^* &>0.
\end{cases}
\end{align*}
Усложнением является то, что $x*$ измерен с ошибками, так что
\begin{align*}
x &=x^*+v, \\
v &\sim \mathcal{N}[0, \, \sigma^2_v].
\end{align*}
Так как $x^* \sim \mathcal{U}[0, \, 1]$ , он имеет дисперсию $\sigma^2_{x^*}=1/12$, и отношение шум--сигнал равно $s=12\sigma^2_v$. Была оценена логит регрессия $y$ на $x^*$.

Чтобы провести симуляцию, мы строим логит регрессию $y$ на $x$ для шести различных значений отношения шум--сигнал, включая 0, что будет являться базой для сравнения. Размер выборки --- 1000, количество проведённых симуляций --- 100. 

В Таблице 26.1 отражены средние значения $(\widehat{\alpha}, \, \widehat{\beta})$ за 100 дублей, где $\widehat{\alpha}$ и $\widehat{\beta}$ --- оценки свободного члена и коэффициента наклона из логит регрессии $y$ на $x$, но не правильной регрессии y на $x^*$, для выборки 1000 и шести различных значений $\sigma^2_v$, которые дают шесть различных значений отношения шум--сигнал $s$. Первая колонка со значением $s=0$ является базовой. Напомним, что для линейной МНК модели такой же постановки  мультипликативное смещение оценки коэффициента наклона равно $1/(1+s)$, или 0.96, 0.8, 0.5, 0.2 и 0.1 соответственно. В этом примере  направление смещений то же, но для случая логит модели они сравнительно больше.

Второй пример – парная линейная в логарифмах мультипликативная регрессионная модель с $\alpha=4$ и $\beta=0.4$ и аддитивными ошибками измерений обеих переменных. В этом случае постановка следующая:
\begin{align*}
y^* &= 4{x^*}^{0.4}u, \, u \sim \mathcal{N}[10, \, 0.0001], \\
x^* &= 100+ \mathcal{U}[0, \, 1], \\
y &= y^*+ \epsilon_y, \, \epsilon_y \sim \mathcal{N}[0, \, \sigma^2_y], \\
x &= x^*+ \epsilon_x, \, \epsilon_x \sim \mathcal{N}[0, \, \sigma^2_x].
\end{align*}
В симуляции используется выборка размером 1000, количество дублей --- 100. Мы меняем значение дисперсии $x^*$ от эксперимента к эксперименту, получая в результате следующие значения $\sigma^2_{x} / \sigma^2_{x^*}$: 0.001, 0.01, 0.1, 1,5, 10, 50, 100, 1000 и 5000.

В верхней строке Таблицы 26.2 приведены средние значения оценок коэффициента наклона в разных экспериментах, в которых менялось значение отношения шум--сигнал. Снова видно, что имеет место смещение затухания.

Оба примера дают результаты, которые являются состоятельными при гипотезе, лежащей в основе «железного закона эконометрики».

\section{Библиографические заметки} 
Вансбик и Мейер (Wansbeek and Meijer, 2000) является самой современной и полной написанной работой по ошибкам измерений с точки зрения эконометрики. В ней содержится глубокий анализ большего числа тем, рассмотренных в этой Главе с упором на линейные модели. Авторы также включили несколько глав, в которых ошибки измерения связаны с факторными моделями, моделями латентной переменной и моделями структурных уравнений. Говоря о результатах, авторы избегают фразы «это может быть показано», чтобы подробно их изложить. Также с эконометрической позиции, Хаусман (Hausman, 2000) провёл исследование последних результатов, полученных в работах его коллег и его самого. Боундб Браун и Матиевец (Bound, Brown and Mathiowets, 2001) исследовали вопросы ошибок измерений на рынке труда.
Проблема ошибок измерений хорошо представлена в статистической литературе. Полезна ссылка на работу Фуллера (Filler, 1987), в частности, его разбор подхода ортогональных регрессий, который применим в случае, если известно отношение шум-сигнал. Хотя наш анализ линейных моделей стандартен для эконометрической литературы, читателю также стоит ознакомиться с альтернативной моделью ошибок Берксона, в которой ненаблюдаемая истинная переменная предполагается константой, но несовершенство измерений является источником ошибок, а также с неоклассической моделью ошибок измерений, рассмотренной Ангристом и Крюгером (Angrist and Krueger, 1999). Мадански (Madansky, 1959) исследовал ряд более ранних результатов и подходов. Смотрите также работу Штефански (Stefanski, 2000).
\begin{enumerate}
\item Модели панельных данных с ошибками измерений анализируются в работе Бъёрна (Иощктб 1992).
\item Вызывающая интерес тема обратной регрессии рассматривалась Голдбергером (Goldberger, 1984) и Грином (Green, 1983) в их комментариях к работе Конвея и Робертса (Conway and Roberts, 1983). Лимер (Leamer, 1978) привёл глубокий анализ обратных регрессий с позиции Байеса. Хан и Хаусман (Hahn and Hausman, 2002) использовали идею обратных регрессий для спецификации теста на валидность инструментов в рамках проблемы ошибок измерений. Эта задача связана с тем, что доступные инструменты могут быть слабыми, что проводит к плохим оценкам. Идея Хана-Хаусмана состоит в получении оценок инструментальных переменных в прямой регрессии, где возникают ошибки измерений в правой части уравнения. Обратная регрессия имеет те же ошибки, но в левой части. Эта регрессия также оценивается с помощью инструментальных переменных, причем тех же, что использовались в прямой регрессии.
\item В литературе тема ошибок измерений в нелинейных моделях более распространена. Работа Y.Amemiya (1985) особенно полезная эконометристам. Со статистической точки зрения, Carroll et al. (1995) рассматривали нелинейные модели, особенно в классе обобщенных линейных моделей, с аддитивными ошибками измерений в регрессорах, используя различные методы, включая ряд подходов, в которых могут применяться реплицированные данные, если они доступны. Ли, Триведи и Гуо (Li, Trivedi and Guo, 2003) развивали и использовали модель ошибок измерений, в которых с ошибками измерена дискретная переменная.
\end{enumerate}


\section*{Упражнения} 
\begin{enumerate}
\item Пусть имеет место результат смещения размывания \emph{(attenuation bias)} для параметра наклона парной модели ошибок переменных (Уравнение 26.9 в Секции 26.2.3). Расширим эту модель, включив константу.
\begin{enumerate}
\item Найдите аналогичный результат  смещения из-за ошибки измерений для константы.
\item	Найдите аналогичный результат при идентификации границ (identification-in-bounds) для МНК-оценки свободного члена, сходного с Уравнением (26.12) в Секции 26.3.1.
\end{enumerate}
\item (Bollinger, 2003) Пусть есть линейная множественная регрессионная модель со скалярным регрессором x, который измерен с ошибками, и вектором регрессоров z, которые свободны от ошибок измерений.
\begin{enumerate}
\item Сохраняя предпосылки относительно ошибок измерений для парной модели ошибок переменных, распространите результат смещения размывания и результат при идентификации границ на этот случай.
\item Проверьте, чтобы новые результаты могли быть адаптированы для парного случая.
\end{enumerate}
\item (Wansbeek and Meijer, 2000) Пусть есть квадратичная регрессионная модель $\mathbf{y}=\alpha + \beta\mathbf{x^*}+\gamma\mathbf{{x^*}^2}+\epsilon$, где регрессор $\mathbf{x^*}=\mathbf{x}+\mathbf{v}$ с $\mathbf{x}$ --– наблюдаемым и $\mathbf{v}$ --– ошибкой измерений. Предположим, что $(\mathbf
x^*, \, \epsilon, \, \mathbf{v})$ попарно не коррелированы и нормально распределены с нулевым средним.
\begin{enumerate}
\item	Сравните смещение МНК-оценок $\beta$ и $\gamma$.
\item	Идентифицируема ли модель? Сравните последний результат с результатом оценивания парной линейной модели ошибок переменных.
\end{enumerate}
\item В литературе, посвященной, мобильности, связывающей поколения \emph{(intergenerational mobility)}, используется следующая модель (Solon, 1992; Zimmerman, 1992):
\begin{equation}
Y^{son}_i=\alpha + \beta Y^{father}_i + \epsilon^{son}_i,
\end{equation}
с $\epsilon \sim \text{iid} \mathcal{N}[0,\, \sigma^2]$. Здесь $Y$ – мера перманентного статуса (как перманентный доход) и $\beta$ измеряет степень приближения (стремления) к среднему экономическому статусу. Положим, что этот перманентный статус не наблюдается. Вместо него, текущий статус $Y_{it}$ наблюдается с $Y_{it}=Y_i+ \gamma X_{it}+ w_{it}$, так что $Y_{it}$  состоит из индивидуального фиксированного эффекта $Y_{i}$, относящегося к перманентному статусу, систематических факторов $X_{it}$ и \emph{(transitory)} ошибки $w_{it}$. Пусть $\widehat{\gamma}$ отражает посчитанный на основе МНК коэффициент и пусть
\[
Y_{it}-\widehat{\gamma}X_{it}= Y_i+ (\gamma - \widehat{\gamma})X_{it} + w_{it} = Y_i + v_{it}.
\]
\begin{enumerate}
\item Пусть $Y^{father}_i=T^{-1} \sum \limits^{T}_{t=1}Y^{father}_{it}$ отражает средний статус отца, который используется как независимая переменная, --- прокси для ненаблюдаемого перманентного статуса в (26.26). Пусть $\widehat{\beta}_{avg}$ отражает соответствующий коэффициент регрессии. Покажите, что $plim \widehat{\beta}_{avg}=\beta P_Y$, где $P_Y=n\sigma^2_Y/(\sigma^2_Y+T^{-1} \sigma^2_{\epsilon})$.
\item Предположим, что \emph{(transitory component)} доходов отца следует авторегрессионной схеме, $v^{father}_{it}=\rho v^{father}_{it}+\xi_{it}$, где $\xi \sim \text{iid} \mathcal{N}[0,\, \sigma^2_{\xi}]$, $i=1,\dots ,T$. Покажите, что теперь $plim \widehat{\beta}_{avg}=\beta P^*_Y$, где $P_Y=n\sigma^2_Y/(\sigma^2_Y+T^{-1} V)$ и $V=\sigma^2_{\xi}[T(1-\rho^2)]^{-1}[(1+2\rho\{ T-(1-\rho^T)/(1-\rho) \} /T(1-\rho)]$.
\end{enumerate}
\end{enumerate} 

