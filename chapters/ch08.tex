
\chapter{Тесты на спецификацию и выбор моделей}
\section{Введение}

Два важных практических аспекта микроэконометрического моделирования --- это выявление, является ли модель верно специфицированной, и выбор между альтернативными моделями. Для этих целей можно использовать методы проверки гипотез, представленные в предыдущей главе, особенно когда модели являются вложенными. В этой главе мы представим некоторые другие методы.

Во-первых, такие М-тесты, как тест на условные моменты, --- это тесты на проверку того, выполняются ли моментные условия, наложенные моделью, или нет. Такой подход близок к обобщённому методу моментов, за исключением того, что моментные условия не накладываются для оценивания, а наоборот используются для тестирования. Такие тесты концептуально очень отличаются от тестов на проверку гипотез, представленных в главе 7, так как нет явной формулировки модели, которая получается при альтернативной гипотезе.

Во-вторых, тесты Хаусмана --- это тесты на разницу между двумя оценками, обе из которых состоятельны, если модель верно специфицирована, но они расходятся, если модель неверно специфицирована.

В-третьих, тесты невложенных моделей требуют специальных методов, поскольку стандартный подход к проверке гипотез может быть применён только, когда одна модель получается при наложении ограничений на другую.

Наконец, бывает полезно рассчитывать статистику адекватности модели, которая не является тестовой статистикой. Например, можно сконструировать аналог $R^2$ для измерения качества подгонки нелинейной модели.

В идеальном случае эти методы используются в процессе спецификации, оценивания, тестирования и трактовки модели, что может способствовать переходу от общей к конкретной модели или от конкретной к более общей модели, которая отражает наиболее важные особенности данных.

В разделе 8.2 описаны М-тесты, в том числе тесты на условный момент, тест информационных матриц и хи-квадрат тест на качество подгонки. Тест Хаусмана рассмотрен в разделе 8.3. Тесты на несколько часто встречающихся неверных спецификаций приведены в разделе 8.4. Выбор между невложенными моделями подробно описан в разделе 8.5. Часто используемые тесты, приведённые в разделах 8.2 --- 8.5, могут опираться на сильные предположения о распределении и/или могут давать плохие результаты на малых выборках. Эти недостатки останавливают многих от применения этих тестов, но эти проблемы устарели, потому что во многих случаях метод бутстрэп, изложенный в главе 11, может исправить эти недостатки. Раздел 8.6 рассматривает последствия тестирования модели на последующие выводы. Диагностика модели представлена отдельно в разделе 8.7.

\section{М-тесты}

М-тесты такие, как тесты на условный момент, являются тестовой процедурой проверки общей спецификации, которая охватывает много стандартных тестов на спецификацию. Тесты легко реализовать, используя вспомогательные регрессии, когда оценки получают с помощью метода максимального правдоподобия. Это тот случай, когда особенно желательно провести тест модельных предположений. Реализация тестов, как правило, затруднена, если оценки основаны на минимальных предположениях о распределении.

Мы сначала определим тестовую статистику и вычислительные методы, а затем приведём примеры и проиллюстрируем тесты.

\subsection{М-тестовая статистика}

Предположим, что модель предполагает условие для теоретического момента
\begin{equation}
H_0: \E[m_i(w_i, \theta)] = 0,
\end{equation}
где $w$ представляет собой вектор наблюдаемых переменных, как правило, зависимой переменной $y$ и регрессоров $x$, а иногда и дополнительных переменных $z$, $\theta$ --- вектор параметров размера $q \times 1$, и $m_i(\cdot)$ --- вектор размера $h \times 1$. Например, в простом случае, $\E[(y = x'\beta)z] = 0$, если $z$ может быть опущен в линейной модели $y = x'\beta + u$. Особенно для полностью параметрических моделей существует много вариантов для $m_i(\cdot)$.

М-тест --- это тест на близость к нулю соответствующего момента выборки 
\begin{equation}
\hat{m}_N (\hat{\theta}) = N^{-1} \sum_{i=1}^N m_i(w_i, \hat{\theta}).
\end{equation}
Этот подход аналогичен тесту Вальда, где $h(\theta) = 0$ проверяется путём тестирования близости $h(\hat{\theta})$ к нулю.

Тестовую статистику получают методом, подобным методу для теста Вальда, описанному в разделе 7.2.4. В разделе 8.2.3 показано, что если (8.1) выполнено, то
\begin{equation}
\sqrt{N}\hat{m}_N (\hat{\theta}) \stackrel{d}{\rightarrow} \mathcal{N}[0,V_m],
\end{equation} 
где $V_m$, которая будет определена далее в (8.10), имеет более сложную структуру, чем в случае теста Вальда, потому что $m_i(w_i, \hat{\theta})$ имеет два источника стохастической вариации: $w_i$ и $\hat{\theta}$ являются случайными величинами. Хи-квадрат тестовая статистика может быть получена с помощью соответствующей квадратичной формы. Таким образом, тестовая М-статистика для (8.1) имеет вид:
\begin{equation}
M = N\hat{m}_N (\hat{\theta})'\hat{V}_m^{-1}\hat{m}_N (\hat{\theta}),
\end{equation}
и имеет асимптотическое $\chi^2(\rank(V_m))$ распределение, если моментные условия (8.1) являются верными. М-тест отвергает моментные условия из (8.1) на уровне значимости $\alpha$, если $M > \chi_{\alpha}^2(h)$, и не отвергает иначе.

Сложность может заключается в том, что матрица $V_m$ может не иметь полного ранга $h$. Например, такое может быть, если оценка $\hat{\theta}$ получается при приравнивании линейной комбинации компонент $\hat{m}_N (\hat{\theta})$ к нулю. В некоторых случаях таких, как тест на сверх-идентифицирующие ограничения (OIR), $\hat{V}_m$ по-прежнему имеет полный ранг и можно вычислить $M$, но хи-квадрат статистика имеет только $\rank[V_m]$ степеней свободы. В других случаях матрица $V_m$ не будет иметь полный ранг. Тогда проще всего откинуть $(h - \rank[V_m])$ моментных условий и провести М-тест, используя только это подмножество моментных условий. Полный набор моментных условий может быть использован, но $\hat{V}_m^{-1}$ из (8.4) заменяется $\hat{V}_m^{-}$, обобщённой обратной матрицей $\hat{V}_m$. Обобщённая матрица Мура-Пенроуза $V^{-}$, обратная $V$,  удовлетворяет условиям $VV^{-}V = V$, $V^{-}VV^{-} = V^{-}$, $(VV^{-})' = VV^{-}$ и $(V^{-}V)' = V^{-}V$. Когда $V_m$ имеет неполный ранг, строго говоря, (8.3) больше не выполняется, так как невырожденное многомерное нормальное распределение требует полного ранга $V_m$. Однако (8.4) по-прежнему выполняется с учётом этих корректировок.
 
Подход М-теста концептуально очень прост. Моментные ограничения (8.1) отвергаются, если квадратичная форма выборочной оценки (8.2) находится достаточно далеко от нуля. Проблемы, которые могут возникнуть, связаны с расчётом $M$, так как $\hat{V}_m$ может иметь достаточно сложный вид (см. раздел 8.2.2), с выбором моментов $m(\cdot)$ для проверки (см. основные примеры из разделов 8.2.3 --- 8.2.6) и интерпретацией причин, из-за которых (8.1) отвергается (см. раздел 8.2.8).

\subsection{Расчёт М-статистики}

Есть несколько способов вычислить М-статистику.

Во-первых, всегда можно вычислить непосредственно $\hat{V}_m$ и, соответственно, $M$, используя состоятельные оценки компонент $V_m$, которые приведены в разделе 8.2.3. Большинство практиков уклоняются от такого подхода, поскольку он требует матричных вычислений.

Во-вторых, всегда можно использовать бутстрэп (см. раздел 11.6.3), так как этот метод позволяет найти оценку $V_m$, которая отвечает за все возможные случайности в $\hat{m}_N (\hat{\theta}) = N^{-1}\sum_i m_i(w_i, \hat{\theta})$.  

В-третьих, чтобы вычислить асимптотически эквивалентные версии $M$, которые не требуют вычисления $\hat{V}_m$, в некоторых случаях можно провести вспомогательные регрессии, аналогичные регрессиям для теста множителей Лагранжа, которые приведены в разделе 7.3.5. Эти вспомогательные регрессии могут быть проведены с методом бутстрэп, чтобы получить асимптотическое уточнение (см. раздел 11.6.3). Мы приведём несколько основных вспомогательных регрессий.

\begin{center}
Вспомогательные регрессии с использованием оценок, полученных методом максимального правдоподобия
\end{center}

Тесты на спецификацию моделей особенно желательно проводить, когда вывод делается в рамках метода правдоподобия, так как в общем случае любая неверная спецификация плотности может привести к несостоятельности метода максимального правдоподобия. К счастью, легко реализовать М-тест, когда оценку получают методом максимального правдоподобия.

В частности, если $\hat{\theta}$ --- оценка, полученная методом максимального правдоподобия, обобщение результата теста множителей Лагранжа из раздела 7.3.5 (см. раздел 8.2.3) даёт асимптотически эквивалентную версию М-теста при проведении вспомогательной регрессии
\begin{equation}
1 = \hat{m}_i' \delta + \hat{s}_i' \gamma + u_i,
\end{equation}
где $\hat{m}_i = m_i(y_i, x_i, \hat{\theta}_{ML})$, $\hat{s}_i = \partial{lnf(y_i|x_i, \theta)}/\partial{\theta}|_{\hat{\theta}_{ML}}$ --- это вклад $i$-того наблюдения в скор-функцию, и $f(y_i|x_i, \theta)$ --- это условная функция плотности при расчёте
\begin{equation}
M^{*} = NR_u^2,
\end{equation}
где $R_u^2$ --- нецентрированный $R^2$, который определён в конце раздела 7.3.5. Другими словами, $M^{*}$ равно $ESS_u$, нецентрированной объяснённой сумме квадратов (сумме квадратов оценочных значений) из регрессии (8.5), или $M^{*}$ равно $N - RSS$, где $RSS$ --- это сумма квадратов остатков из регрессии (8.5). Статистика $M^{*}$ имеет асимптотическое $\chi^2(h)$ распределение при нулевой гипотезе.

Тестовая статистика $M^{*}$ --- это внешнее произведение градиента для М-теста, и это является обобщением вспомогательной регрессии для теста множителей Лагранжа (см. раздел 7.3.5). Несмотря на то, что внешнее произведение градиента можно легко вычислить, на малых выборках статистика имеет плохие свойства с большими искажениями размера теста. Однако аналогично тесту множителей Лагранжа эти проблемы для малых выборок могут быть значительно уменьшены с помощью использования метода бутстрэп (см. раздел 11.6.3).

Тестовая статистика $M^{*}$ также может быть уместна в условиях, отличных от условий метода максимального правдоподобия. Вспомогательная регрессия применяется всегда, когда выполняется $\E[\partial{m}/\partial{\theta}'] = - \E[ms']$ (см. раздел 8.2.3). С помощью обобщённого равенства информационных матриц (см. раздел 5.6.3), это условие выполняется для метода максимального правдоподобия, когда математическое ожидание считается по указанной функции плотности $f(\cdot)$. Оно также может выполняться при более слабых предположениях о распределении в некоторых случаях.

\begin{center}
Вспомогательные регрессии в случае $\E[\partial{m}/\partial{\theta}'] = 0$
\end{center}

В некоторых случаях $m_i(w_i, \theta)$ удовлетворяет условию
\begin{equation}
\E[\partial{m_i(w_i,\theta)}/\partial{\theta}'|_{\theta_0}] = 0,
\end{equation}
в дополнении к (8.1).

Тогда можно показать, что асимптотическое распределение $\sqrt{N} \hat{m}_N(\hat{\theta})$ не отличается от $\sqrt{N}m_N(\theta_0)$. Таким образом, $V_m = \plim N^{-1}\sum_i m_{i0}m_{i0}'$, а её оценка $\hat{V}_m = \plim N^{-1}\sum_i \hat{m}_i\hat{m}_i'$. Тестовую статистику можно вычислить способом, аналогичным (8.5), но вспомогательная регрессия в данном случае будет более простой:
\begin{equation}
1 = \hat{m}_i \delta + u_i,
\end{equation}
с тестовой статистикой $M^{**}$, которая равна $NR_u^2$.

Эта вспомогательная регрессия действительна для любой $\sqrt{N}$ состоятельной оценки $\hat{\theta}$, а не только
для оценок, полученных методом максимального правдоподобия, при условии, что (8.7) выполняется. Условие (8.7) выполняется в нескольких примерах, см. раздел 8.2.9.

Даже если (8.7) не выполняется, можно провести более простую регрессию (8.8), так как она задаёт нижнюю границу верного значения $M$ для М-тестовой статистики. Если эта более простая регрессия приводит к тому, что нулевая гипотеза отвергается, то (8.1) точно отвергается.

\begin{center}
Другие вспомогательные регрессии
\end{center}

Возможны регрессии, которые альтернативны (8.5) и (8.8), если $m(y, x, \theta)$ и $s(y, x, \theta)$ могут быть разложены на сомножители.

Во-первых, если $s(y, x, \theta) = g(x, \theta)r(y, x, \theta)$ и $m(y, x, \theta) = h(x, \theta)r(y, x, \theta)$ для некоторой простой скалярной функции $r(\cdot)$ с $\V[r(y, x, \theta)] = 1$ и если оценивание происходит с помощью метода максимального правдоподобия, тогда асимптотически эквивалентной (8.5) регрессией будет $NR_u^2$ из регрессии $\hat{r}_i$ на $\hat{g}_i$ и $\hat{h}_i$.

Во-вторых, если $m(y, x, \theta) = h(x, \theta)v(y, x, \theta)$ для некоторой скалярной функции $v(\cdot)$ с $\V[v(y, x, \theta)] = 1$ и $\E[\partial{m}/\partial{\theta}'] = 0$, тогда асимптотически эквивалентной (8.8) регрессией будет $NR_u^2$ из регрессии $\hat{v}_i$ на $\hat{h}_i$. Подробности представлены в книге Вулдриджа (1991).

Дополнительные вспомогательные регрессии существуют и в других частных случаях. Примеры приведены в разделе 8.4, а также Уайт (1994) приводит довольно общее описание.

\subsection{Вывод тестовых М-статистик}

Чтобы избежать необходимости вычислять $V_m$, ковариационную матрицу из (8.3), обычно проводятся М-тесты с помощью вспомогательных регрессий или метода бутстрэп. Для полноты в этом разделе представлены фактическое выражение для $V_m$ и обоснование вспомогательных регрессий (8.5) и (8.8).

Основная задача --- это получение распределения $\hat{m}_N(\hat{\theta})$ из (8.2). Задача осложняется тем, что $m_N(\hat{\theta})$ является стохастической матрицей по двум причинам: из-за случайных величин $w_i$ и оценивания в точке $\hat{\theta}$.

Предположим, что $\hat{\theta}$ является М-оценкой, т.е. является решением 
\begin{equation}
\frac{1}{N} \sum_{i=1}^N s_i(w_i, \hat{\theta}) = 0,
\end{equation}
для некоторой функции $s(\cdot)$, возможно несовпадающей с $\partial{\ln f(y|x, \theta)}/\theta$ в данном случае. Также необходимо сделать обычное для пространственных данных предположение о том, что данные независимы по $i$. Тогда мы покажем, что $\sqrt{N}\hat{m}_N(\hat{\theta}) \stackrel{d}{\rightarrow} \mathcal{N}[0, V_m]$, как и в (8.3), где
\begin{equation}
V_m = H_0J_0H_0',
\end{equation} 
матрица размера $h \times (h + q)$
\begin{equation}
H_0 = [I_h - C_0A_0^{-1}],
\end{equation}
где $C_0 = \plim N^{-1} \sum_i \partial{m_{i0}}/\partial{\theta}'$ и $A_0 = \plim N^{-1} \sum_i \partial{s_{i0}}/\partial{\theta}'$, и матрица размера $(h + q) \times (h + q)$
\begin{equation}
J_0 = \plim N^{-1} \begin{bmatrix} \sum_{i=1}^N m_{i0} m_{i0}' &  \sum_{i=1}^N m_{i0} s_{i0}' \\ \sum_{i=1}^N s_{i0} m_{i0}' & \sum_{i=1}^N s_{i0} s_{i0}' \end{bmatrix},
\end{equation}
где $m_{i0} = m_i(w_i, \theta_0)$ и $s_{i0} = s_i(w_i, \theta_0)$.

Чтобы вывести (8.10), возьмём разложение в ряд Тейлора первого порядка в окрестности точки $\theta_0$ и получи
\begin{equation}
\sqrt{N}\hat{m}_N(\hat{\theta}) = \sqrt{N}m_N(\theta_0) + \frac{\partial{m_N(\theta_0)}}{\partial{\theta}'} \sqrt{N}(\hat{\theta} - \theta_0) + o_p(1).
\end{equation}

Для $\hat{\theta}$, определённой в (8.9), это означает, что
\begin{equation}
\sqrt{N}\hat{m}_N(\hat{\theta}) = \frac{1}{\sqrt{N}} \sum_{i=1}^N m_i(\theta_0) - C_0A_0^{-1}\frac{1}{\sqrt{N}} \sum_{i=1}^N s_{i0} + o_p(1), 
\end{equation}
где $m_N = N^{-1} \sum_i m_i$, $\partial{m_N}/\partial{\theta}' =  N^{-1} \sum_i \partial{m_i}/\partial{\theta}' \stackrel{p}{\rightarrow} C_0$, и $\sqrt{N}(\hat{\theta} - \theta_0)$ имеет то же самое предельное распределение, что и $A_0^{-1}N^{-1/2} \sum_i s_{i0}$, при применении обычного разложения в ряд Тейлора до первого члена для (8.9). Уравнение (8.14) можно переписать как
\begin{equation}
\sqrt{N}\hat{m}_N(\hat{\theta}) = \begin{bmatrix} I_h & - C_0A_0^{-1} \end{bmatrix} \begin{bmatrix}  \frac{1}{\sqrt{N}}\sum_{i=1}^N m_{i0} \\ \frac{1}{\sqrt{N}}\sum_{i=1}^N s_{i0} \end{bmatrix} + o_p(1).
\end{equation}

Уравнение (8.10) вытекает из применения теоремы о нормальности предела произведения (теорема А.17), так как второй член произведения в (8.15) имеет предельное нормальное распределение при нулевой гипотезе с нулевым математическим ожиданием и дисперсией $J_0$.

Для вычисления $M$ из (8.4) может быть получена состоятельная оценка $\hat{V}_m$ для $V_m$ с помощью замены каждого компонента $V_m$ на его состоятельную оценку. Например, для $C_0$ можно получить состоятельную оценку $\hat{C} = N^{-1}\sum_i \partial{m}_i/\partial{\theta'}|_{\hat{\theta}}$. Вариант с использованием вспомогательных регрессий реализовать легче, если они доступны.

Во-первых, рассмотрим вспомогательную регрессию (8.5), где $\hat{\theta}$ --- это оценка, полученная методом максимального правдоподобия. С помощью обобщённого равенства информационных матриц (см. раздел 5.6.3) $\E[\partial{m_{i0}}/\partial{\theta'}] = - \E[m_{i0}s_{i0}']$, где для случая метода максимального правдоподобия мы рассмотрим $s_i = \partial{\ln f(y_i, x_i, \theta)}/\partial{\theta'}$. Можно значительно упростить выражение, так как $C_0 = - \plim N^{-1}\sum_i m_{i0}s_{i0}'$ и $A_0 = - \plim N^{-1}\sum_i s_{i0}s_{i0}'$, которые тоже встречаются в матрице $J_0$. Это приводит к тесту в виде внешнего произведения градиента. Более подробно можно посмотреть в книгах Ньюи (1985) или Пагана и Велла (1989).

Во-вторых, для вспомогательной регрессии (8.8) обратите внимание, что $\E[\partial{m_{i0}}/\partial{\theta'}] = 0$, и тогда $C_0 = 0$, то $H_0 = \begin{bmatrix} I_h & 0\end{bmatrix}$ и $H_0J_0H_0' = \plim N^{-1}\sum_i m_{i0}m_{i0}'$.

\subsection{Тесты на условный момент}

Тесты на условный момент, введённые Ньюи (1985) и Таушеном (1985), --- это М-тесты на безусловные моментные ограничения, которые получаются из лежащего в их основе условного моментного ограничения.

В качестве примера рассмотрим модель линейной регрессии $y = x'\beta + u$. Стандартное предположение о состоятельности МНК-оценки состоит в том, что ошибка имеет нулевое условное математическое ожидание или, что  условное моментное ограничение имеет вид:
\begin{equation}
\E[y - x'\beta|x] = 0.
\end{equation}
В главе 6 мы рассматривали использование некоторых накладываемых безусловных моментных ограничений в качестве основы для оценивания методом моментов или обобщённым методом моментов. В частности (8.16) подразумевает, что $\E[x(y - x'\beta)] = 0$. Решение соответствующего выборочного моментного условия $\sum_i x_i(y_i - x_i'\beta) = 0$ приводит к получению МНК-оценки $\beta$. Тем не менее, (8.16) требует многих других моментных условий, которые не используются для оценивания. Рассмотрим безусловное моментное ограничение
\[
\E[g(x)(y - x'\beta)] = 0,
\]
где вектор $g(x)$ должен отличаться от $x$, который уже используется в оценивании МНК. Например, $g(x)$ может содержать квадраты и смешанные произведения компонентов вектора $x$. Это наводит на мысль о тесте, основанном на том, близок ли соответствующий выборочный момент $\hat{m}_N(\hat{\theta}) = N^{-1}\sum_i g(x_i)(y_i - x_i'\hat{\beta})$ к нулю или нет.

В более общем случае рассмотрим условное моментное ограничение
\begin{equation}
\E[r(y, x, \theta)|x] = 0,
\end{equation}
для некоторой скалярной функции $r(\cdot)$. Тест на условный момент --- М-тест, основанный на безусловных моментных ограничениях
\begin{equation}
\E[g(x)r(y, x, \theta)] = 0,
\end{equation}
где $g(x)$ и/или $r(y, x, \theta)$ выбраны так, чтобы эти ограничения ранее не использовались при оценивании.

Модели, основанные на методе максимального правдоподобия, приводят к многим потенциальным ограничениям. Для неполностью параметрических моделей примеры $r(y, x, \theta)$ включают $y - \mu(x, \theta)$, где $\mu(\cdot)$ --- заданная функция условного математического ожидания, и $(y - \mu(x, \theta))^2 - \sigma^2(x, \theta)$, где $\sigma^2(x, \theta)$ --- заданная функция условной дисперсии.

\subsection{Информационный матричный тест Уайта}

Для оценивания методом максимального правдоподобия равенство информационных матриц приводит к моментным ограничениям, которые могут быть использованы и в М-тесте, так как они, как правило, не используются при получении оценки метода максимального правдоподобия.

В частности, из раздела 5.6.3 равенство информационных матриц приводит к тому, что
\begin{equation}
\E[\Vech[D_i(y_i, x_i, \theta_0)]] = 0,
\end{equation}
где матрица $D_i$ имеет размера $q \times q$ и представляется в виде:
\begin{equation}
D_i(y_i, x_i, \theta_0) = \frac{\partial^{2}{\ln f_i}}{\partial{\theta}\partial{\theta}'} + \frac{\partial{\ln f_i}}{\partial{\theta}} \frac{\partial{\ln f_i}}{\partial{\theta'}},
\end{equation}
и математическое ожидание берётся согласно предполагаемой условной функции плотности $f_i(y_i| x_i, \theta)$. Здесь $\Vech$ --- оператор, который ставит столбцы матрицы $D_i$ таким же образом, как и $Vec$ оператор. Отличие состоит в том, что только $q(q + 1)/2$ уникальных элементов симметричной матрицы $D_i$ размещаются в столбец.

Уайт (1982) предложил информационно-матричный тест, который проверяет, близок ли соответствующий выборочный момент
\begin{equation}
\hat{D}_N(\hat{\theta}) = N^{-1}\sum_{i=1}^N \Vech[D_i(y_i, x_i, \hat{\theta}_{ML})]
\end{equation}
к нулю или нет. Применяя (8.4), получаем, что тестовая статистика в данном случае
\begin{equation}
IM = N\hat{d}_N(\hat{\theta})'\hat{V}^{-1}\hat{d}_N(\hat{\theta}),
\end{equation}
где выражение для $\hat{V}$, предложенное Уайтом (1982), имеет довольно сложный вид. Гораздо проще провести тест, как утверждают Ланкастер (1984) и Чешер (1984), при помощи вспомогательной регрессии (8.5), которую можно проводить, так как в (8.21) используется метод максимального правдоподобия.

Информационно-матричный тест также может быть применён и для подмножества ограничений (8.19). Так следует поступать, если $q$ велико, так как в этом случае количество проверяемых ограничений $q(q + 1)/2$ очень велико.

Большие значения статистики информационно-матричного теста приводят к отвержению ограничений равенства информационных матриц и выводу, что плотность неверно специфицирована. В общем случае это означает, что оценка, полученная методом максимального правдоподобия, является несостоятельной. В некоторых особых случаях, описанных в разделе 5.7, оценка, полученная методом максимального правдоподобия, всё же может быть состоятельной, хотя стандартные ошибки в этом случае должны быть основаны на сэндвич форме ковариационной матрицы.

\subsection{Хи-квадрат тест на качество подгонки}

Полезный тест на спецификацию для полностью параметрических моделей --- сравнение предсказанных вероятностей с относительными частотами из выборки. Модель является плохой, если эти величины существенно различаются.

Начнём с дискретных одинаково распределённых случайных переменных $y$, которые могут принимать одно из $J$ возможных значений с вероятностью $p_1, p_2, \dots, p_j$, $\sum_{j=1}^J p_j = 1$.

Верная спецификация вероятностей может быть проверена с помощью теста на равенство теоретических частот $Np_j$ и наблюдаемых частот $N\bar{p}_j$, где $\bar{p}_j$ --- доля выборки, которая принимает $j$-ое возможное значение. Статистика хи-квадрат теста Пирсона на качество подгонки имеет вид:
\begin{equation}
PCGF = \sum_{j=1}^J \frac{(N\bar{p}_j - Np_j)^2}{Np_j}.
\end{equation}
Эта статистика имеет асимптотическое $\chi^2(J-1)$ распределение при нулевой гипотезе о том, что вероятности $p_1, p_2, \dots, p_j$ являются верными. Тест может быть обобщен на прогнозирование вероятностей из регрессии (см. упражнение 8.2). Рассмотрим мультиномиальную модель для дискретных $y$ с вероятностями $p_{ij} = p_{ij}(x_i, \theta)$. Тогда $p_j$ из (8.23) заменяется на $\hat{p}_j = N^{-1}\sum_i F_j(x_i, \hat{\theta})$ и, если $\hat{\theta}$ --- это мультиномиальная оценка, полученная методом максимального правдоподобия, мы опять получаем хи-квадрат распределение, но уже с ограниченным числом степеней свободы $(J - \dim(\theta) - 1)$, которые получаются вследствие оценивания $\theta$ (см. Эндриус, 1988а).

Для регрессионных моделей отличных от мультиномиальных, статистика $PCGF$ из (8.23) может быть вычислена путём группировки $y$ в интервалы, но в этом случае статистика $PCGF$ больше не имеет хи-квадрат распределение. Вместо этого применяется похожая статистика М-теста. Чтобы вывести эту статистику, надо разбить диапазон $y$ на $J$ взаимоисключающих интервалов, где $J$ интервалов охватывают все возможные значения $y$. Пусть $d_{ij}$ --- переменная-индикатор, равная единице, если $y_i \in$ интервале $j$, и равная нулю в противном случае. Пусть $p_{ij}(x_i, \theta) = \int_{y_i \in cell j} f(y_i| x_i, \theta)dy_i$ --- спрогнозированная вероятность того, что наблюдение $i$ попадает в интервал $j$, где $f(y|x, \theta)$ --- условная плотность $y$. Также для начала мы предполагаем, что вектор параметров $\theta$ известен. Если условная плотность верно специфицирована, то
\begin{equation}
\E[d_{ij}(y_i) - p_{ij}(x_i, \theta)] = 0, j = 1, \dots, J.
\end{equation}
Объединяя все $J$ моментов в единый вектор, мы получаем
\begin{equation}
\E[d_i(y_i) - p_i(x_i, \theta)] = 0,
\end{equation}
где $d_i$ и $p_i$ --- это векторы размера $J \times 1$ с $j$-тыми элементами $d_{ij}$ и $p_{ij}$. Мы приходим к  М-тесту на близость к нулю соответствующего выборочного момента
\begin{equation}
\widehat{dp}_N(\hat{\theta}) = N^{-1}\sum_{i=1}^N (d_i(y_i) - p_i(x_i, \hat{\theta})),
\end{equation}
который представляет собой разницу между вектором относительных выборочных частот $N^{-1}\sum_i d_i$ и вектором предсказанных частот $N^{-1}\sum_i \hat{p}_i$. Используя (8.5), мы получаем хи-квадрат статистику теста Эндриуса на качество подгонки (1988а, 1988б):
\begin{equation}
CGF = N\widehat{dp}_N(\hat{\theta})'\hat{V}^{-1}\widehat{dp}_N(\hat{\theta}), 
\end{equation}
где выражение для $\hat{V}$ имеет довольно сложный вид. Статистику $CGF$ теста можно легко посчитать, используя вспомогательную регрессию (8.5), с $\hat{m}_i = d_i - \hat{p}_i$. Эта вспомогательная регрессия здесь уместна, потому что тестируется полностью параметрическая модель и поэтому $\hat{\theta}$ будет получена с помощью метода максимального правдоподобия.

Необходимо отбросить одну из категорий из-за  ограничения, что сумма вероятностей равнялась единице. Мы получаем тестовую статистику, которая имеет асимптотическое $\chi^2(J - 1)$ распределение при нулевой гипотезе о том, что $f(y|x, \theta)$ верно специфицирована. Возможно, будет необходимо отбросить другие категории в некоторых особых случаях, таких, как мультиномиальный пример, который обсуждался после (8.23). В дополнение к рассчитанной тестовой статистике может быть полезно посмотреть на компоненты $N^{-1}\sum_i d_i$ и $N^{-1}\sum_i \hat{p}_i$.

Соответствующая асимптотическая теории рассматривается Эндриусом (1988a), а также более простая интерпретация и несколько приложений приведены в книге Эндриуса (1988б). Для простоты мы представили группировку, которая определяется значением $y$, но разделение может быть и по $y$, и по $x$. Группировка наблюдений должна быть проведена таким образом, чтобы в каждую группу попало достаточное число наблюдений. Более подробная информация и история этого этого теста представлены в этих статьях.

Для непрерывной случайной величины $y$ в случае одинаково распределённых $y$ более общим тестом, чем тест $SCGF$, является тест Колмогорова, который использует всё распределение $y$, а не только группы наблюдений, образованные $y$.  Эндриус (1997) представляет регрессионную версию теста Колмогорова, но её гораздо сложнее реализовать, чем тест $CGF$.

\subsection{Тест на сверх-идентифицирующие ограничения}

Тесты на сверх-идентифицирующие предположения (см. раздел 6.3.8) являются примерами М-тестов.

В обозначениях главы 6 оценки, полученные обобщённым методом моментов, основаны на предположении о том, что $\E[h(w_i,\theta)] = 0$. Если модель сверх-идентифицирована, то только $q$ из этих моментных ограничений используются для оценивания, что приводит к $(r - q)$ линейно зависимым ортогональным условиям, где $r = \dim[h(\cdot)]$. Их можно использовать для проведения М-теста. Тогда мы используем $M$ из (8.4), где $\hat{m}_N = N^{-1}\sum_i h(w_i, \hat{\theta})$. 

Как показано в разделе 6.3.9, если $\hat{\theta}$ --- оптимальная оценка, полученная обобщённым методом моментов, то $\hat{m}_N(\hat{\theta})'\hat{S}_N^{-1}\hat{m}_N(\hat{\theta})$, где $\hat{S}_N = N^{-1}\sum_{i=1}^N \hat{h}_i\hat{h}_i'$ имеет асимптотическое $\chi^2(r - q)$ распределение. Более интуитивный линейный пример метода инструментальных переменных приведён в разделе 8.4.4.

\subsection{Мощность и состоятельность тестов на условный момент}

Так как нет никаких явных альтернативных гипотез, М-тесты отличаются от тестов из главы 7.

Некоторые авторы приводили примеры, где может быть показано, что информационно матричный тест эквивалентен обычному тесту множителей Лагранжа нулевой гипотезы против альтернативной. Чешер (1984) интерпретировал информационно матричный тест как тест на неоднородность случайного параметра. Для линейной модели с нормальным распределением остатков А. Холл (1987) показал, что подкомпоненты информационно матричного теста соответствуют тестам множителей Лагранжа на гетероскедастичность, симметрию и эксцесс. Кэмерон и Триведи (1998) приводят некоторые дополнительные примеры и объяснения результатов для линейного экспоненциального семейства.

В более общем случае М-тесты могут быть проинтерпретированы в рамках условного момента. Начнём с теста на добавленную переменную в модели линейной регрессии. Предположим, мы хотим проверить, выполняется ли $\beta_2 = 0$ в модели $Y = x_1'\beta_1 + x_2'\beta_2 + u$. Это тест на проверку гипотезы $H_0: \E[y - x_1'\beta_1|x] = 0$ против альтернативной гипотезы $H_a: \E[y - x_1'\beta_1|x] = x_2'\beta_2$. Самый мощный тест $\beta_2 = 0$ в регрессии $y - x_1'\beta_1$ на $x_2$ основан на эффективной оценки, полученной с помощью обобщённого метода моментов,
\[
\hat{\beta}_2 = \left[ \sum_{i=1}^N \frac{x_{2i}x_{2i}'}{\sigma_i^2}\right]^{-1} \sum_{i=1}^N \frac{x_{2i}(y_i - x_{1i}'\beta_1)}{\sigma_i^2},
\]
где $\sigma_i^2 = \V[y_i|x_i]$ при нулевой гипотезе и предполагается независимость по $i$. Этот тест эквивалентен тесту, основанному на второй сумме в отдельности, который является М-тестом на
\begin{equation}
\E\left[ \frac{x_{2i}(y_i - x_{1i}'\beta_1)}{\sigma_i^2} \right] = 0.
\end{equation}
Смотря на этот процесс с обратной стороны, мы можем интерпретировать М-тест, основанный на (8.28), как тест на условный момент на проверку гипотезы $H_0: \E[y - x_1'\beta_1|x] = 0$ против альтернативной гипотезы $H_a: \E[y - x_1'\beta_1|x] = x_2'\beta_2$. Также М-тест, основанный на $\E[x_2(y - x_1'\beta_1)] = 0$, можно интерпретировать как тест на условный момент на проверку гипотезы $H_0: \E[y - x_1'\beta_1|x] = 0$ против альтернативной гипотезы $H_a: \E[y - x_1'\beta_1|x] = \sigma_{y|x}^2 x_2'\beta_2$, где $\sigma_{y|x}^2 = \V[y|x]$ при нулевой гипотезе.

В более общем случае предположим, что мы начнём с условного моментного ограничения
\begin{equation}
\E[r(y_i, x_i, \theta)|x_i] = 0,
\end{equation}
для некоторой скалярной функции $r(\cdot)$. Тогда М-тест, основанный на безусловном моментном ограничении
\begin{equation}
\E[g_i(x_i)r(y_i, x_i, \theta)] = 0,
\end{equation}
может быть интерпретирован как тест на условный момент с нулевой и альтернативной гипотезами
\begin{equation}
H_0: \E[r(y_i, x_i, \theta)|x_i] = 0, H_a: \E[r(y_i, x_i, \theta)|x_i] = \sigma_i^2 g(x_i)'\gamma,
\end{equation}
где $\sigma_i^2 = \V[r(y_i, x_i, \theta)|x_i]$ при нулевой гипотезе.

Такой подход позволяет понять, когда тест на условный момент имеет большую мощность. Хотя (8.30) предполагает, что мощность возрастает по $g(x)$, из (8.31) следует более точное утверждение о том, что мощность растёт с увеличение произведения  $g(x)$ на дисперсию $r(y, x, \theta)$. Это уточнение имеет важное значение для приложений к пространственным данным, потому что дисперсия не является постоянной для всех наблюдений. Более подробная информация и ссылки представлены в книге Кэмерона и Триведи (1998), которые называют этот подход основанным на регрессионном анализе с помощью теста на условный момент. Этот подход может быть обобщён и для вектора $r(\cdot)$, хотя для этого необходимы более громоздкие вычисления.

М-тест представляет собой тест на конечное число моментных условий. Таким образом, можно построить процесс, порождающий данные, для которого лежащее в основе условие для условного момента, например, такое, как в (8.29), неверно, хотя моментные условия выполняются. В этом случае тест на условный момент является несостоятельным, так как он не отвергается с вероятностью, равной единице, при $N \rightarrow \infty$. Биренс (1990) предложил способ задать $g(x)$ в (8.30) чтобы получить состоятельный тест на условный момент для тестов на функциональную форму в нелинейной регрессионной модели, где $r(y, x, \theta) = y - f(x, \theta)$. Однако состоятельность теста не означает высокую мощность против конкретных альтернатив.

\subsection{Пример М-тестов}

Чтобы проиллюстрировать различные М-тесты, мы рассмотрим пример регрессионной модели Пуассона, которая приведена в разделе 5.2, с функцией вероятности $f(y) = e^{-\mu}\mu^y/y!$ и $\mu = \exp(x'\beta)$.

Мы хотим протестировать
\[
H_0: \E[m(y, x, \beta)] = 0,
\]
для разных функций $m(\cdot)$. Этот тест будет проведён при предположении, что процесс, порождающий данные, --- это действительно распределение Пуассона.

\begin{center}
Вспомогательные регрессии
\end{center}

Так как оценивание производится с помощью метода максимального правдоподобия, мы можем использовать М-тестовую статистку $M^*$, рассчитанную как $NR_u^2$ из вспомогательной регрессии (8.5), где
\begin{equation}
1 = \hat{m}(y_i, x_i, \hat{\beta})'\delta + (y_i - \exp(x_i'\hat{\beta}))x_i'\gamma + u_i,
\end{equation}
так как $\hat{s} = |\partial{\ln f(y)}/\partial{\beta}|_{\hat{\beta}} = (y - \exp(x'\hat{\beta}))x$ и $\hat{\beta}$ --- это оценка, полученная методом максимального правдоподобия. При нулевой гипотезе статистика имеет $\chi^2(\dim(m))$ распределение.

Альтернативная $M^{**}$ статистика из вспомогательной регрессии
\begin{equation}
1 = \hat{m}(y,x,z,\hat{\beta})'\delta + u.
\end{equation}
Этот тест асимптотически эквивалентен $LM^*$, если $m(\cdot)$ такое, что $\E[\partial{m}/\partial{\beta}] = 0$, но в противном случае он не имеет хи-квадрат распределения.

\begin{center}
Тесты на моменты
\end{center}

Верная спецификация условной функции математического ожидания, $\E[y - exp(x'\beta)|x] = 0$, может быть протестирована с помощью М-теста на
\[
\E[(y - exp(x'\beta))z] = 0,
\]
где $z$ может быть функцией от $x$. Для Пуассона и других моделей экспоненциального семейства, $z$ не может равняться $x$, потому что 
условия первого порядка для $\hat{\beta}_{ML}$ накладывают ограничение, что $\sum_i (y_i - \exp(x_i'\hat{\beta}))x_i = 0$, что приводит к $M = 0$, если $z = x$. Вместо этого, $z$ может включать квадраты и произведения регрессоров.

Верная спецификация дисперсии также может быть проверена, так как распределение Пуассона предполагает равенство условной дисперсии и математического ожидания. Так как $\V[y|x] - \E[y|x] = 0$ и $\E[y|x] = \exp(x'\beta)$ мы можем провести М-тест на
\[
\E[\{(y - exp(x'\beta))^2 - \exp(x'\beta)\}x] = 0.
\]
Другая версия наоборот тестирует
\[
\E[\{(y - \exp(x'\beta))^2 - y\}x] = 0,
\]
так как $\E[y|x] = \exp(x'\beta)$. Тогда $m(\beta) = \{(y - \exp(x'\beta))^2 - y\}x$ обладает свойством $\E[\partial{m}/\partial{\beta}] = 0$. В этом случае (8.7) выполняется, и альтернативная регрессия (8.33) даёт асимптотически эквивалентный тест для регрессии (8.32).

Стандартная спецификация теста для параметрических моделей --- это информационно матричный тест. Для функции плотности Пуассона, $D$, которая определена в (8.19), становится равной $D(x,y,\beta) = \{(y - \exp(x'\beta))^2 - y\}xx'$, и мы тестируем 
\[
\E[\{(y - \exp(x'\beta))^2 - y\}\Vech[xx']] = 0.
\]
Очевидно, что для примера Пуассона информационно матричный тест является тестом на условия для первого и второго моментов, которые накладываются моделью Пуассона. Этот результат выполняется для более общего случая моделей из экспоненциального семейства. Тестовая статистика $M^{**}$ асимптотически эквивалентна $M^{*}$, так как здесь $\E[\partial{m}/\partial{\beta}]$.

Предположение Пуассона также может быть протестировано с помощью хи-квадрат теста на качество подгонки. Например, так как малое число частот превышает число три в последующем примере моделирования, создадим четыре группы, которые соответствуют $y = 0, 1, 2$ и $3$ и больше, где при выполнении теста группа с $y \geq 3$ отбрасывается, так как сумма вероятностей равна одному. Таким образом, для $j = 0, \dots, 2$ вычислим индикатор $d_{ij} = 1$, если $y_i = j$, и $d_{ij} = 1$ в противном случае. Потом вычислим спрогнозированную вероятность $\hat{p}_{ij} = e^{-\hat{\mu}_i}\hat{\mu}_i^j/j!$, где $\hat{\mu}_i = \exp(x_i'\hat{\beta})$. В этом случае мы тестируем
\[
\E[(d - p)] = 0,
\]
где $d_i = [d_{i0}, d_{i1}, d_{i2}]$ и $p_i = [p_{i0}, p_{i1}, p_{i2}]$ из вспомогательной регрессии (8.33), и $\hat{m}_i = d_i - \hat{p}_i$. 

\begin{center}
Результаты моделирования
\end{center}

Данные были сгенерированы по модели Пуассона с математическим ожиданием $\E[y|x] = \exp(\beta_1 + \beta_2x_2)$, где $x_2 \sim \mathcal{N}[0,1]$ и $(\beta_1, \beta_2) = (0,1)$. Оценивание методом максимального правдоподобия регрессии Пуассона $y$ на $x$ на выборке размером в 200 наблюдений даёт
\[
\hat{\E}[y|x] = \exp(-\underset{(0.089)}{0.165} + \underset{(0.069)}{1.124}x_2),
\]
где соответствующие стандартные ошибки представлены в скобках.

Результаты различных М-тестов представлены в таблице 8.1.

\begin{table}[h]
\begin{center}
\begin{scriptsize}
\caption{\label{tab:mtest} М-тесты на спецификацию для примера регрессии Пуассона}
\begin{minipage}{16cm}
\begin{tabular}[t]{l*{6}{{c}}}
\hline
\hline
\bf{Тип теста}\footnote{\begin{scriptsize} Процесс, порождающий данные для $y$ --- распределение Пуассона с параметром математического ожидания $\exp(0 + x_2)$ и размером выборки $N = 200$. М-тестовая статистика имеет хи-квадрат распределение с числом степеней свободы, которые представлены в столбце ст.св., и $p$-значения представлены в отдельном столбце. Альтернативная $M^{**}$ статистика имеет смысл только для тестов 3 и 4. \end{scriptsize}} & \bf{$H_0$, где $\mu = \exp(x' \beta)$} & \bf{$M^*$} & \bf{ст.св.} & \bf{$p$-значение} & \bf{$M^{**}$} \\
\hline
1. Верное математическое ожидание & $\E[(y - \mu)x_2^2] = 0$ & 3.27 & 1 & 0.07 & 0.44 \\
2. Дисперсия = математическое ожидание & $\E[\{(y - \mu)^2 - \mu\}|x] = 0$ & 2.43 & 2 & 0.30 & 1.89 \\
3. Дисперсия = математическое ожидание & $\E[\{(y - \mu)^2 - y\}|x] = 0$ & 2.43 & 2 & 0.30 & 2.41 \\
4. Информационная матрица & $\E[\{(y - \mu)^2 - y\}\Vech[xx']] = 0$ & 2.95 & 3 & 0.40 & 2.73 \\
5. Хи-квадрат критерия согласия & $\E[d - p] = 0$ & 2.50 & 3 & 0.48 & 0.75 \\
\hline
\hline
\end{tabular}
\end{minipage}
\end{scriptsize}
\end{center}
\end{table}

В качестве примера вычисления $M^*$, используя (8.32), рассмотрим информационно матричный тест. Так как $x = [1, x_2]'$ и $\Vech[xx'] = [1, x_2, x_2^2]'$, вспомогательная регрессия 1 на $\{(y - \hat{\mu})^2 - y\}$, $\{(y - \hat{\mu})^2 - y\}x_2$, $\{(y - \hat{\mu})^2 - y\}x_2^2$, $(y - \hat{\mu})$ и $(y - \hat{\mu})x_2$ даёт $R^2 = 0.01473$ при $N = 200$, что приводит к $M^* = 2.95$. Такое же значение $M^*$ можно получить и непосредственно из нецентрированной объясненной суммы квадратов, равной 2.95, так и косвенно как $N$ минус 197.05, остаточная сумма квадратов из этой регрессии. Тестовая статистика имеет $\chi^2(3)$ распределение с $p = 0.40$, поэтому нулевая гипотеза не отвергается на уровне значимости 0.05.

Для хи-квадрат теста на качество подгонки фактические частоты равны, соответственно, 0.435, 0.255 и 0.110,  соответствующие спрогнозированные вероятности равны 0.429, 0.241 и 0.124. Мы получаем $PCGF = 0.47$, используя (8.23), но эта статистика не имеет хи-квадрат распределения, так как она не учитывает ошибку при оценивании $\hat{\beta}$. Вспомогательная регрессия для правильной $CGF$ статистики из (8.27) приводит к $M^* = 2.50$, которая имеет хи-квадрат распределение.

В данном моделировании все пять моментных условий не отвергаются на уровне значимости 0.05, так как $p$-значение для $M^*$ превышает 0.05. Именно этот результат и ожидался, так как данные в этом примере  сгенерированы по заданному распределению, поэтому тесты на уровне значимости 0.05 должны отвергаться только в 5\% случаев. Альтернативная статистика $M^{**}$ имеет смысл только для тестов 3 и 4, так как только тогда $\E[\partial{m}/\partial{\beta}] = 0$, в противном случае она только даёт значение нижней границы для $M$.

\section{Тест Хаусмана}

Тесты, основанные на сравнении между двумя различными оценками, называются тестами Хаусмана, предложенными Хаусманом (1978), или тестами Ву-Хаусмана, или даже тестами Дарбина-Ву-Хаусмана, предложенными Ву (1973) и Дарбином (1954), который предложил аналогичные тесты.

\subsection{Тест Хаусмана}

Рассмотрим тест на эндогенность регрессора в одном уравнении. Две альтернативные оценки --- МНК- и ДМНК-оценки, где ДМНК-оценка использует инструменты для учёта возможной эндогенности регрессора. Если эндогенность присутствует, то МНК-оценка несостоятельна, так что две оценки будут иметь различные пределы по вероятности. Если нет эндогенности, то обе оценки состоятельны, поэтому они имеют и тот же предел по вероятности. Это говорит о том, что можно тестировать на эндогенность путём тестирования на разницу между МНК- и ДМНК-оценками, см. раздел 8.4.3 для подробностей.

В более общем случае рассмотрим две оценки $\hat{\theta}$ и $\tilde{\theta}$. Будем проводить тест на проверку приведённых ниже гипотез:
\begin{equation}
H_0: \plim(\hat{\theta} - \tilde{\theta}) = 0,
H_a: \plim(\hat{\theta} - \tilde{\theta}) \not= 0.
\end{equation}

Предположим, что разница между двумя $\sqrt{N}$ состоятельными оценками тоже $\sqrt{N}$-состоятельная при нулевой гипотезе с математическим ожиданием 0 и предельным нормальным распределением, и
\[
\sqrt{N}(\hat{\theta} - \tilde{\theta}) \stackrel{d}{\rightarrow} \mathcal{N}[0, V_H],
\]
где $V_H$ обозначает ковариационную матрицу для предельного распределения. Тогда статистика теста Хаусмана
\begin{equation}
H = (\hat{\theta} - \tilde{\theta})'(N^{-1}V_H)^{-1}(\hat{\theta} - \tilde{\theta})
\end{equation}
имеет $\chi^2(q)$ распределение при нулевой гипотезе. Мы отвергаем нулевую гипотезу на уровне значимости $\alpha$, если $H > \chi_{\alpha}^2(q)$.

В некоторых приложениях таких, как тесты на эндогенность, $\V[\hat{\theta} - \tilde{\theta}]$ имеет неполный ранг. Тогда используется обобщённая обратная матрица из (8.35) и хи-квадрат тест имеет число степеней
свободы равное рангу $\V[\hat{\theta} - \tilde{\theta}]$.

Тест Хаусмана может быть применён только для подмножества параметров. Так, например, интерес может заключаться исключительно в коэффициенте при, возможно, эндогенном регрессоре и в том, меняется ли он при переходе от МНК к ДМНК. Тогда лишь одна из компонент $\theta$ используется, и тестовая статистика имеет $\chi^2(1)$ распределение. Как и в других условиях, этот тест на подмножество параметров может привести к выводу, который отличается от теста на все параметры.

\subsection{Расчёт теста Хаусмана}

Посчитать тест Хаусмана в принципе несложно, но на практике это тяжело, так как необходимо получить состоятельную оценку $V_H$, предельной ковариационную матрицу $\sqrt{N}(\hat{\theta} - \tilde{\theta})$. В общем случае
\begin{equation}
N^{-1}V_H = \V[\hat{\theta} - \tilde{\theta}] = \V[\hat{\theta}] + \V[\tilde{\theta}] - 2\Cov[\hat{\theta},\tilde{\theta}].
\end{equation}
Первые две величины уже рассчитаны для обычного случая, но третье слагаемое --- нет.

\begin{center}
Расчёт для полностью эффективной оценки при нулевой гипотезе
\end{center}

Хотя в общем нулевая и альтернативная гипотезы для теста Хаусмана определены в (8.34), в приложениях, как правило, под нулевой и альтернативной гипотезами подразумеваются конкретные модели. Например, в сравнении МНК- и ДМНК-оценок нулевая гипотеза подразумевает, что все  регрессоры экзогенны, в то время как в альтернативной гипотезе допускаются эндогенные регрессоры.

Если $\hat{\theta}$ --- эффективная оценка для модели при нулевой гипотезе, то $\Cov[\hat{\theta},\tilde{\theta}] = \V[\hat{\theta}]$. Для доказательства смотрите упражнение 8.3. Это означает, что $\V[\hat{\theta} - \tilde{\theta}] = \V[\tilde{\theta}] - \V[\hat{\theta}]$, и
\begin{equation}
H = (\hat{\theta} - \tilde{\theta})'(\V[\tilde{\theta}] - \V[\hat{\theta}])^{-1}(\hat{\theta} - \tilde{\theta}).
\end{equation}

Эта статистика имеет значительное преимущество, так как ей необходимы только оценки асимптотических ковариационных матриц оценок параметров $\hat{\theta}$ и $\tilde{\theta}$. Полезно использовать программу, которая позволяет сохранить оценки параметров и оценки ковариационных матриц, а также в которой можно выполнять матричные вычисления.

Например, упрощение возможно для тестов на эндогенность в линейной регрессионной модели, если ошибки предполагаются гомоскедастичными. Тогда $\hat{\theta}$ --- МНК-оценка, которая полностью эффективна при нулевой гипотезе об отсутствии эндогенности, а $\tilde{\theta}$ --- ДМНК-оценка. Однако необходимо проявлять осторожность, чтобы состоятельные оценки ковариационных матриц оказались такими, что $\V[\tilde{\theta}] - \V[\hat{\theta}]$ была бы положительно определена (см. Рууд, 1984). При сравнении МНК и ДМНК-оценок в оценках ковариационных матриц $\V[\hat{\theta}]$ и $\V[\tilde{\theta}]$ должна использоваться одна и та же оценка дисперсии ошибки $\sigma^2$.

Версию (8.37) теста Хаусмана особенно легко рассчитать вручную, если $\theta$ является скаляром, или если проверяется только одна компонента вектора параметров. Тогда
\[
H = (\hat{\theta} - \tilde{\theta})^2/(\tilde{s}^2 - \hat{s}^2)
\]
имеет $\chi^2(1)$ распределение, а $\hat{s}$ и $\tilde{s}$ --- это стандартные ошибки $\hat{\theta}$ и $\tilde{\theta}$.

\begin{center}
Вспомогательные регрессии
\end{center}

В некоторых основных случаях тест Хаусмана может быть легко рассчитан как стандартный тест на значимость подмножества регрессоров в расширенной регрессии МНК, полученной при предположении о том, что $\hat{\theta}$ полностью эффективная оценка. Примеры приведены в разделе 8.4.3 и в разделе 21.4.3.

\begin{center}
Тесты Хаусмана с поправкой
\end{center}

Более простая версия (8.37) теста Хаусмана и стандартные вспомогательные регрессии требуют сильного предположения  о распределении --- эффективности $\hat{\theta}$. Это противоречит подходу робастного оценивания  при относительно слабых предположениях о распределении.

Прямое оценивание $Cov[\hat{\theta},\tilde{\theta}]$ и, следовательно, $V_H$, в принципе, возможно. Предположим, что $\hat{\theta}$ и $\tilde{\theta}$ --- М-оценки, которые получаются из решения $\sum_i h_{1i}(\hat{\theta}) = 0$ и $\sum_i h_{2i}(\tilde{\theta}) = 0$. Зададим $\hat{\delta}' = [\hat{\theta},\tilde{\theta}]$. Тогда $\V[\hat{\delta}] = G_0^{-1}S_0( G_0^{-1})'$, где $G_0$ и $S_0$ определены в разделе 6.6, c упрощением, что $G_{12} = 0$. В таком случае $\V[\hat{\theta} - \tilde{\theta}] = R\V[\hat{\delta}]R'$, где $R = [I_q, - I_q]$. На практике может потребоваться дополнительный программный код, который может быть особенным для каждого конкретного случая.

Более простой подход --- метод бутстрэп (см. раздел 11.6.3), хотя необходима осторожность в некоторых приложениях, для того посчитать правильное число степеней свободы хи-квадрат теста.

Другой возможный подход для неэффективной $\hat{\theta}$ --- использование вспомогательной регрессии, которая подходит в случае эффективных оценок. Но в данном случае надо провести тест на подмножество регрессоров, используя скорректированные стандартные ошибки. Этот тест с поправкой прост в реализации и будет иметь мощность при тестировании на неверную спецификацию, хотя он необязательно будет эквивалентным тесту Хаусмана, который использует более общий вид $H$, который приведён в (8.35). Пример приведён в разделе 21.4.3.

Наконец, могут быть рассчитаны границы, которые не требуют вычисления $\Cov[\hat{\theta},\tilde{\theta}]$. Для скалярных случайных величин $\Cov[x,y] \leq s_xs_y$. Для скалярного случая это приводит к верхней границе для $H$ $N(\hat{\theta} - \tilde{\theta})^2/(\tilde{s}^2 + \hat{s}^2 - 2\tilde{s}\hat{s})$, где $\hat{s}^2 = \hat{\V}[\hat{\theta}]$ и $\hat{s}^2 = \hat{\V}[\tilde{\theta}]$. Нижняя граница для $H$ $N(\hat{\theta} - \tilde{\theta})^2/(\tilde{s}^2 + \hat{s}^2)$ при предположении, что $\hat{\theta}$ и $\tilde{\theta}$ положительно коррелированы. На практике, однако, эти границы достаточно широкие.

\subsection{Мощность теста Вальда}

Тест Хаусмана --- это довольно общая процедура, которая не задаёт альтернативную гипотезу в явном виде, поэтому она необязательно имеет большую мощность против конкретных альтернатив. 

Рассмотрим, например, тесты на исключающие ограничения в полностью параметрических моделях. Нулевая гипотеза $H_0: \theta_2 = 0$, где $\theta = (\theta_1', \theta_2')'$. 


Очевидная спецификация --- тест Хаусмана на разницу $\hat{\theta}_1 - \tilde{\theta}_1$, где $(\hat{\theta}_1, \hat{\theta}_2)$ --- оценка, полученная с помощью метода максимального правдоподобия для неограниченной модели, и $(\tilde{\theta}_1, 0)$ --- оценка, полученная с помощью метода максимального правдоподобия для ограниченной модели.

Холли (1982) показал, что этот тест Хаусмана совпадает с классическим тестом ($Wald$, $LR$ или $LM$) c $H_0: \mathcal{I}_{11}^{-1}\mathcal{I}_{12}\theta_2 = 0$, где $\mathcal{I}_{ij} = \E[\partial{\mathcal{L}(\theta_1, \theta_2)}^2/\partial{\theta_i}\partial{\theta_i}]$, а не $H_0: \theta_2 = 0$. Два теста совпадают, если $\mathcal{I}_{12}$ имеет полный ранг и $\dim(\theta_1) \geq \dim(\theta_2)$, так как тогда $\mathcal{I}_{11}^{-1}\mathcal{I}_{12}\theta_2 = 0$. В противном случае они могут отличаться. Очевидно, что тест Хаусмана не будет мощным против $H_0$, если информационная матрица имеет блочно-диагональной вид, так как тогда $\mathcal{I}_{12} = 0$. Холли (1987) обобщил анализ на нелинейные гипотезы.

\section{Стандартные тесты на неверную спецификацию}

В этом разделе мы рассмотрим стандартные тесты на некоторые виды неверной спецификации модели. Внимание акцентируется на тестовой статистике, которую можно вычислить с помощью вспомогательной регрессии, используя минимальные предположения, позволяющие сделать вывод, основываясь на стандартных ошибках с поправкой на гетероскедастичность.

\subsection{Тест на пропущенные переменные}

Пропущенные переменные обычно приводят к несостоятельным оценкам параметров за исключением особых случаев таких, как ортогональность пропущенный регрессор к остальным в линейной модели. Поэтому всегда важно проверить модель на возможность пропуска переменных.

Наиболее часто используется тест Вальда, так как обычно оценить модель с  включенными потенциально пропущенными переменными, так же легко, как и с исключенными. Кроме того, этот тест может использовать скорректированные  стандартные ошибки в сэндвич-форме, хотя это имеет смысл, только если оценка остаётся состоятельной в подобной ситуациях.

В методе максимального правдоподобия в качестве альтернативы можно оценить модели с и без потенциально нерелевантных регрессоров и провести тест отношения правдоподобия.

Версию теста множителей Лагранжа с поправкой можно легко вычислить в некоторых условиях. Рассмотрим, например, тест $H_0: \beta_2 = 0$ в модели Пуассона с математическим ожиданием $\exp(x'\beta_1 + x'\beta_2)$. Статистика теста множителей Лагранжа основана на скор статистике $\sum_i x_i\tilde{u}_i$, где $\tilde{u}_i = y_i - \exp(x_{1i}'\tilde{\beta}_1)$ (см. раздел 7.3.2). Теперь оценка с поправкой на гетероскедастичность дисперсии $N^{-1/2}\sum_i x_iu_i$, где $u_i = y_i - \E[y_i|x_i]$, равна $N^{-1}\sum_i u_i^2x_ix_i'$, и можно показать, что
\[
LM^+ = \left[ \sum_{i=1}^n x_i\tilde{u}_i \right]' \left[\sum_{i=1}^n \tilde{u}_i^2x_ix_i'\right]^{-1} \left[ \sum_{i=1}^n x_i\tilde{u}_i\right]
\]
является статистикой  теста множителей Лагранжа с поправкой, которая не требует ограничения Пуассона, что $\V[u_i|x_i] = \exp(x_{1i}'\beta_1)$ при нулевой гипотезе. Статистику можно вычислить как $NR_u^2$ из регрессии 1 на $x_{1i}\tilde{u}_i$ и $x_{2i}\tilde{u}_i$. Такие тесты множителей Лагранжа с поправкой возможны в более общем случае для моделей из  экспоненциального семейства, так как скор статистика в таких моделях --- средневзвешенная сумма остатков $\tilde{u}_i$ (см. Вулдридж, 1991). Этот класс включает МНК, также возможны модификации, когда оценивание проводится с помощью ДМНК или НМНК, см. Вулдридж (2002).

\subsection{Тесты на гетероскедастичность}

Оценки параметров в линейной или нелинейной регрессионных моделях условного математического ожидания, которые оценивается с помощью метода наименьших квадратов или инструментальных переменных, сохраняют свою состоятельность при наличии гетероскедастичности. Единственное, что необходимо, --- скорректировать стандартные ошибки этих оценок. Это не требует моделирования гетероскедастичности, а стандартные ошибки с поправкой на гетероскедастичность можно вычислить при минимальных предположениях о распределении, используя результат Уайта (1980). Таким образом, нет необходимости проводить тест на гетероскедастичность кроме случая, когда эффективность оценки вызывает большое беспокойство. Тем не менее, мы обобщим некоторые результаты тестов на гетероскедастичность.

Начнём с оценивания методом наименьших квадратов модели линейной регрессии $y = x'\beta + u$. Предположим, что гетероскедастичность моделируется как $\V[u|x] = g(\alpha_1 + z'\alpha_2)$, где $z$, как правило, --- подмножество $x$ и $g(\cdot)$ --- экспоненциальная функция. В литературе основное внимание уделяется тестам на $H_0: \alpha_2 = 0$ методом множителей Лагранжа, потому что в отличие от теста Вальда и теста отношения правдоподобия он требуют только получения МНК-оценки $\beta$. Стандартный тест множителей Лагранжа Брейша и Пагана (1979) во многом основывается на предположении о нормальном распределении ошибок, так как он предполагает, что $\E[u^4|x^4] = 3\sigma^4$ при нулевой гипотезе. Коенкер (1981) предложил более надёжную версию теста множителей Лагранжа, $NR^2$ из регрессии $\hat{u}_i^2$ на 1 и $z_i$, где $\hat{u}_i$ --- МНК-остатки. Этот тест требует слабое предположение о том, что $\E[u^4|x]$ является константой. Как и тест Бройша-Пагана, он инвариантен к выбору функции $g(\cdot)$. Тест Уайта (1980a) на гетероскедастичность эквивалентен этому тесту множителей Лагранжа с $z = \Vech[xx']$. Тест можно обобщить, чтобы $\E[u^4|x]$ изменялось по $x$, хотя постоянство может быть разумным предположением для теста, так как нулевая гипотеза уже указывает, что $\E[u^2|x]$ является константой.

Качественно подобные результаты переносятся и на нелинейные модели условного математического ожидания, предполагающие определённую форму гетероскедастичности, которую можно проверить на неверную спецификацию. Например, в модели регрессии Пуассона $\V[y|x] = \exp(x'\beta)$. В более общем случае для моделей из линейного экспоненциального семейства, метод квази-максимального правдоподобия является состоятельным, несмотря на неверную спецификацию гетероскедастичности, и для него верны качественные результаты  аналогичные представленным здесь. При использовании скорректированных стандартных ошибок, представленных в разделе 5.7.4 будут сделаны верные статистические выводы, даже если вид гетероскедастичности неверно специфицирован. Если исследователь всё же хочет проверить гетероскедастичность на верную спецификацию, то можно провести тесты множителей Лагранжа с поправкой (см. Вулдридж, 1991).

Гетероскедастичность может привести к более серьёзным последствиям несостоятельности оценок параметров в некоторых нелинейных моделях. Ярким примером является модель тобит (см. главу 16), линейная регрессионная модель с нормальными гомоскедастичными ошибками, которая становится нелинейной в результате цензурирования или усечения. В таком случае проверка на гетероскедастичность становится более важной. Задав модель для $\V[u|x]$, можно выполнить тест Вальда, тест отношения правдоподобия и тест множителей Лагранжа, или можно использовать М-тесты на гетероскедастичность (см. Паган и Велла, 1989).

\subsubsection{Тесты Хаусмана на эндогенность}

Оценки инструментальных переменных следует использовать только там, где есть необходимость в них, так как оценки, полученные с помощью метода наименьших квадратов, являются более эффективными, если все регрессоры экзогенные и, из раздела 4.9, эта потеря эффективности может быть существенной. Может быть полезно проверить, нужен ли метод инструментальных переменных или нет. Тест на эндогенность регрессоров сравнивает оценки инструментальных переменных с оценками МНК. Если регрессоры являются эндогенными, то в пределе эти оценки будут отличаться. Если регрессоры экзогенны, то две оценки не будут отличаться. Таким образом, большие различия между МНК-оценками и оценками инструментальных переменных можно интерпретировать как свидетельство эндогенности.

Этот пример иллюстрирует первоначальную мотивацию для теста Хаусмана. Рассмотрим модель линейной регрессии
\begin{equation}
y = x_1'\beta_1 + x_2'\beta_2 + u,
\end{equation}
где $x_1$ потенциально эндогенный и $x_2$ экзогенный. Пусть $\hat{\beta}$ --- МНК-оценка, а $\tilde{\theta}$ --- ДМНК-оценка в (8.38). Предположим гомоскедастичность ошибок. МНК-оценка является эффективной при нулевой гипотезе об отсутствии эндогенности, тест Хаусмана на эндогенность $x_1$ можно вычислить с помощью тестовой статистики $H$, которая определена в (8.37). Поскольку матрица $\V[\hat{\beta}] - \V[\tilde{\beta}]$ может быть неполного ранга, необходима обобщённая обратная матрица, и число степеней свободы равно $\dim(\beta_1)$, а не $\dim(\beta)$.

Хаусман (1978) показал, что тест можно проще провести с помощь проверки $\gamma = 0$ в расширенной регрессии МНК
\[
y = x_1'\beta_1 + x_2'\beta_2 + \hat{x}_1'\gamma + u,
\]
где $\hat{x}_1$ --- предсказанное значение эндогенного регрессора $x_1$ из сокращённой формы многомерной регрессии $x_1$ на инструменты $z$. Эквивалентно мы можем проверить $\gamma = 0$ в расширенной регрессии МНК
\[
y = x_1'\beta_1 + x_2'\beta_2 + \hat{v}_1'\gamma + u,
\]
где $\hat{v}_1$ --- остатки из приведённой формы многомерной регрессии $x_1$ на инструменты $z$. Интуиция этих тестов состоит в том, что если $u$ из (8.38) не коррелирует с $x_1$ и $x_2$, то $\gamma = 0$. Если вместо этого $u$ коррелирует с $x_1$, то это будет подхвачено значимостью дополнительных преобразованных $x_1$ таких, как $\hat{x}_1$ и $\hat{v}_1$.

Для пространственных данных принято предполагать гетероскедастичность ошибок. Тогда МНК-оценка $\hat{\beta}$ является неэффективной в (8.38), и нельзя использовать более простую версию (8.37) теста Хаусмана. Однако можно использовать представленные выше расширенные регрессии МНК при условии, что $\gamma = 0$ проверяется с использованием оценки ковариационной матрицы, устойчивой к гетероскедастичности. Это эквивалентно тесту Хаусмана, в книге Дэвидсона и МакКиннона (1993, с. 239) показано, что $\hat{\gamma}_{OLS}$ в этих расширенных регрессиях равна $A_N(\hat{\beta} - \tilde{\beta})$, где $A_N$ --- матрица полного ранга с конечным пределом по вероятности.

Дополнительные тесты Хаусмана на эндогенность возможны. Пусть $y =  x_1'\beta_1 + x_2'\beta_2 + x_3'\beta_3 + u$, где $x_1$ потенциально эндогенный, $x_2$ считается эндогенным, а $x_3$ считается экзогенным. Тогда эндогенность $x_1$ может быть проверена с помощью сравнения ДМНК-оценки с инструментами только для $x_2$ и ДМНК-оценки с инструментами для $x_1$ и $x_2$. Тест Хаусмана также может быть обобщён на нелинейные регрессионные модели с МНК, заменённым на НМНК, и ДМНК, заменённым на ДНМНК. Дэвидсон и МакКиннон (1993) представили расширенные регрессии, которые могут быть использованы для вычисления релевантного теста Хаусмана, предполагая гомоскедастичность ошибок. Мроз (1987) приводит хорошие примеры тестов на эндогенность, включая примеры вычисления $\V[\hat{\theta} - \tilde{\theta}]$, когда $\hat{\theta}$ не является эффективной.

\subsubsection{OIR тесты на экзогенность}

Если используются оценки инструментальных переменных, то инструменты должны быть экзогенными, для того чтобы оценки были состоятельными. Для точно идентифицированных моделей невозможно провести тест на экзогенность инструментов. Вместо этого для подтверждения действенности инструмента должны использоваться априорные показатели. Некоторые примеры приведены в разделе 4.8.2. Однако для сверхидентифицированных моделей можно провести тест на экзогенность инструментов.

Начнём с линейной регрессии. Тогда $y = x'\beta + u$ и инструменты $z$ действенны, если $\E[u|z] = 0$ или если $\E[zu] = 0$. Очевидный тест на $H_0: \E[zu] = 0$ основан на том, насколько величина $N^{-1}\sum_i z_i\hat{u}_i$ далека от нуля. В точно идентифицированном случае оценка инструментальных переменных является решением $N^{-1}\sum_i z_i\hat{u}_i = 0$, поэтому этот тест не является полезным. 

В случае сверх-идентифицированной модели, тест на сверх-идентифицирующие  ограничения, представленный в разделе 6.3.8, имеет вид
\begin{equation}
OIR = \hat{u}Z\hat{S}^{-1}Z'\hat{u},
\end{equation}
где $\hat{u} = y - X\hat{\beta}$ --- это оптимальная оценка, полученная с помощью обобщённого метода моментов, и $S$ --- состоятельная оценка для $\plim N^{-1}\sum_i u_i^2z_iz_i'$. OIR тест Хансена (1982) является расширением теста, предложенного Сарганом (1958) для линейного метода инструментальных переменных, и тестовую статистику (8.39) часто называют тестом Саргана. Если значение $OIR$ велико, то моментные условия отвергаются и оценка метода инструментальных переменных несостоятельна. Отвержение нулевой гипотезы обычно интерпретируется как доказательство того, что инструменты $z$ являются эндогенными, но это также может быть свидетельством неверной спецификации модели, т.е. что на самом деле $y \not= x'\beta + u$. В любом случае тот факт, что гипотеза отвергается, указывает на проблемы с оценкой.

Как формально показано в разделе 6.3.9, $OIR$ имеет $\chi^2(r - K)$ распределение при нулевой гипотезе, где $(r - K)$ --- количество сверх-идентифицирующих ограничений. Чтобы получить некоторую интуицию для этого результата, полезно использовать гомоскедастичные ошибки. Тогда $\hat{S} = \hat{\sigma}^2Z'Z$, где $\hat{\sigma}^2 = \hat{u}'\hat{u}/(N - K)$, поэтому
\[
OIR = \frac{\hat{u}'P_z\hat{u}}{\hat{u}'\hat{u}/(N - K)},
\]
где $P_Z = Z(Z'Z)^{-1}Z'$. Таким образом, $OIR$ является отношением квадратичных форм $\hat{u}$. При нулевой гипотезе числитель имеет предел по вероятности $\sigma^2(r - K)$, а знаменатель имеет $\plim\hat{\sigma}^2 = \sigma^2$, так что дробь примерно равна $r - K$, т.е. математическому ожиданию $\chi^2(r - K)$ случайной величины.

Тестовая статистика (8.39) обобщается сразу на нелинейную регрессию, путем определения $u_i = y - g(x, \beta)$ или $u_i = r(y,x,\beta)$, как и в разделе 6.5, а также на линейные системы и на панельные оценки с помощью соответствующего определения $u$ (см. разделы 6.9 и 6.10).

Для линейного метода инструментальных переменных с гомоскедастичными ошибками были предложены тесты, альтернативные $OIR$ из (8.39). Мандалинос (1988) сравнивает такие тесты. Можно также использовать вариации $OIR$ тестов на подмножество сверх-идентифицирующих ограничений.

\subsubsection{RESET-тест}

Типичная ошибка  спецификации функциональной формы может состоять в игнорировании нелинейности зависимости от некоторых регрессоров. Рассмотрим регрессию $y = x'\beta + u$, где предполагается, что регрессоры линейны и асимптотически не коррелируют с ошибкой $u$. Прямой способ проверить нелинейность --- это добавить в список регрессоров степени экзогенных переменных, обычно квадраты,  и проверить статистическую значимость этих дополнительных переменных с помощью теста Вальда или $F$-теста. У исследователя должны быть конкретные основания предполагать нелинейность, и ясно, что метод не будет работать для категориальных переменных $x$.

Рамсей (1969) предложил тест на пропущенные переменные в регрессии, которые могут быть сформулированы в виде теста на функциональную форму. 

Предложение Рамсея --- найти оценённые значения из начальной регрессии и сгенерировать новые регрессоры, которые являются функциями от оценённых значений $\hat{y} = x'\hat{\beta}$, например, $w = [(x'\hat{\beta})^2, (x'\hat{\beta})^3, \dots, (x'\hat{\beta})^p]$. Затем необходимо оценить модель $y = x'\beta + w'\gamma + u $ и провести тест на нелинейность --- тест Вальда на $p$ ограничений, $H_0: \gamma = 0$ против $H_a: \gamma \not= 0$. Обычно используются небольшие значения $p$ такие, как 2 или 3. Для этого теста можно сделать поправку на  гетероскедастичность.

\section{Выбор между невложенными моделями}

Две модели являются вложенными, если одна является частным случаем другой. Они являются невложенными, если ни одна из них не может быть представлена как частный случай другой. Выбор между вложенными моделями можно делать с помощью стандартного теста на параметрические ограничения, которые сводят одну модель к другой. Однако в случае невложенных моделей должны быть разработаны альтернативные методы.

Особое внимание уделяется выбору между невложенными моделями в рамках метода максимального правдоподобия, где теория является хорошо разработанной. Краткое описание остальных случаев приведено в разделе 8.5.4. Байесовские методы для выбора между моделями приведены в разделе 13.8.

\begin{center}
Информационные критерии
\end{center}

Информационные критерии строятся на логарифме функции правдоподобия с учетом числа степеней свободы. Модель с наименьшим информационным критерий является предпочтительной.

Интуиция состоит в том, что есть выбор между качеством подгонки модели, измеряемым с помощью логарифма функции правдоподобия, и принципом бережливости, который выступает в пользу простой модели. Подгонка модели может быть улучшена за счёт увеличения сложности модели. Тем не менее, параметры добавляются, только если улучшение подгонки компенсирует потерю бережливости. Отметим, что с этой точки зрения необязательно, чтобы множество рассматриваемых моделей включало <<истинный>> процесс, порождающий данные. Различные информационные критерии отличаются тем, насколько сильно они штрафуют модель за сложность.
Акаике (1973) предложил информационный критерий Акаике (Akaire Information Criterion)
\begin{equation}
AIC = - 2\ln L + 2q,
\end{equation}
где $q$ --- число параметров. Выбирается модель с наименьшим $AIC$. Термин информационный критерий используется, потому что лежащая в основе теория, представленная проще всего в книге Амэмия (1980), выбирает между моделями, используя информационный критерий Кульбака-Лейблера $(KLIC)$.

Было предложено значительное количество модификаций $AIC$, которые имеют вид $ - 2\ln L + g(q, N)$ для указанной  функции-штрафа $g(\cdot)$, которая превышает $2q$. Самый популярный вариант --- Байесовский информационный критерий (Bayesian Information Criterion)
\begin{equation}
BIC = - 2\ln L + (\ln L)q,
\end{equation}
который был предложен Шварцем (1978). Шварц предполагал, что $y$ имеет плотность из экспоненциального семейства с параметром $\theta$, $j$-тая модель имеет параметр $\theta_j$ с $\dim(\theta_j) = q_j$, и априорные вероятности для всех моделей равны. Он показал, что при этих предположениях максимизация апостериорной вероятности (см. главу 13) является асимптотически эквивалентной выбору модели, для которой величина $- 2\ln L + (\ln L)q_j/2$ максимальна. Так как это эквивалентно минимизации (8.41), процедура Шварца была названа Байесовским информационным критерием. Версия $AIC$, основанная на минимизации $KLIC$, похожая на $BIC$, называется состоятельным (consistent) $AIC$, $CAIC = - 2\ln L + (1 + \ln N)q$. Некоторые авторы определяют такие критерии, как $AIC$ и $BIC$, путём дополнительного деления на $N$ правой части равенства (8.40) и (8.41).

Если важна простота модели, то чаще используется $BIC$, так как штраф за размер модели у $AIC$ является относительно маленьким. Рассмотрим две вложенные модели с $q_1$ и $q_2$ параметрами соответственно, где $q_2 = q_1 + h$. Тогда можно провести тест отношения правдоподобия, по результатам которого расширенная модель лучше на уровне значимости 5\%, если $2\ln L$ увеличивается на $\chi_{0.05}^2(h)$. По $AIC$ можно сделать вывод, что расширенная модель лучше, если $2\ln L$ возрастает больше, чем на $2h$. Штраф за размер модели у $AIC$ меньше, чем у теста отношения правдоподобия при $h < 7$. В частности, для $h = 1$, то есть при одном ограничении, на уровне значимости 5\% для теста отношения правдоподобия критическое значение равно 3.84, тогда как для $AIC$ оно намного меньше и равно 2. По $BIC$ можно сказать, что расширенная модель лучше, если $2\ln L$ увеличивается на $h\ln L$. Этот штраф гораздо больше, чем $AIC$ или тест отношения правдоподобия с уровнем значимости 0.05 (кроме случая, когда $N$ очень мало).

Байесовский информационный критерий увеличивает штраф с увеличением размера выборки, в то время как традиционные тесты гипотезы на уровне значимости, например, 5\% не имеют такой особенности. Для вложенных моделей с $q_2 = q_1 + 1$ выбор расширенной модели на основе более низкого значения $BIC$ эквивалентен использованию критического значения двустороннего $t$-теста $\sqrt{\ln N}$, которое равно 2.15, 3.03 и 3.72 для $N = 102$, 104 и 106 соответственно. Для сравнения тесты на традиционные гипотезы с размером 0.05 используют неизменное критическое значение 1.96. В более общем случае для тестовой статистики, имеющей $\chi^2(h)$ распределение, $BIC$ предлагает использовать критическое значение $h\ln L$, а не обычное $\chi^2(h)$.

Учитывая их простоту, критерии, которые накладывают штраф и которые основаны на методе максимального правдоподобия, часто используются для выбора <<лучшей модели>>. Тем не менее, нет чёткого ответа на вопрос, какому критерию, если таковой имеется, следует отдать предпочтение. Используется значительная аппроксимация при выводе формул для $AIC$ и связанных с ним показателей. Функции потерь, отличные от минимизации $KLIC$, или максимизация апостериорной вероятности в случае $BIC$, могут быть гораздо более уместными. С теоретической точки зрения, выбор модели из множества моделей должен зависеть от предполагаемого применения этой модели. Например, целью модели может быть краткое описание основных особенностей реальности или прогнозирование некоторых результатов, или проверка некоторой важной гипотезы. В прикладных работах редко можно увидеть ясное изложение предполагаемого использования эконометрической модели.

\subsection{Тест отношения правдоподобия Кокса на невложенные модели}

Рассмотрим выбор между параметрическими моделями. Пусть модель $F_{\theta}$ имеет функцию плотности $f(y|x, \theta)$, а модель $G_{\gamma}$ --- $g(y|x, \gamma)$.

Тест отношения правдоподобия для модели $F_{\theta}$ против модели $G_{\gamma}$ основан на
\begin{equation}
LR(\hat{\theta}, \hat{\gamma}) = \mathcal{L}_f(\hat{\theta}) - \mathcal{L}_g(\hat{\gamma}) = \sum_{i=1}^N \ln  \frac{f(y_i|x_i, \hat{\theta})}{g(y_i|x_i, \hat{\gamma})}.
\end{equation}
Если $G_{\gamma}$ вложена в $F_{\theta}$, то, согласно разделу 7.3.1,  $2LR(\hat{\theta}, \hat{\gamma})$ имеет хи-квадрат распределение при нулевой гипотезе, что $F_{\theta} = G_{\gamma}$. Тем не менее, этот результат не выполняется, если модели являются невложенными. Кокс (1961, 1962б) предложил решение этой задачи для случая, когда $F_{\theta}$ является истинной моделью, но модели не являются вложенными, с помощью применения центральной предельной теоремы.

Расчёты для такого подхода нелегко осуществить, если нельзя получить аналитически \\ $\E_f[\ln (f(y|x, \theta)/g(y|x, \gamma))]$, где $\E_f$ обозначает математическое ожидание. Кроме того, если получить подобную тестовую статистику с $F_{\theta}$ и  $G_{\gamma}$, помененными ролями, то можно получить, что модель $F_{\theta}$ отвергается в пользу $G_{\gamma}$ и что модель $G_{\gamma}$ отвергается в пользу $F_{\theta}$. По этой причине тест необязательно является тестом на выбор модели, поскольку он необязательно выбирает одну или другую, вместо этого он может выбрать одну модель, обе или ни одной.

В некоторых случаях статистика Кокса может быть получена аналитически. Про невложенные линейные регрессионные модели $y = x'\beta + u$ и и $y = z'\gamma + v$ с гомоскедастичными нормально распределёнными ошибками можно посмотреть книгу Песарана (1974). Про преобразования невложенных моделей $h(y) = x'\beta + u$ и $g(y) = z'\gamma + v$, где $h(y)$ и $g(y)$ --- известные преобразования, написано у Песарана и Песарана (1995), которые использует симуляционный подход. Это позволяет, например, выбрать между линейной и лог-линейной параметрическими моделями с тождественным преобразованием $h(\cdot)$  и логарифмическим преобразованием $g(\cdot)$. Песаран и Песаран (1995) применяют идею о выборе между логит и пробит-моделями, представленными в главе 14.

\subsection{Тест отношения правдоподобия Вуонга на невложенные модели}

Вуонг (1989) привёл общую теорию распределения для статистики теста отношения правдоподобия, которая охватывает как вложенные, так и невложенные модели, а также позволяет процессу, порождающему данные, иметь неизвестную плотность, которая отличается от $f(\cdot)$ и $g(\cdot)$.

Асимптотические результаты Вуонга, представленные здесь, чтобы облегчить понимание тестов описанных в статье Вуонга, являются относительно сложными, так как в некоторых случаях тестовая статистика --- взвешенная сумма хи-квадратов с весами, которые может быть сложно вычислить. Вуонг предложил тест
\begin{equation}
H_0: \E_0 \left[ \ln \frac{f(y|x, \theta)}{g(y|x, \gamma)} \right] = 0,
\end{equation}
где $\E_0$ обозначает математическое ожидание по отношению к истинному процессу, порождающему данные, $h(y|x)$, который может быть неизвестным. Это эквивалентно тестированию $\E_h[\ln (h/g)] - \E_h[\ln (h/f)] = 0$ или тестированию на то, имеют ли две плотности $f$ и $g$ одинаковый информационный критерий Кульбака-Лейблера (см. раздел 5.7.2). Возможны односторонние альтернативные варианты с $H_f: \E_0[\ln (f/g)] > 0$ и $H_g: \E_0[\ln (f/g)] < 0$.

Очевидным тестом для проверки нулевой гипотезы является М-тест на то, отличается ли от нуля выборочный аналог $LR(\hat{\theta}, \hat{\gamma})$, определённый в (8.42). Здесь необходимо получить распределение тестовой статистики с возможно неизвестным процессом, порождающим данные. Это возможно, поскольку из раздела 5.7.1 оценка, полученная с помощью метода квази-максимального правдоподобия, $\hat{\theta}$ сходится к псевдо-истинному значению $\theta^*$, и $\sqrt{N}(\hat{\theta} - \theta^*)$ имеет предельное нормальное распределение. Аналогичный результат верен для оценки $\hat{\gamma}$, полученной с помощью метода квази-максимального правдоподобия.

\begin{center}
Общий результат
\end{center}

Полученное распределение $LR(\hat{\theta}, \hat{\gamma})$ варьируется в зависимости от того, действительно ли две модели, возможно, обе некорректные, эквивалентны в том смысле, что $f(y|x, \theta_*) = g(y|x, \gamma_*)$, где $\theta_*$ и $\gamma_*$ являются псевдо-истинными значениями $\theta$ и $\gamma$.

Если $f(y|x, \theta_*) = g(y|x, \gamma_*)$, то
\begin{equation}
2LR(\hat{\theta}, \hat{\gamma}) \stackrel{d}{\rightarrow} M_{p + q}(\lambda_*),
\end{equation}
где $p$ и $q$ --- размеры $\theta$ и $\gamma$, а $M_{p + q}(\lambda_*)$ обозначает функцию распределения взвешенной суммы хи-квадрат переменных $\sum_{j=1}^{p+q} \lambda_{*j}Z_j^2$. Величины $Z_j^2$ независимы и одинаково распределены по $\chi^2(1)$ и $\lambda_{*j}$ являются собственными значениями матрицы размера $(p+q)(p+q)$
\begin{equation}
W = \begin{bmatrix} -B_f(\theta_*)A_f(\theta_*)^{-1} & -B_{fg}(\theta_*, \gamma_*)A_g(\gamma_*)^{-1} \\ -B_{gf}(\gamma_*,\theta_*)A_f(\theta_*)^{-1} & -B_g(\gamma_*)A_g(\gamma_*)^{-1} \end{bmatrix},
\end{equation}
где $A_f(\theta_*) = \E_0[\partial{\ln f}^2/\partial{\theta}\partial{\theta}']$, $B_f(\theta_*) = \E_0[(\partial{\ln f}/\partial{\theta})(\partial{\ln f}/\partial{\theta}')]$. Матрицы $A_g(\gamma_*)$ и $B_g(\gamma_*)$ аналогично определяются для плотности $g(\cdot)$, кросс-матрица $B_{fg}(\theta_*, \gamma_*) = \E_0[(\partial{\ln f}/\partial{\theta})(\partial{\ln g}/\partial{\gamma}')]$, и математические ожидания считаются по истинному процессу, порождающему данные. Объяснение и вывод этих результатов можно посмотреть у Вуонга (1989).

Если наоборот $f(y|x, \theta_*) \not= g(y|x, \gamma_*)$, то при нулевой гипотезе
\begin{equation}
N^{-1/2}LR(\hat{\theta},\hat{\gamma}) \stackrel{d}{\rightarrow} \mathcal{N}[0, w_*^2],
\end{equation}
где
\begin{equation}
w_*^2 = V_0 \left[ \ln \frac{f(y|x, \theta_*)}{g(y|x, \gamma_*)} \right],
\end{equation}
и дисперсия считается по истинному процессу, порождающему данные. Вывод можно посмотреть в работе Вуонга (1989).

Использование этих результатов зависит от вида вложенности двух моделей и от того, предполагается ли, что одна модель верно специфицирована, или нет.

Вуонг выбирал между тремя типами сравнения моделей. Модели $F_{\theta}$ и $G_{\gamma}$ являются (1) вложенными так, что $G_{\gamma}$ вложена в $F_{\theta}$, если $G_{\gamma} \subset F_{\theta}$; (2) строго невложенными моделями тогда и только тогда, когда $F_{\theta} \cap G_{\gamma} = \emptyset$, то есть ни одну из моделей нельзя привести к другой; и (3) пересекающимися, если $F_{\theta} \cap G_{\gamma} \not= \emptyset$, $F_{\theta} \varsubsetneq G_{\gamma}$ и $G_{\gamma} \varsubsetneq F_{\theta}$. Похожие методы выбора описаны Песараном и Песараном (1995).

И (2), и (3) являются невложенными моделями, но они требуют различных процедур тестирования. Примерами строго невложенных моделей являются линейные модели с различным распределением ошибок и нелинейные регрессионные модели с одним и тем же распределением ошибок, но разными функциональными формами условного математического ожидания. Для пересекающихся моделей некоторые частные случаи этих двух моделей совпадают. Примером являются линейные модели с частично совпадающим множеством регрессоров.

\begin{center}
Вложенные модели
\end{center}

Для вложенных моделей обязательно выполнено условие $f(y|x, \theta_*) = g(y|x, \gamma_*)$. Для $G_{\gamma}$, вложенной в $F_{\theta}$, $H_0$ проверяется с помощью $H_f: \E_0[\ln (f/g)] > 0$.

Для плотности, возможно, неверно специфицированной, результат (8.44) про взвешенную сумму хи-квадратов остается верным, если использовать собственные значения $\hat{\lambda}_j$ выборочного аналога $W$ из (8.45). Также можно использовать собственные значения $\tilde{\lambda}_j$ выборочного аналога для меньшей матрицы
\[
\underbar{W} = B_f(\theta_*)[D(\gamma_*)A_g(\gamma_*)^{-1}D(\gamma_*)' - A_f(\theta_*)^{-1}],
\]
где $D(\gamma_*) = \partial{\phi(\gamma_*)}/\partial{\gamma}$ и $\tilde{\theta} = \phi(\hat{\gamma})$ --- оценка, полученная с помощью ограниченного метода квази-максимального правдоподобия, как и у Вуонга (1989). Данный тест является робастной версией стандартного теста отношения правдоподобия на вложенные модели.

Если плотность $f(\cdot)$ на самом деле верно специфицирована, или в более общем случае удовлетворяет
равенству информационных матриц, мы получим ожидаемый результат, что $2LR(\hat{\theta}, \hat{\gamma}) \stackrel{d}{\rightarrow} \chi^2(p - q)$, так как $(p - q)$ собственных значений $W$ или $\underbar{W}$ равны единице, в то время как все остальные равны нулю.

\begin{center}
Строго невложенные модели
\end{center}

Для строго невложенных моделей выполнено условие $f(y|x, \theta_*) \not= g(y|x, \gamma_*)$. Результат (8.46) о нормальности распределения верен, а состоятельная оценка $w_*^2$ имеет вид:
\begin{equation}
\hat{w}^2 = \frac{1}{N} \sum_{i=1}^N \left( \ln \frac{f(y_i|x_i, \hat{\theta})}{g(y_i|x_i, \hat{\gamma})} \right)^2 - \left( \frac{1}{N} \sum_{i=1}^N \ln \frac{f(y_i|x_i, \hat{\theta})}{g(y_i|x_i, \hat{\gamma})} \right)^2.
\end{equation}
Поэтому
\begin{equation}
T_{LR} = N^{-1/2}LR(\hat{\theta}, \hat{\gamma})/\hat{w} \stackrel{d}{\rightarrow} \mathcal{N}[0,1].
\end{equation}
Для тестов с критическим значением $c$, $H_0$ отвергается в пользу $H_f: \E_0[\ln (f/g)] > 0$, если $T_{LR} > c$, $H_0$ отвергается в пользу $H_g: \E_0[\ln (f/g)] < 0$, если $T_{LR} < - c$, и выбор между двумя моделями невозможен, если $|T_{LR}| < c$. Данный тест может быть модифицирован, чтобы можно было вводить штрафы на базе логарифма функции правдоподобия аналогично $AIC$ и $BIC$, см. Вуонг (1989, стр. 316). Асимптотически эквивалентную статистику для (8.49) можно получить, заменив $\hat{w}^2$ на $\tilde{w}^2$, равной первому члену правой части выражения (8.48).

Данный тест предполагает, что обе модели имеют неверную спецификацию. Если вместо этого предполагается, что одна из моделей имеет верную спецификацию, то необходимо использовать подход Кокса из раздел 8.5.2.

\begin{center}
Пересекающие модели
\end{center}

Для пересекающихся моделей неясно априори, будет ли $f(y|x, \theta_*) = g(y|x, \gamma_*)$ или нет, и необходимо сначала проверить это условие.

Вуонг (1989) предлагает проверку на то, равна ли нулю дисперсия $w_*^2$, определённая в (8.47), так как $w_*^2 = 0$, если и только если $f(\cdot) = g(\cdot)$. Таким образом, необходимо вычислить $\hat{w}^2$ из (8.48). При нулевой гипотезе $H_0^w: w_*^2 = 0$
\begin{equation}
N\hat{w}^2 \stackrel{d}{\rightarrow} M_{p + q}(\lambda_*),
\end{equation}
где распределение $M_{p + q}(\lambda_*)$ определяется как (8.44). Гипотеза $H_0^w$ отвергается на уровне значимости $\alpha$, если $N\hat{w}^2$ превышает верхний процентный квантиль $\alpha$ у $M_{p + q}(\hat{\lambda})$ распределения, при этом используются собственные значения $\hat{\lambda}_j$ выборочного аналога $W$ из (8.45). Альтернативно и более просто можно проверить гипотезу, что $\theta_*$ и $\gamma_*$ удовлетворяют равенству $f(\cdot) = g(\cdot)$. Примеры приведены Лиеном и Вуонгом (1987).

Если $H_0^w$ не отвергается или условия для равенства $f(\cdot) = g(\cdot)$ не отвергаются, то можно сделать вывод, что невозможно выбирать между двумя моделями с учётом имеющихся данных. Если $H_0^w$ отвергается или  условия для равенства $f(\cdot) = g(\cdot)$  отвергаются, то необходимо протестировать $H_0$ против $H_f$ или $H_g$, используя $T_{LR}$, как указано в случае строго невложенных моделей. В этом случае уровень значимости не превышает максимального уровни значимости для каждого из двух тестов.

Этот тест предполагает, что обе модели имеют неверную спецификацию. Если вместо этого предполагается, что одна из моделей верно специфицирована, то другая модель также должна быть верно специфицирована, для того чтобы обе модели были эквивалентными. Таким образом, $f(y|x, \theta_*) = g(y|x, \gamma_*)$ при $H_0$, и можно перейти к тесту отношения правдоподобия, используя взвешенный хи-квадрат результат (8.44). Пусть $c_1$ и $c_2$ --- это верхнее и нижнее критические значения соответственно. Если $2LR(\hat{\theta}, \hat{\gamma}) > c_1$ , то $H_0$ отвергается в пользу $H_f$, если $2LR(\hat{\theta}, \hat{\gamma}) < c_2$, то $H_0$ отвергается в пользу $H_g$, и в противном случае тест не позволяет сделать однозначного  вывода.

\begin{center}
Другие сравнения невложенных моделей
\end{center}

Предшествующие методы можно использовать только для полностью параметрических моделей. Методы для выбора между моделями, которые лишь частично параметризованы, такие, как линейная регрессия без предположения о нормальном распределении, являются менее изученными.

Информационный критерий из раздела 8.5.1 можно заменить критериями, разработанными с использованием функций потерь, отличных от $KLIC$. Различные меры, соответствующие различным функциям потерь, представлены у Амэмия (1980). Эти меры часто предназначены для вложенных моделей, но также могут быть применимы и для невложенных моделей.

Простой подход заключается в сравнении способности прогнозирования, т.е. выбирает модель с наименьшим значением среднеквадратической ошибки $(N - q)^{-1}\sum_i (y_i - \hat{y}_i)^2$. Для линейной регрессии это эквивалентно выбору модели с наиболее высоким скорректированным $R^2$, который обычно рассматривается как ставящий слишком маленький штраф за сложность модели. Модификация для непараметрической регрессии --- кросс-валидация с исключением отдельных наблюдений (см. раздел 9.5.3).

Формальные тесты для выбора между невложенными моделями в случае, отличном от метода максимального правдоподобия, часто используют один из двух подходов. Искусственное вложение, предложенное Дэвидсоном и МакКинноном (1984), подразумевает вложение двух невложенных моделей в более общую искусственную модель и приводит к так называемым $J$ тестам, $P$ тестам и связанным с ними тестам. Принцип всеобщности, предложенный Мизоном и Ричардом (1986), приводит к общим условиям для тестирования одной модели против альтернативной невложенной модели. Уайт (1994) связывает этот подход с тестами на условные моменты. Обзор этих  источников можно почитать у Дэвидсона и МакКиннона (1993, глава 11).

\subsection{Пример невложенных моделей}

Сгенерируем выборку из 100 наблюдений с помощью модели Пуассона с математическим ожиданием $\E[y|x] = \exp(\beta_1 + \beta_2x_2 + \beta_3x_3)$, где $x_2, x_3 \sim \mathcal{N}[0,1]$ и $(\beta_1, \beta_2, \beta_3) = (0.5, 0.5, 0.5).$

\begin{table}[h]
\begin{center}
\caption{\label{tab:nonnestpoiss} Сравнение невложенных моделей для примера регрессии Пуассона}
\begin{minipage}{17cm}
\begin{tabular}[t]{llll}
\hline
\hline
\bf{Тип теста}\footnote{$N = 100$. Модель 1 --- регрессия Пуассона $y$ на константу и $x_2$. Модель 2 --- регрессия Пуассона $y$ на константу, $x_3$ и $x_3^2$. Две последние строчки приведены для теста Вуонга на непересекающиеся модели (см. текст).} & \bf{Модель 1} & \bf{Модель 2} & \bf{Вывод} \\
\hline
$-2\ln L$ & 366.86 & 352.18 & Предпочитается модель 2 \\
$AIC$ & 370.86 & 358.18 & Предпочитается модель 2 \\
$BIC$ & 376.06 & 366.00 & Предпочитается модель 2 \\
$N\hat{w}^2$ & 7.84 c $p = 0.000$ &  & $T_{LR}$ тест применим \\
$T_{LR} = N^{-1/2}LR/\hat{w}$ & $-0.883$ c $p = 0.377$ &  & Ни одна модель не предпочитается \\
\hline
\hline
\end{tabular}
\end{minipage}
\end{center}
\end{table}

Зависимая переменная $y$ имеет выборочное среднее 1.92 и стандартное отклонение 1.84. Две неверные невложенные модели были оценены с помощью регрессии Пуассона:
\[
\text{Модель 1}: \hat{\E}[y|x] = \exp(\underset{(8.08)}{0.608} + (\underset{(4.03)}{0.291}x_2),
\]
\[
\text{Модель 2}: \hat{\E}[y|x] = \exp(\underset{(5.14)}{0.493} + (\underset{(5.10)}{0.359}x_3 + (\underset{(1.78)}{0.091}x_3^2),
\]
где $t$-статистики приведены в скобках.

В первых трёх строках таблицы 8.2 представлены различные информационные критерии, а также модель с наименьшим предпочтительным значением критерия. Первый не штрафует число параметров и даёт результат в пользу модели 2. Второй и третий способы, определённые в (8.40) и (8.41), накладывают больший штраф на модель 2, которая имеет дополнительный параметр, но всё же дают результат в пользу модели 2.

Последние две строки таблицы 8.2 суммируют информацию о тесте Вуонга на пересекающиеся модели.

Сначала необходимо проверить условие равенства функций плотности при оценке псевдо-истинных значений. Статистика $\hat{w}^2$ из (8.48) легко вычисляется, если даны выражения для функций плотности. Сложность заключается в вычислении оценки матрицы $W$ из (8.45). Для распределения Пуассона мы можем использовать $\hat{A}$ и $\hat{B}$, которые были определены в конце раздела 5.2.3, и $\hat{B}_{fg} = N^{-1}\sum_i (y_i - \hat{\mu}_{fi})x_{fi} \times (y_i - \hat{\mu}_{gi})x_{gi}$. Собственные значения $W$ равны $\lambda_1 = 0.29$, $\lambda_2 = 1.00$, $\lambda_3 = 1.06$, $\lambda_4 = 1.48$ и $\lambda_5 = 2.75$. $P$-значение для тестовой статистики $N\hat{w}^2$, распределение которой приведено в (8.44), --- доля случаев в выборке в которых $\sum_{j=1}^5 \lambda_j z_j^2$, превышает $N\hat{w}^2 = 69.14$. Можно сгенерировать, например, 10000 случайных наблюдений. Здесь $p = 0.000 < 0.05$. Отсюда можно заключить, что можно сравнивать модели. Критическое значение на уровне значимости 0.05 в данном случае равно 16.10. Это немного выше, чем $\chi_{0.05}^2 (5) = 11.07$.

Учитывая то, что возможно сравнение, можно применять второй тест. Здесь $T_{LR} = - 0.883$ выступает в пользу второй модели, так это значение отрицательно. Однако, стандартный нормальный двусторонний тест, на уровне значимости 5\% говорит, что разница не является статистически значимой. В этом примере значение $\hat{w}^2$  довольно велико, поэтому первая тестовая статистика $N\hat{w}^2$ имеет большое значение, а вторая тестовая статистика $N^{-1/2}LR(\hat{\theta},\hat{\gamma})$ имеет маленькое значение.

\section{Последствия проверки гипотез}

На практике проводят несколько тестов, прежде чем выбирают предпочтительную модель. Это приводит к ряду осложнений, которые практики обычно игнорируют.

\subsection{Предварительное оценивание перед проведением тестов}

Использование тестов на спецификацию при выборе модели усложняет распределение оценки. Предположим, что мы выбираем между двумя оценками $\hat{\theta}$ и $\tilde{\theta}$ с помощью статистического теста на уровне значимости 5\%. Например, $\hat{\theta}$ и $\tilde{\theta}$ могут быть оценками для ограниченной и неограниченной моделей соответственно. 

Тогда фактическая оценка имеет вид $\theta^+ = w\hat{\theta} + (1 - w)\tilde{\theta}$, где случайная величина $w$  принимает значение 1, если тест выступает в пользу $\hat{\theta}$, и 0, если тест выступает в пользу $\tilde{\theta}$. То есть оценка зависит от оценок ограниченной и неограниченной моделей и от случайной величины $w$, которая, в свою очередь, зависит от уровня значимости теста. Следовательно $\theta^+$ --- оценка со сложными свойствами. Данная оценка называется оценкой с предварительным тестированием, или претест-оценкой (pretest estimator). Распределение $\theta^+$ было получено для модели линейной регрессии при условии нормального распределения остатков, и оно не является стандартным.

В теории статистические выводы должны быть основаны на распределении $\theta^*$. На практике вывод основывается на распределении $\hat{\theta}$, если $w = 1$, или $\tilde{\theta}$, если $w = 0$, при этом игнорируется наличие случайности в  $w$. Это делается для простоты, так как даже в простейших моделях распределение оценки становится крайне сложным, когда проводится несколько таких тестов.

\subsection{Порядок тестирования}

Можно сделать различные выводы в зависимости от порядка, в котором проводятся тесты.

Один из возможных порядков ---  от общих к частным моделям. Например, можно оценить общую модель спроса перед тестированием ограничений из теории потребительского спроса таких, как однородность и симметрия. Или  можно идти от частной к общей модели с добавлением регрессоров по мере необходимости и дополнительных обобщений таких, как учёт  эндогенности, если она присутствует. Эти два упорядочения являются естественными при выборе регрессоров для включения в модель, но когда проводятся тесты на спецификацию, часто используются оба способа в одном и том же исследовании.

Связанный с этим вопрос заключается в выборе между совместными и отдельными тестами. Например, значимость двух регрессоров может быть проверена либо с помощью двух отдельных $t$-тестов на значимость, либо с помощью совместного $F$-теста, или $\chi^2(2)$ теста на значимость. Общее описание было представлено в разделе 7.2.7. Пример приведён ниже в разделе 18.7.

\subsection{Интеллектуальный анализ данных}

Доведённое до крайности широкое использование тестов для выбора модели было названо добычей данных или интеллектуальным анализом данных (Ловелл, 1983). Например, можно искать среди нескольких сотен показателей, которые, возможно, предсказывают $y$, и выбрать только те, которые являются значимыми на уровне значимости 5\% при проведении двустороннего теста. Существуют компьютерные программы, которые автоматизируют такой процесс поиска, и они часто используются в некоторых отраслях прикладной статистики. К сожалению, такой широкий поиск приводит к обнаружению ложных соотношений, так как тест с уровнем значимости 0.05 приводит к ошибочному обнаружению зависимости в 5\% случаев. Ловелл отметил, что применение такой методологии, как правило, ведёт к завышению параметров качества подгонки (например, $R^2$) и к недооценке выборочных дисперсий коэффициентов регрессии, даже если она успешно определит переменные, которые отражают особенности процесса, порождающего данные. Использование стандартных тестов и приведение $p$-значений без учёта процедуры поиска модели может ввести в заблуждение, поскольку номинальные и истинные $p$-значения не являются одинаковыми. Уайт (2001б) и Салливан, Тиммерманн и Уайт (2001) показывают, как использовать метод бутстрэп, чтобы рассчитать истинную статистическую значимость регрессоров. Об этом также можно посмотреть у П. Хансена (2003).

Мотивацией для интеллектуального анализа данных иногда может быть сохранение степеней свободы или избежание сверхпараметризации. Что ещё более важно, многие аспекты спецификации, например, функциональная форма зависимости, остаются нерешёнными лежащей в основе теорией. Учитывая возможность ошибки спецификации,  существуют аргументы в пользу поиска спецификации (Сарган, 2001). Тем не менее, необходимо быть внимательным, особенно когда анализируются малые выборки и количество шагов при поиске спецификации велико по отношению размера выборки. Когда процесс поиска спецификации является пошаговым с большим числом шагов и каждый новый шаг зависит от результатов предыдущего теста, статистические свойства процедуры в целом являются сложными и их нельзя получить аналитически.

\subsection{Практический подход}

Прикладные микроэконометрические исследования обычно минимизирует проблему претест оценивания путём разумного использования проверки гипотез. Экономическая теория используется при выборе регрессоров, чтобы значительно уменьшить число потенциальных регрессоров. Если выборка имеет большой размер, то не имеет смысла исключать <<незначимые>> переменные. Финальная модель часто включает статистически незначимые регрессоры такие, как дамми на регион и отрасль  или профессию в регрессии дохода. Замусоривания отчёта можно избежать, не включив в него незначимые коэффициенты, но обратив внимание на сам факт их наличия.  Включение большого количества переменных может привести к некоторой потере точности оценки ключевых регрессоров таких, как длительность школьного образования в регрессии дохода, но защищает от смещения, вызванного ошибочным исключением переменных, которые должны быть включены.

Хорошей практикой является использование части выборки (<<обучающей выборки>>) для поиска спецификации и выбора модели, а затем сообщить о результатах оценивания выбранной модели, используя другую части выборки (<<тестовую выборка>>). В таком случае предварительное тестирование не влияет на распределение оценки, если подвыборки являются независимыми. Эта процедура, как правило, применяется, только когда размер выборки очень большой, потому что использование неполной выборки в конечном оценивании приводит к потере в точности оценки.

\section{Диагностика модели}

В этом разделе мы рассмотрим показатели качества подгонки и варианты определения остатков в нелинейных моделях. Полезные являются те показатели, который позволяют выявить недостатки модели.

\subsection{Псевдо-$R^2$ показатели}

Качество подгонки интерпретируется как близость оценённых значений к выборочным значениям зависимой переменной. 

Для линейных моделей с $K$ регрессорами наиболее прямой мерой является стандартная ошибка регрессии, которая является оценённым значением стандартного отклонения случайной ошибки,

\[
s = \left[ \frac{1}{N - K} \sum_{i=1}^N (y_i - \hat{y}_i)^2 \right]^{1/2}.
\]

Например, стандартная ошибка, равная 0.10, в регрессии логарифма дохода означает, что примерно 95\% от оценённых значений находятся в пределах 0.20 от фактического значения логарифма дохода, или в пределах 22\% от фактического дохода т.к. $e^{0.2} \simeq 1.22$. Этот показатель совпадает с выборочной среднеквадратическая ошибка, $\hat{y}_i$ рассматривается как прогноз $y_i$, только число степеней свободы скорректировано. В качестве альтернативы можно использовать среднюю абсолютную ошибку $(N - K)^{-1}\sum_i |y_i - \hat{y}_i|$. Можно использовать те же самые показатели для нелинейных регрессионных моделей при условии, что нелинейные модели дают прогнозные значения зависимой переменной $\hat{y}_i$.

Похожий показатель в линейных моделях --- $R^2$, коэффициент множественной детерминации. Он равен доле выборочной дисперсии зависимой переменной, которая объяснена регрессорами. Величина $R^2$ рассчитывается чаще, чем $s$, хотя $s$ может быть более информативной для оценки качества подгонки.

Псевдо-$R^2$ является обобщением $R^2$ для нелинейной регрессионной модели. Есть несколько интерпретаций $R^2$ в линейной модели. Они приводят к нескольким возможным псевдо-$R^2$ показателям, которые отличаются в нелинейных моделях и необязательно  лежат между нулём и единицей или увеличиваются при добавлении регрессоров. Мы представляем некоторые из этих показателей и  для простоты не корректируем их на степени свободы.

Один из подходов к $R^2$ основан на  разложении общей суммы квадратов $(TSS)$, 
\[
\sum_{i} (y_i - \bar{y})^2 = \sum_{i} (y_i - \hat{y}_i)^2 + \sum_{i} (\hat{y}_i - \bar{y})^2 + 2\sum_{i} (y_i - \hat{y}_i)(\hat{y}_i - \bar{y})
\]

Первая сумма в правой части --- сумма квадратов остатков $(RSS)$, а вторая --- объяснённая сумма квадратов $(ESS)$. Это приводит к двум возможным показателям:
\[
R_{RES}^2 = 1 - RSS/TSS
\]
\[
R_{EXP}^2 = ESS/TSS
\]

Для МНК регрессии в линейной модели с константой третья сумма равна нулю, поэтому $R_{RES}^2 = R_{EXP}^2$. Однако это упрощение невозможно в других моделях, и в общем случае $R_{RES}^2 \not= R_{EXP}^2$ в нелинейных моделях. Показатель $R_{RES}^2$ может быть меньше нуля, $R_{EXP}^2$ может превышать единицу, и оба показателя могут уменьшаться при добавлении регрессоров, хотя $R_{RES}^2$ увеличится для НМНК регрессии нелинейной модели, так как тогда оценка минимизирует $RSS$.

Другой показатель равен
\[
R_{COR}^2 = \widehat{\Cor}^2[y_i, \hat{y}_i],
\]
квадрату выборочной корреляции между фактическими и оценёнными значения. Показатель $R_{COR}^2$ лежит между нулём и единицей, и он равен $R^2$ в $OLS$ регрессии для линейной модели с константой. В нелинейных моделях $R_{COR}^2$ может уменьшиться при добавлении регрессоров. 

Третий подход использует взвешенные суммы квадратов, которые учитывают внутреннюю гетероскедастичность пространственных данных. Пусть $\hat{\sigma}_i^2$ --- оценка условной дисперсии $y_i$, где предполагается, что гетероскедастичность явно смоделирована как в случае с ДОМНК или для таких моделей, как логит и модель Пуассона. Тогда мы можем использовать
\[
R_{WSS}^2 = 1 - WRSS/WTSS,
\]
где взвешенные сумма квадратов остатков $WRSS = \sum_i (y_i - \hat{y}_i)^2/\hat{\sigma}_i^2$, $WTSS = \sum_i (y_i - \hat{\mu})^2/\hat{\sigma}^2$, $\hat{\mu}$ и $\hat{\sigma}^2$ --- оценка математического ожидания и оценка дисперсии в модели только с константой. Данный показатель можно назвать $R^2$ Пирсона, потому что $WRSS$ равно статистике Пирсона, которая без поправок на конечную выборку должна быть равна $N$, если гетероскедастичность правильно смоделирована. Следует отметить, что $R_{WSS}^2$ может быть меньше нуля, и оно уменьшается при добавлении регрессоров.

Четвёртый подход является обобщением $R^2$ до целевых функций помимо суммы квадратов остатков. Пусть $Q_N(\theta)$ обозначает целевую функцию, которая максимизируется. $Q_0$ --- её значение в модели только с константой, $Q_{fit}$ --- значение в оценённой модели, а $Q_{max}$ --- наибольшее возможное значение $Q_N(\theta)$. Тогда максимально возможный прирост целевой функции, который получается в результате включения регрессоров, --- $Q_{max} - Q_0$, а фактический прирост --- $Q_{fit} - Q_0$. Тогда показатель выглядит так:
\[
R_{RG}^2 = \frac{Q_{fit} - Q_0}{Q_{max} - Q_0} = 1 - \frac{Q_{max} - Q_{fit}}{Q_{max} - Q_0},
\]
где индекс $RG$ внизу означает относительный прирост (relative gain). Для оценивания методом МНК максимум функции потерь ---  минус сумма квадратов остатков. Тогда $Q_0 = - TSS$, $Q_{fit} = - RSS$ и $Q_{max} = 0$. Тогда $R_{RG}^2 = ESS/TSS$ для МНК или НМНК регрессии. Показатель $R_{RG}^2$ имеет то преимущество, что он лежит между нулём и единицей и он увеличивается при добавлении регрессоров. Для оценивания методом максимального правдоподобия функция потерь имеет вид $Q_N(\theta) = \ln L_N(\theta)$. Тогда $R_{RG}^2$ не может быть всегда применён, так как в некоторых моделях может и не быть границы $Q_{max}$. Например, для линейной модели с нормальными остатками $L_N(\beta, \sigma^2) \rightarrow \infty$ при $\sigma^ 2 \rightarrow 0$. Для метода максимального правдоподобия и метода квази-максимального правдоподобия при оценивание линейных моделей из экспоненциального семейства, например, логит или модели Пуассона, обычно известно значение $Q_{max}$. Можно показать, что $R_{RG}^2$ --- это $R^2$, основанный на остатках отклонений (deviance residuals), которые определены в следующем разделе.

Связанный с $R_{RG}^2$ показатель --- $R_{Q}^2 = 1 - Q_{fit}/Q_0$. Этот показатель увеличивается при добавлении регрессоров. Он равен $R_{RG}^2$, если $Q_{max} = 0$, что выполняется в случае МНК-регрессии, а также для бинарных и полиномиальных моделей. В противном случае для дискретных данных этот показатель может иметь верхнюю границу, которая меньше единицы, тогда как для непрерывных данных показатель не может быть ограничен нулём и единицей, так как логарифм функции правдоподобия может быть положительным или отрицательным. Например, для оценивания методом максимального правдоподобия с непрерывной функцией плотности возможно, что $Q_0 = 1$ и $Q_{fit} = 4$, что приводит к $R_Q^2 = -3$, или, что $Q_0 = - 1$ и $Q_{fit} = 4$, что приводит к $R_Q^2 = 5$.

Для нелинейных моделей, следовательно, не существует универсального псевдо-$R^2$. Самыми полезными показателями могут быть $R_{COR}^2$, так как коэффициенты корреляции легко интерпретируются, и $R_{RG}^2$ в особых случаях, когда $Q_{max}$ известно. Кэмерон и Виндмейер (1997) анализируют многие показатели, а Кэмерон и Виндмейер (1996) применяют эти показатели на реальных данных.

\subsection{Анализ остатков}

Микроэконометрический анализ фактически уделяет мало внимания анализу остатков по сравнению с некоторыми другими областями статистики. Если наборы данных малы, есть опасение, что анализ остатков может привести к модели с излишней подгонкой. Если набор данных большой, то существует мнение, что анализ остатков может оказаться ненужным, так как одно наблюдение мало повлияет на анализ. Поэтому мы приведём краткое описание. Более обширное описание приведено, например, в работе МакКуллаха и Нельдера (1989), а также Кэмерона и Триведи (1998, глава 5). Эконометристы много внимания уделяли способам  определения остатков в цензурированных и усеченных моделях.

Много вариантов остатков было предложены для нелинейной регрессионной модели. Рассмотрим скалярную зависимую переменную $y_i$ с оценёнными значениями $\hat{y}_i = \hat{\mu}_i = \mu(x_i, \hat{\theta})$. Обычные остатки --- это $r_i = y_i - \hat{\mu}_i$. Остатки Пирсона являются очевидной поправкой на гетероскедастичность --- $p_i = (y_i - \hat{\mu}_i)/\hat{\sigma}_i$, где $\hat{\sigma}_i$ является оценкой условной дисперсии $y_i$. Это требует  спецификации дисперсии для $y_i$, что сделано, например, в рамках модели Пуассона. Для распределения из экспоненциального семейства (см. раздел 5.7.3) определяют остатки отклонений --- это $d_i = \sign(y_i - \hat{\mu}_i)\sqrt{2[l(y_i) - l(\hat{\mu}_i)]}$, где $l(y)$ обозначает логарифм функции плотности $y|\mu$, оценённый в точке $\mu = y$, а $l(\hat{\mu})$ обозначает оценивание в точке $\mu = \hat{\mu}$. Логика остатков отклонений состоит в том, что сумма квадратов этих остатков является статистикой отклонения (deviance statistic) --- обобщением для моделей экспоненицального семейства суммы квадратов обычных остатков в линейной модели. Остатки Анскомба определяются как преобразование $y$, при котором получается распределение наиболее близкое к нормальному, а затем проводится масштабирование к нулевому математическому ожиданию и единичной дисперсии. Это преобразование было получено для распределений из экспоненциального семейства.

Поправки остатков для малых выборок были предложены для учёта ошибки оценивания в $\hat{\mu}_i$. В линейной модели используется деление остатков на $\sqrt{1 - h_{ii}}$, где $h_{ii}$ --- диагональный элемент матрицы $H = X(X'X)^{-1}X$. Считается, что эти остатки показывают более хорошие результаты на конечных выборках. Так как матрица $H$ имеет ранг $K$, равный числу регрессоров, среднее значение $h_{ii}$ равно $K/N$, а значения $h_{ii}$, превышающие $2K/N$, рассматриваются как имеющие большой рычаг. Эти результаты распространяются на модели экспоненциального семейства с $H = W^{1/2}X(X'WX)^{-1}XW^{1/2}$, где $W = \Diag[w_{ii}]$ и $w_{ii} = g'(x_i'\beta)/\sigma_i^2$ с $g(x_i'\beta)$ и $\sigma_i^2$ --- заданное условное математическое ожидание и заданная дисперсия соответственно. Маккуллаг и Нельдер (1989) приводят описание данного обобщения.

Кокс и Снелл (1968) определяют обобщённые остатки как любую скалярную функцию $r_i = r(y_i, x_i, \theta)$, которая удовлетворяет некоторым относительно слабым условиям. Причина, из-за которой такие остатки возникают, состоит в том, что многие оценки имеют условие первого порядка вида $\sum_i g(x_i, \theta)r(y_i, x_i, \hat{\theta}) = 0$, где $y_i$ встречается в скаляре $r(\cdot)$, но не в векторе $g(\cdot)$. Про этот сюжет можно также прочитать у Уайта (1994).

Для регрессионных моделей, основанных на нормально распределенной скрытой переменной, (см. главы 14 и 16), Чешер и Айриш (1987) предлагают использовать $\E[\e_i^*|y_i^*]$ в качестве остатокв, где $y_i^* = \mu_i + \e_i^*$ --- ненаблюдаемая скрытая переменная, и $y_i = g(y_i^*)$ --- наблюдаемая зависимая переменная. Частные случаи $g(\cdot)$ соответствуют пробит и тобит моделям. Гурьеру и другие (1987) обобщают этот подход для распределений из экспоненциального семейства. Естественный подход здесь заключается в рассмотрении  остатков как пропущенной переменной, по аналогии с  алгоритмом максимизации ожидания из раздела 10.3.

Обычно изображают графики, на которых по одной оси изображена интересующая переменная, а по другой --- остатки. Например, график остатков против оценённых значений может выявить плохую подгонку модели; график остатков против невключенных переменных может мотивировать включение дополнительных регрессоров в модель, а график остатков против включённых регрессоров может указать на то, что необходимо изменить функциональную форму. Может быть полезно включить линию непараметрической регрессии в таких графиках (см. главу 9). Если данные принимают всего несколько дискретных значений, то может быть трудно интерпретировать графики из-за скученности данных в нескольких точках. Поэтому может быть полезно добавить небольшой искусственный случайный шум к данным, чтобы уменьшить скученность точек на графике.

Некоторые параметрические модели подразумевают, что  остатки должны иметь нормальное распределение. Это можно проверить с помощью графика квантилей, который сортирует остатки $r_i$ от меньшего к большему и отображает на графике значения остатков против предсказанных значений остатков, в предположении, что остатки имеют именно нормальное распределение. Таким образом, график отсортированных $r_i$ против $\bar{r} + s_r\Phi^{-1}((i - 0.5)/N)$, где $\bar{r}$ и $s_r$ --- выборочное среднее значение и стандартное отклонение $r$ и $\Phi^{-1}(\cdot)$ --- обратное значение функции распределения стандартного нормального распределения.

\subsection{Пример диагностики}

Таблица 8.3 использует тот же самый процесс, порождающий данные, что и в разделе 8.5.5. Зависимая переменная $y$ имеет выборочное среднее, равное 1.92, и стандартное отклонение, равное 1.84. Регрессия Пуассона $y$ на $x_3$ и $y$ на $x_3$ и $x_3^2$ даёт
\[
\text{Модель 1}: \hspace{0.5cm} \hat{\E}[y|x] = \exp(\underset{(5.20)}{0.586} + \underset{(7.60)}{0.389}x_3)
\]
\[
\text{Модель 2}: \hspace{0.5cm} \hat{\E}[y|x] = \exp(\underset{(5.14)}{0.493} + \underset{(5.10)}{0.359}x_3 + \underset{(1.78)}{0.091}x_3^2),
\]
где $t$-статистики приведены в скобках.

В этом примере все показатели $R^2$ увеличиваются с добавлением $x_3^2$ в качестве регрессора, при этом прибавка отличается довольно сильно, несмотря на то, что все кроме последнего $R^2$ изначально имеют близкие значения. В более общем случае первые три $R^2$ увеличиваются в похожих пропорциях, а $R_{RES}^2$ и $R_{COR}^2$ могут быть довольно близки, но остальные три показателя меняются довольно по-разному. Только два последние показателя $R^2$ гарантированно увеличатся при добавлении регрессоров в случае, когда целевая функция не является суммой квадратов ошибок. Показатель $R_{RG}^2$ может быть посчитан в данном случае, т.к.  логарифма функции правдоподобия для регрессии Пуассона достигает максимума, когда оценённое среднее равно $\hat{\mu}_i = y_i$ для всех $i$, что приводит к $Q_{max} = \sum_i [y_i\ln y_i - y_i - \ln y_i!]$, где $y\ln y = 0$ при $y = 0$.

Кроме того, три вида остатков были рассчитаны для второй модели. Выборочное среднее и стандартное отклонение остатков составили соответственно 0 и 1.65 для обычных остатков, 0.01 и 1.97 для остатков Пирсона, $- 0.21$ и 1.22 для остатков отклонений. Нулевое среднее для обычных остатков является свойством регрессии Пуассона с константой и присутствует в небольшом числе других моделей. Большое стандартное отклонение обычных остатков отражает отсутствие масштабирования и тот факт, что здесь стандартное отклонение $y$ превышает 1. Все попарные корреляции между этими остатками превышают 0.96. Это часто происходит, когда $R^2$ маленький, так что  $\hat{y}_i \simeq \bar{y}$.


\begin{table}[h]
\begin{center}
\caption{\label{tab:pseudor2} Псевдо-$R^2$: пример регрессии Пуассона}
\begin{minipage}{13.5cm}
\begin{tabular}[t]{l*{4}{{c}}}
\hline
\hline
\bf{Тип теста}\footnote{$N = 100$. Модель 1 --- регрессия Пуассона $y$ на константу и $x_3$. Модель 2 --- регрессия Пуассона $y$ на константу, $x_3$ и $x_3^2$. $RSS$ --- сумма квадратов остатков $(SS)$, $ESS$ --- объяснённая сумма квадратов $SS$, $TSS$ --- общая сумма квадратов, $WRSS$ --- взвешенный $RSS$, $WTSS$ --- взвешенный $TSS$, $Q_{fit}$ --- оценённое значение целевой функции, $Q_0$ --- оценённое значения в модели только с константой, и $Q_{max}$ --- максимальное возможное значение целевой функции при имеющихся данных, и это значение существует лишь для некоторых целевых функций.} & \bf{Модель 1} & \bf{Модель 2} & \bf{Разница} \\
\hline
$s, \text{где} \hspace{0.2cm} s^2 = RSS/(N - K)$ & 0.1662 & 0.1661 & 0.0001 \\
$R_{RES}^2 = 1 - RSS/TSS$ & 0.1885 & 0.1962 & + 0.0077 \\
$R_{EXP}^2 = ESS/TSS$ & 0.1667 & 0.2087 & + 0.0402 \\
$R_{COR}^2 = \widehat{\Cor}^2[y_i, \hat{y}_i]$ & 0.1893 & 0.1964 & + 0.0067 \\
$R_{WSS}^2 = 1 - WRSS/WTSS$ & 0.1562 & 0.1695 & + 0.0233 \\
$R_{RG}^2 = (Q_{fit} - Q_0)/(Q_{max} - Q_0)$ & 0.1552 & 0.1712 & + 0.0160 \\
$R_Q^2 = 1 - Q_{fit}/Q_0$ & 0.0733 & 0.0808 & + 0.0075 \\
\hline
\hline
\end{tabular}
\end{minipage}
\end{center}
\end{table}

\section{Практические соображения}

М-тесты и тесты Хаусмана легко провести с помощью вспомогательных регрессий. Следует помнить, что эти вспомогательные регрессии имеют смысл только при предположениях о распределении, которые сильнее, чем те, которые были сделаны для получения обычных скорректированных стандартных ошибок коэффициентов регрессии. Некоторые тесты с поправками были представлены в разделе 8.4.

С достаточно большим набором данных и при фиксированном уровне значимости таким, как 5\%, выборочные моментные условия, накладываемые моделью, будут отвергнуты, за исключением нереалистичного случая, когда все аспекты функциональной формы модели, регрессоры и распределение верно специфицированы. В классической проверке гипотез часто это и является желаемым результатом. В частности, при достаточно большой выборке коэффициенты регрессии всегда будет существенно отличаться от нуля, и цель многих исследований --- добиться такого результата. Тем не менее, для тестов на спецификацию желаемый результат состоит в том, чтобы не отвергать нулевую гипотезу, потому что тогда можно утверждать, что модель верно специфицирована. Возможно, именно по этой причине нечасто проводят тесты на спецификацию.

В качестве примера рассмотрим тесты на верную спецификацию моделей жизненного цикла потребления. Кроме случая, когда выборки малые, исследователь скорее всего будет отвергать модель на уровне значимости 5\%. Например, предположим, что статистика теста на спецификацию модели имеет $\chi^2(12)$ распределение, когда он проводится на выборке с $N = 3000$ и имеет $p$-значение 0.02. Неясно, действительно ли модель жизненного цикла  плохо описывает данные, хотя эта гипотеза формально будет отвергнута на уровне значимости 5\%. Одной из возможностей является увеличение критического значения с увеличением размера выборки, как при  использовании  $BIC$ (см. раздел 8.5.1).

Другая причина неширокого использования тестов на спецификацию --- трудность вычислений и большая вероятность ошибки первого рода, если используются более удобные вспомогательные регрессии при построении асимптотически эквивалентной версии теста. Эти недостатки могут быть значительно снижены при использовании метода бутстрэп. Глава 11 представляет метод бутстрэп для проведения многих тестов, приведённых в этой главе.

\section{Библиографические заметки}

\begin{itemize}
\item [$8.2$] Тест на условный момент, предложенный Ньюи (1985) и Таушеном (1985), является обобщением информационно матричного теста Уайта (1982). Для оценивания с помощью метода максимального правдоподобия расчёт М-теста для вспомогательной регрессии обобщает методы Ланкастера (1984) и Чешера (1984) для информационно матричного теста. Хороший обзор М-тестов представлен у Пагана и Велла (1989). М-тест определяет общий подход к тестированию. Частными случаями М-теста являются  все тесты Вальда, тесты множителей Лагранжа, тесты отношения правдоподобия и тесты Хаусмана. Эта унификация представлена в работе Уайта (1994).
\item [$8.3$] Тест Хаусмана, предложенный Хаусманом (1978), был введён ранее в разделе 8.3, а также хорошее описание было сделано Руудом (1984).
\item [$8.4$] В эконометрических работах Грина (2003), Дэвидсона и Маккинона (1993), а также Вулдриджа (2002) описаны стандартные тесты на спецификацию. 
\item [$8.5$] Песаран и Песаран (1993) рассматривают в своей работе, каким образом можно применить тест Кокса на невложенные модели (1961, 1962б), когда нельзя получить аналитическое выражение для математического ожидания логарифма функции правдоподобия. В таком случае можно применять тест Вуонга (1989).
\item [$8.7$] Диагностика нелинейных моделей часто проводится с помощью расширения результатов для линейной регрессионной модели до обобщённой линейной модели такой, как, например, логит или модель Пуассона. Детальное обсуждение со ссылками на литературу представлено в работе Кэмерона и Триведи (1998, глава 5).
\end{itemize}


\section{Упражнения}

\begin{enumerate}
\item [$8 - 1$] 

Предположим, что $y = x'\beta + u$, где $u \sim \mathcal{N}[0,\sigma^2]$ с вектором параметром $\theta = [\beta', \sigma^2]'$ и функцией плотности $f(y|\theta) = (1/\sqrt{2\pi}\sigma)\exp[-(y - x'\beta)^2/2\sigma^2].$  У нас есть выборка из $N$ независимых наблюдений.
\begin{enumerate}
\item Объясните, почему тест на моментное условие $\E[x(y - x'\beta)^3]$ является тестом на проверку предположения о нормальном распределении ошибок.
\item Приведите выражения для $\hat{\mu}_i$ и $\hat{s}_i$, заданных в (8.5). Они необходимы для проведения М-теста, основанного на моментном условии из пункта (а).
\item Пусть $\dim[x] = 10$, $N = 100$, и вспомогательная регрессия из (8.5) даёт нецентрированный $R^2$, равный 0.2. Какой вывод Вы сделаете на уровне значимости 0.05?
\item Для этого примера приведите моментные условия, которые проверяются с помощью информационно матричного теста Уайта.
\end{enumerate}
\item [$8 - 2$] Рассмотрите мультиномиальную версию $PCGF$ теста из (8.23) c $p_j$, заменённым на $\hat{p}_j = N^{-1}\sum_i F_j(x_i, \hat{\theta})$. Покажите, что $PCGF$ можно выразить как $CGF$ из (8.27)  с $V = \Diag[N\hat{p}_j]$. [Сделайте вывод, что в мультиномиальном случае тестовую статистику Андриуса можно упростить до статистики Пирсона].
\item [$8 - 3$] (Переработанный вариант из Амэмия, 1985). Для теста Хаусмана, описанного в разделе 8.4.1, пусть $V_{11} = \V[\hat{\theta}]$, $V_{22} = \V[\tilde{\theta}]$ и $V_{12} = \Cov[\hat{\theta},\tilde{\theta}]$.
\begin{enumerate}
\item Покажите, что оценка $\bar{\theta} = \hat{\theta} + [V_{11} + V_{22} - 2V_{12}]^{-1}(\tilde{\theta},\hat{\theta})$ имеет асимптотическую ковариационную матрицу $\V[\bar{\theta}] = V_{11} - [V_{11} - V_{12}][V_{11} + V_{22} - 2V_{12}]^{-1}[V_{11} - V_{12}]$.
\item Покажите, что $\V[\bar{\theta}]$ меньше, чем $\V[\hat{\theta}]$ в матричном смысле кроме случая, когда $\Cov[\hat{\theta},\tilde{\theta}] = \V[\hat{\theta}]$.
\item Теперь предположим, что $\hat{\theta}$ полностью эффективна. Может ли $\V[\bar{\theta}]$ быть меньше $\V[\hat{\theta}]$? Каков Ваш вывод?
\end{enumerate}
\item [$8 - 4$] Предположим, что две модели являются невложенными и что есть $N = 200$ наблюдений. Для модели 1 количество параметров $q = 10$ и $\ln L = - 400$. Для модели 3 количество параметров $q = 10$ и $\ln L = - 380$.
\begin{enumerate}
\item Какая модель будет предпочитаться при использовании $AIC$?
\item Какая модель будет предпочитаться при использовании $BIC$?
\item Какая модель будет предпочитаться, если модели были на самом деле вложенными, а мы использовали тест отношения правдоподобия на уровне значимости 0.05?
\end{enumerate}
\item [$8 - 5$] Используйте данные о расходах на здравоохранение из раздела 16.6. Модель --- пробит-регрессия $DMED$, переменной-индикатора для положительных расходов на здравоохранение, на 17 регрессоров, которые были перечислены во втором параграфе раздела 16.6. Необходимо получить оценки, которые приведены в первой колонке таблицы 16.1.
\begin{enumerate}
\item Проведите совместный тест на статистическую значимость субъективных индикаторов  здоровья $HLTHG$, $HLTHF$ и $HLTHP$ на уровне значимости 0.05, используя тест Хаусмана. [Чтобы ответить на этот вопрос может потребоваться дополнительный код, который зависит от используемого программного обеспечения]
\item Является ли тест Хаусмана лучшим в данном случае?
\item Отвергает ли информационно матричный тест на уровне значимости 0.05 ограничения этой модели? [Чтобы ответить на этот вопрос может потребоваться дополнительный код]
\item Сравните модель, которая не учитывает $HLTHG$, $HLTHF$ и $HLTHP$, с моделью, которая не учитывает $LC$, $IDP$ и $LPI$ на основе $R_{RES}^2$, $R_{EXP}^2$, $R_{COR}^2$ и $R_{RG}^2$.
\end{enumerate}
\end{enumerate}


